# -*- coding: utf-8 -*-
"""
Structura HR ì´ì§ ì˜ˆì¸¡ Flask ë°±ì—”ë“œ ì„œë¹„ìŠ¤
XGBoost + xAI (SHAP, LIME) ê¸°ë°˜ ì„¤ëª… ê°€ëŠ¥í•œ AI ì‹œìŠ¤í…œ
React ì—°ë™ì— ìµœì í™”ëœ REST API ì„œë²„
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
from werkzeug.exceptions import HTTPException
from werkzeug.utils import secure_filename
import logging
import os
import json
import pickle
import traceback
import numpy as np
import pandas as pd
from datetime import datetime
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Tuple, Any
from collections import defaultdict

# ML ê´€ë ¨ imports
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import (
    roc_auc_score, average_precision_score, f1_score,
    precision_score, recall_score, accuracy_score,
    classification_report, confusion_matrix, precision_recall_curve
)
import xgboost as xgb
from xgboost import XGBClassifier

# xAI ê´€ë ¨ imports
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    print("Warning: SHAP not available. Install with: pip install shap")

try:
    import lime
    import lime.lime_tabular
    LIME_AVAILABLE = True
except ImportError:
    LIME_AVAILABLE = False
    print("Warning: LIME not available. Install with: pip install lime")

# Optuna for hyperparameter optimization
try:
    import optuna
    from optuna.pruners import MedianPruner
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    print("Warning: Optuna not available. Using default hyperparameters.")

import warnings
warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ------------------------------------------------------
# ë°ì´í„° ëª¨ë¸ ì •ì˜
# ------------------------------------------------------

@dataclass
class PredictionResult:
    """ì˜ˆì¸¡ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    employee_id: Optional[str]
    attrition_probability: float
    attrition_prediction: int
    risk_category: str
    confidence_score: float
    prediction_timestamp: str = None
    
    def __post_init__(self):
        if self.prediction_timestamp is None:
            self.prediction_timestamp = datetime.now().isoformat()
    
    def to_dict(self):
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ìš©)"""
        return asdict(self)

@dataclass
class ExplainabilityResult:
    """ì„¤ëª… ê°€ëŠ¥ì„± ë¶„ì„ ê²°ê³¼"""
    employee_id: Optional[str]
    feature_importance: Dict[str, float]
    shap_values: Optional[Dict[str, float]]
    lime_explanation: Optional[Dict[str, Any]]
    top_risk_factors: List[Dict[str, Any]]
    top_protective_factors: List[Dict[str, Any]]
    explanation_timestamp: str = None
    
    def __post_init__(self):
        if self.explanation_timestamp is None:
            self.explanation_timestamp = datetime.now().isoformat()
    
    def to_dict(self):
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ìš©)"""
        return asdict(self)

# ------------------------------------------------------
# HR ì´ì§ ì˜ˆì¸¡ê¸° í´ë˜ìŠ¤ (Flask ìµœì í™”)
# ------------------------------------------------------

class StructuraHRPredictor:
    """Structura HR ì´íƒˆ ì˜ˆì¸¡ ì‹œìŠ¤í…œ (xAI í¬í•¨)"""
    
    def __init__(self, data_path: str = None, random_state: int = 42):
        self.data_path = data_path
        self.random_state = random_state
        self.model = None
        self.data = None  # ë°ì´í„° ì €ì¥ìš© ì†ì„± ì¶”ê°€
        self.feature_columns = None
        self.optimal_threshold = 0.018  # ë…¸íŠ¸ë¶ì—ì„œ ìµœì í™”ëœ ì„ê³„ê°’ (ì¬í˜„ìœ¨ 70% ëª©í‘œ)
        self.scale_pos_weight = 1.0
        
        # xAI ê´€ë ¨
        self.shap_explainer = None
        self.lime_explainer = None
        self.X_train_sample = None  # LIMEìš© ë°°ê²½ ë°ì´í„°
        
        # ì „ì²˜ë¦¬ ì„¤ì •
        self.setup_preprocessing_config()
        
    def setup_preprocessing_config(self):
        """ì „ì²˜ë¦¬ ì„¤ì • ì´ˆê¸°í™” (ë…¸íŠ¸ë¶ ê¸°ë°˜)"""
        # ìˆœì„œí˜• ë³€ìˆ˜ë“¤ (ë…¸íŠ¸ë¶ê³¼ ë™ì¼)
        self.ordinal_cols = ['RelationshipSatisfaction', 'Education', 'PerformanceRating', 
                            'JobInvolvement', 'EnvironmentSatisfaction', 'JobLevel', 
                            'JobSatisfaction', 'StockOptionLevel', 'WorkLifeBalance']

        # ëª…ëª©í˜• ë³€ìˆ˜ë“¤ (ë…¸íŠ¸ë¶ê³¼ ë™ì¼)
        self.nominal_cols = ['BusinessTravel', 'Department', 'EducationField', 'Gender', 
                            'JobRole', 'MaritalStatus', 'OverTime']

        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ (ë…¸íŠ¸ë¶ê³¼ ë™ì¼)
        self.numerical_cols = ['Age', 'DailyRate', 'DistanceFromHome', 'HourlyRate', 
                              'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 
                              'PercentSalaryHike', 'TotalWorkingYears', 'TrainingTimesLastYear', 
                              'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 
                              'YearsWithCurrManager', 'EmployeeCount', 'EmployeeNumber', 'StandardHours']
        
        # ì €ìƒê´€ ì„ê³„ê°’ (ë…¸íŠ¸ë¶ê³¼ ë™ì¼)
        self.low_corr_threshold = 0.03
        
        # ë ˆê±°ì‹œ í˜¸í™˜ì„±ì„ ìœ„í•œ ë§¤í•‘ë“¤
        self.ORDINAL_MAPS = {
            "Education": {1: "Below College", 2: "College", 3: "Bachelor", 4: "Master", 5: "Doctor"},
            "JobLevel": {1: "Entry", 2: "Junior", 3: "Mid", 4: "Senior", 5: "Executive"},
            "JobInvolvement": {1: "Low", 2: "Medium", 3: "High", 4: "Very High"},
            "JobSatisfaction": {1: "Low", 2: "Medium", 3: "High", 4: "Very High"},
            "EnvironmentSatisfaction": {1: "Low", 2: "Medium", 3: "High", 4: "Very High"},
            "RelationshipSatisfaction": {1: "Low", 2: "Medium", 3: "High", 4: "Very High"},
            "WorkLifeBalance": {1: "Bad", 2: "Good", 3: "Better", 4: "Best"},
            "PerformanceRating": {1: "Low", 2: "Good", 3: "Excellent", 4: "Outstanding"},
            "StockOptionLevel": {0: "None", 1: "Level 1", 2: "Level 2", 3: "Level 3"},
        }
        
        # ëª…ëª©í˜• ë³€ìˆ˜ë“¤ (ë ˆê±°ì‹œ í˜¸í™˜ì„±)
        self.NOMINAL_COLS = self.nominal_cols
        
        # ìˆœì„œí˜• ë¼ë²¨ ìˆœì„œ
        self.ORDINAL_LABEL_ORDERS = {
            "JobInvolvement": ["Low", "Medium", "High", "Very High"],
            "JobSatisfaction": ["Low", "Medium", "High", "Very High"],
            "EnvironmentSatisfaction": ["Low", "Medium", "High", "Very High"],
            "RelationshipSatisfaction": ["Low", "Medium", "High", "Very High"],
            "WorkLifeBalance": ["Bad", "Good", "Better", "Best"],
            "PerformanceRating": ["Low", "Good", "Excellent", "Outstanding"],
            "Education": ["Below College", "College", "Bachelor", "Master", "Doctor"],
            "JobLevel": ["Entry", "Junior", "Mid", "Senior", "Executive"],
            "StockOptionLevel": ["None", "Level 1", "Level 2", "Level 3"],
        }

    def load_data(self) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë”©"""
        if not os.path.exists(self.data_path):
            raise FileNotFoundError(f"Data file not found: {self.data_path}")
        
        df = pd.read_csv(self.data_path)
        self.data = df  # ë°ì´í„°ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ì— ì €ì¥
        logger.info(f"ë°ì´í„° ë¡œë”© ì™„ë£Œ: {df.shape}")
        return df
    
    def preprocess_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """ë°ì´í„° ì „ì²˜ë¦¬ (ë…¸íŠ¸ë¶ ê¸°ë°˜ ìµœì‹  ë²„ì „)"""
        logger.info("ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
        
        # 1. ë³€ìˆ˜ íƒ€ì…ë³„ ë¶„ë¥˜ (ë…¸íŠ¸ë¶ê³¼ ë™ì¼)
        all_feature_columns = self.ordinal_cols + self.nominal_cols + self.numerical_cols
        
        # 2. ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§
        available_columns = df.columns.tolist()
        existing_feature_columns = [col for col in all_feature_columns if col in available_columns]
        missing_columns = [col for col in all_feature_columns if col not in available_columns]
        
        if missing_columns:
            logger.warning(f"ëˆ„ë½ëœ ì»¬ëŸ¼ë“¤: {missing_columns}")
            logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë“¤: {available_columns}")
        
        # 3. í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ + íƒ€ê²Ÿ
        target_columns = existing_feature_columns + ['Attrition']
        df_processed = df[target_columns].copy()
        logger.info(f"ì„ íƒëœ íŠ¹ì„± ë³€ìˆ˜ ê°œìˆ˜: {len(existing_feature_columns)} (ì „ì²´ {len(all_feature_columns)}ê°œ ì¤‘)")
        
        # 3. ìƒìˆ˜ ì»¬ëŸ¼ ì œê±° (ë…¸íŠ¸ë¶ê³¼ ë™ì¼)
        constant_cols_found = []
        for col in df_processed.columns:
            if col != 'Attrition':
                if df_processed[col].nunique() <= 1:
                    constant_cols_found.append(col)
        
        if constant_cols_found:
            df_processed = df_processed.drop(columns=constant_cols_found)
            logger.info(f"ìƒìˆ˜ ì»¬ëŸ¼ ì œê±°: {constant_cols_found}")
        
        # 4. ì»¬ëŸ¼ íƒ€ì… ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ìœ ì§€)
        self.ordinal_cols = [col for col in self.ordinal_cols if col in df_processed.columns]
        self.nominal_cols = [col for col in self.nominal_cols if col in df_processed.columns]
        self.numerical_cols = [col for col in self.numerical_cols if col in df_processed.columns]
        
        # 5. ëª…ëª©í˜• ë²”ì£¼í˜• ë³€ìˆ˜ë§Œ ë¼ë²¨ ì¸ì½”ë”© (ë…¸íŠ¸ë¶ê³¼ ë™ì¼)
        logger.info(f"ëª…ëª©í˜• ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©: {self.nominal_cols}")
        self.encoders = {}
        for col in self.nominal_cols:
            if col in df_processed.columns:
                from sklearn.preprocessing import LabelEncoder
                le = LabelEncoder()
                df_processed[col] = le.fit_transform(df_processed[col].astype(str))
                self.encoders[col] = le
        
        # 6. íƒ€ê²Ÿ ë³€ìˆ˜ ì¸ì½”ë”© (ë…¸íŠ¸ë¶ê³¼ ë™ì¼)
        df_processed['Attrition'] = (df_processed['Attrition'] == 'Yes').astype(int)
        
        # 7. ìƒê´€ê´€ê³„ ë¶„ì„ ë° ì €ìƒê´€ ë³€ìˆ˜ ì œê±° (ë…¸íŠ¸ë¶ê³¼ ë™ì¼)
        correlation_with_target = df_processed.corr()['Attrition'].abs().sort_values(ascending=False)
        low_corr_features = correlation_with_target[correlation_with_target < self.low_corr_threshold].index.tolist()
        if 'Attrition' in low_corr_features:
            low_corr_features.remove('Attrition')
        
        if low_corr_features:
            df_processed = df_processed.drop(columns=low_corr_features)
            logger.info(f"ì €ìƒê´€ ë³€ìˆ˜ ì œê±° (< {self.low_corr_threshold}): {low_corr_features}")
        
        # 8. X, y ë¶„ë¦¬
        y = df_processed['Attrition']
        X = df_processed.drop(columns=['Attrition'])
        
        # 9. ë°ì´í„° íƒ€ì… ë³€í™˜ (XGBoost í˜¸í™˜ì„±)
        logger.info("ë°ì´í„° íƒ€ì… ë³€í™˜ ì¤‘...")
        for col in X.columns:
            if X[col].dtype == 'object':
                # object íƒ€ì…ì„ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜
                try:
                    X[col] = pd.to_numeric(X[col], errors='coerce')
                    # NaN ê°’ì´ ìˆìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€
                    if X[col].isna().any():
                        X[col] = X[col].fillna(0)
                        logger.info(f"  {col}: object â†’ numeric (NaN â†’ 0)")
                    else:
                        logger.info(f"  {col}: object â†’ numeric")
                except:
                    # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¼ë²¨ ì¸ì½”ë”©
                    from sklearn.preprocessing import LabelEncoder
                    le = LabelEncoder()
                    X[col] = le.fit_transform(X[col].astype(str))
                    logger.info(f"  {col}: object â†’ label encoded")
        
        # 10. ìµœì¢… íŠ¹ì„± ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        self.final_features = X.columns.tolist()
        logger.info(f"ìµœì¢… íŠ¹ì„± ë³€ìˆ˜ ê°œìˆ˜: {len(self.final_features)}")
        logger.info("ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")
        
        return X, y
    
    def _preprocess_single_employee(self, df: pd.DataFrame) -> pd.DataFrame:
        """ê°œë³„ ì§ì› ë°ì´í„° ì „ì²˜ë¦¬ (ì˜ˆì¸¡ìš©)"""
        df_processed = df.copy()
        
        # Attrition ì»¬ëŸ¼ ì œê±° (ìˆë‹¤ë©´)
        if 'Attrition' in df_processed.columns:
            df_processed = df_processed.drop(columns=['Attrition'])
        
        # í›ˆë ¨ ì‹œ ì‚¬ìš©ëœ í”¼ì²˜ë§Œ ì„ íƒ
        if hasattr(self, 'final_features') and self.final_features:
            # ëˆ„ë½ëœ í”¼ì²˜ëŠ” 0ìœ¼ë¡œ ì±„ì›€
            for feature in self.final_features:
                if feature not in df_processed.columns:
                    df_processed[feature] = 0
            
            # í•„ìš”í•œ í”¼ì²˜ë§Œ ì„ íƒí•˜ê³  ìˆœì„œ ë§ì¶¤
            df_processed = df_processed[self.final_features]
        else:
            # ê¸°ë³¸ ì „ì²˜ë¦¬ ì ìš©
            # 1. í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
            available_cols = [col for col in (self.ordinal_cols + self.nominal_cols + self.numerical_cols) 
                            if col in df_processed.columns]
            df_processed = df_processed[available_cols]
            
            # 2. ëª…ëª©í˜• ë³€ìˆ˜ ì¸ì½”ë”© (í›ˆë ¨ ì‹œ ì‚¬ìš©ëœ ì¸ì½”ë” ì ìš©)
            if hasattr(self, 'encoders'):
                for col, encoder in self.encoders.items():
                    if col in df_processed.columns:
                        try:
                            # ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬ëŠ” ê°€ì¥ ë¹ˆë²ˆí•œ ê°’ìœ¼ë¡œ ëŒ€ì²´
                            unknown_mask = ~df_processed[col].astype(str).isin(encoder.classes_)
                            if unknown_mask.any():
                                most_frequent = encoder.classes_[0]  # ì²« ë²ˆì§¸ í´ë˜ìŠ¤ ì‚¬ìš©
                                df_processed.loc[unknown_mask, col] = most_frequent
                            
                            df_processed[col] = encoder.transform(df_processed[col].astype(str))
                        except Exception as e:
                            logger.warning(f"ì¸ì½”ë”© ì‹¤íŒ¨ {col}: {str(e)}, ê¸°ë³¸ê°’ 0 ì‚¬ìš©")
                            df_processed[col] = 0
        
        # 3. ë°ì´í„° íƒ€ì… ë³€í™˜ (XGBoost í˜¸í™˜ì„±)
        for col in df_processed.columns:
            if df_processed[col].dtype == 'object':
                try:
                    df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')
                    if df_processed[col].isna().any():
                        df_processed[col] = df_processed[col].fillna(0)
                except:
                    # ë³€í™˜ ì‹¤íŒ¨ ì‹œ 0ìœ¼ë¡œ ì„¤ì •
                    df_processed[col] = 0
        
        return df_processed
    
    def _coerce_ordinal_to_numeric(self, df: pd.DataFrame, ordinal_cols: List[str]) -> pd.DataFrame:
        """ìˆœì„œí˜• ë³€ìˆ˜ë¥¼ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜"""
        df = df.copy()
        
        for c in ordinal_cols:
            s = df[c]
            
            if pd.api.types.is_numeric_dtype(s):
                df[c] = pd.to_numeric(s, errors="coerce")
                continue
            
            if pd.api.types.is_categorical_dtype(s):
                if getattr(s.dtype, "ordered", False):
                    codes = s.cat.codes.replace(-1, np.nan)
                    df[c] = codes.astype(float)
                    continue
                
                try_num = pd.to_numeric(s.astype(str), errors="coerce")
                if try_num.notna().any():
                    df[c] = try_num
                    continue
                
                if c in self.ORDINAL_LABEL_ORDERS:
                    mapped = self._map_labels_to_codes(s, self.ORDINAL_LABEL_ORDERS[c])
                    if mapped.notna().any():
                        df[c] = mapped.astype(float)
                        continue
                
                df[c] = s.cat.codes.replace(-1, np.nan).astype(float)
                continue
            
            try_num = pd.to_numeric(s, errors="coerce")
            if try_num.notna().any():
                df[c] = try_num
                continue
            
            if c in self.ORDINAL_LABEL_ORDERS:
                mapped = self._map_labels_to_codes(s, self.ORDINAL_LABEL_ORDERS[c])
                df[c] = mapped.astype(float)
            else:
                codes, _ = pd.factorize(s, na_sentinel=-1)
                df[c] = pd.Series(codes, index=s.index).replace(-1, np.nan).astype(float)
        
        return df
    
    def _map_labels_to_codes(self, series: pd.Series, labels_in_order: List[str]) -> pd.Series:
        """ë¼ë²¨ì„ ì½”ë“œë¡œ ë§¤í•‘"""
        s = series.astype(str).str.strip()
        start_code = 0 if labels_in_order[0] == "None" else 1
        mapping = {lab.lower(): code for code, lab in enumerate(labels_in_order, start=start_code)}
        return s.str.lower().map(mapping)

    def train_model(self, X_train: pd.DataFrame, y_train: pd.Series, 
                   hyperparams: Optional[Dict] = None) -> XGBClassifier:
        """ëª¨ë¸ í›ˆë ¨ ë° xAI ì„¤ì •"""
        import time
        start_time = time.time()
        
        logger.info(f"ğŸš€ XGBoost ëª¨ë¸ í›ˆë ¨ ì‹œì‘... (ë°ì´í„°: {len(X_train)}í–‰ x {len(X_train.columns)}ì—´)")
        
        if hyperparams is None:
            hyperparams = self._get_default_params()
        
        # í´ë˜ìŠ¤ ë¶ˆê· í˜• ê°€ì¤‘ì¹˜ ê³„ì‚°
        neg = int((y_train == 0).sum())
        pos = int((y_train == 1).sum())
        self.scale_pos_weight = neg / max(pos, 1)
        
        logger.info(f"ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬: Negative={neg}, Positive={pos}, Scale_pos_weight={self.scale_pos_weight:.3f}")
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        params = {
            'n_estimators': 1000,
            'objective': 'binary:logistic',
            'eval_metric': 'auc',
            'tree_method': 'hist',
            'enable_categorical': True,
            'scale_pos_weight': self.scale_pos_weight,
            'n_jobs': -1,
            'random_state': self.random_state,
            'verbosity': 1,  # í•™ìŠµ ê³¼ì • í‘œì‹œ
            **hyperparams
        }
        
        logger.info(f"ğŸ”§ XGBoost íŒŒë¼ë¯¸í„°: n_estimators={params['n_estimators']}, tree_method={params['tree_method']}")
        
        self.model = XGBClassifier(**params)
        
        # ì‹¤ì œ í•™ìŠµ ì‹œê°„ ì¸¡ì •
        fit_start_time = time.time()
        self.model.fit(X_train, y_train)
        fit_end_time = time.time()
        
        fit_duration = fit_end_time - fit_start_time
        logger.info(f"â±ï¸ XGBoost í•™ìŠµ ì™„ë£Œ: {fit_duration:.2f}ì´ˆ ({fit_duration/60:.1f}ë¶„)")
        
        self.feature_columns = X_train.columns.tolist()
        
        # xAI ì„¤ì •
        logger.info("ğŸ” xAI ì„¤ëª…ê¸° ì„¤ì • ì¤‘...")
        xai_start_time = time.time()
        self._setup_explainers(X_train, y_train)
        xai_end_time = time.time()
        
        xai_duration = xai_end_time - xai_start_time
        logger.info(f"ğŸ” xAI ì„¤ì • ì™„ë£Œ: {xai_duration:.2f}ì´ˆ")
        
        total_duration = time.time() - start_time
        logger.info(f"âœ… ì „ì²´ ëª¨ë¸ í›ˆë ¨ ë° xAI ì„¤ì • ì™„ë£Œ: {total_duration:.2f}ì´ˆ ({total_duration/60:.1f}ë¶„)")
        return self.model
    
    def _setup_explainers(self, X_train: pd.DataFrame, y_train: pd.Series):
        """SHAP ë° LIME ì„¤ëª…ê¸° ì„¤ì •"""
        
        # SHAP ì„¤ì •
        if SHAP_AVAILABLE:
            try:
                logger.info("ğŸ” SHAP ì„¤ëª…ê¸° ì„¤ì • ì¤‘...")
                self.shap_explainer = shap.TreeExplainer(self.model)
                logger.info("âœ… SHAP ì„¤ì • ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ SHAP ì„¤ì • ì‹¤íŒ¨: {str(e)}")
                logger.error(f"SHAP ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                self.shap_explainer = None
        else:
            logger.warning("âš ï¸ SHAP ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            self.shap_explainer = None
        
        # LIME ì„¤ì •
        if LIME_AVAILABLE:
            try:
                logger.info("ğŸ” LIME ì„¤ëª…ê¸° ì„¤ì • ì¤‘...")
                # ìƒ˜í”Œë§ëœ í›ˆë ¨ ë°ì´í„° (LIME ë°°ê²½ìš©)
                sample_size = min(1000, len(X_train))
                sample_indices = np.random.choice(len(X_train), sample_size, replace=False)
                self.X_train_sample = X_train.iloc[sample_indices].values
                
                # ë²”ì£¼í˜• í”¼ì²˜ ì¸ë±ìŠ¤ ì°¾ê¸°
                categorical_features = []
                for i, col in enumerate(X_train.columns):
                    if col in self.nominal_cols or pd.api.types.is_categorical_dtype(X_train[col]):
                        categorical_features.append(i)
                
                self.lime_explainer = lime.lime_tabular.LimeTabularExplainer(
                    self.X_train_sample,
                    feature_names=self.feature_columns,
                    categorical_features=categorical_features,
                    mode='classification',
                    random_state=self.random_state
                )
                logger.info("âœ… LIME ì„¤ì • ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ LIME ì„¤ì • ì‹¤íŒ¨: {str(e)}")
                logger.error(f"LIME ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                self.lime_explainer = None
        else:
            logger.warning("âš ï¸ LIME ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            self.lime_explainer = None
        
        # XAI ì„¤ì • ìƒíƒœ ë¡œê¹…
        logger.info(f"ğŸ” XAI ì„¤ì • ìƒíƒœ:")
        logger.info(f"  - SHAP explainer: {'âœ… í™œì„±í™”' if self.shap_explainer else 'âŒ ë¹„í™œì„±í™”'}")
        logger.info(f"  - LIME explainer: {'âœ… í™œì„±í™”' if self.lime_explainer else 'âŒ ë¹„í™œì„±í™”'}")
        logger.info(f"  - Feature columns: {len(self.feature_columns)}ê°œ")
    
    def _get_default_params(self) -> Dict:
        """ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ë…¸íŠ¸ë¶ì—ì„œ ìµœì í™”ëœ ê°’)"""
        return {
            'n_estimators': 800,
            'max_depth': 8,
            'learning_rate': 0.1,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'reg_alpha': 5,
            'reg_lambda': 10,
            'min_child_weight': 5,
            'scale_pos_weight': 8.0
        }

    def predict(self, X: pd.DataFrame, return_proba: bool = False) -> np.ndarray:
        """ì˜ˆì¸¡"""
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if self.feature_columns is not None:
            missing_cols = set(self.feature_columns) - set(X.columns)
            if missing_cols:
                raise ValueError(f"í•„ìš”í•œ í”¼ì²˜ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_cols}")
            X = X[self.feature_columns]
        
        if return_proba:
            return self.model.predict_proba(X)[:, 1]
        else:
            proba = self.model.predict_proba(X)[:, 1]
            return (proba >= self.optimal_threshold).astype(int)

    def predict_single(self, employee_data: Dict) -> PredictionResult:
        """ë‹¨ì¼ ì§ì› ì˜ˆì¸¡"""
        # ë”•ì…”ë„ˆë¦¬ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame([employee_data])
        
        # ì „ì²˜ë¦¬ ì ìš© (ê°œë³„ ì§ì›ìš©)
        df_processed = self._preprocess_single_employee(df)
        
        # ì˜ˆì¸¡
        probability = self.predict(df_processed, return_proba=True)[0]
        prediction = int(probability >= self.optimal_threshold)
        
        # ìœ„í—˜ ë²”ì£¼ ê²°ì •
        if probability >= 0.7:
            risk_category = "HIGH"
        elif probability >= 0.4:
            risk_category = "MEDIUM"
        else:
            risk_category = "LOW"
        
        # ì‹ ë¢°ë„ ì ìˆ˜ (í™•ë¥ ê³¼ ì„ê³„ê°’ì˜ ê±°ë¦¬ ê¸°ë°˜)
        confidence_score = abs(probability - 0.5) * 2
        
        return PredictionResult(
            employee_id=employee_data.get('EmployeeNumber', None),
            attrition_probability=float(probability),
            attrition_prediction=prediction,
            risk_category=risk_category,
            confidence_score=float(confidence_score)
        )

    def explain_prediction(self, employee_data: Dict) -> ExplainabilityResult:
        """ì˜ˆì¸¡ ì„¤ëª… (xAI) - ì‹¤ì œ XAI ë°ì´í„° ìƒì„±"""
        # ë”•ì…”ë„ˆë¦¬ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame([employee_data])
        
        # ì „ì²˜ë¦¬ ì ìš© (ê°œë³„ ì§ì›ìš©)
        df_processed = self._preprocess_single_employee(df)
        
        # í”¼ì²˜ ì¤‘ìš”ë„ (ëª¨ë¸ ê¸°ë³¸) - ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš©
        feature_importance = {}
        if self.model and hasattr(self.model, 'feature_importances_') and self.feature_columns:
            for feature, importance in zip(self.feature_columns, self.model.feature_importances_):
                feature_importance[feature] = float(importance)
            logger.info(f"ì‹¤ì œ Feature Importance ê³„ì‚° ì™„ë£Œ: {len(feature_importance)}ê°œ í”¼ì²˜")
        else:
            logger.warning("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ê±°ë‚˜ í”¼ì²˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # SHAP ê°’ - ì‹¤ì œ ê³„ì‚°ë§Œ ìˆ˜í–‰
        shap_values = {}
        if self.shap_explainer and len(df_processed) > 0:
            try:
                # ì‹¤ì œ SHAP ê³„ì‚°
                shap_vals = self.shap_explainer.shap_values(df_processed[self.feature_columns])
                if isinstance(shap_vals, list):
                    shap_vals = shap_vals[1] if len(shap_vals) > 1 else shap_vals[0]
                
                for feature, shap_val in zip(self.feature_columns, shap_vals[0]):
                    shap_values[feature] = float(shap_val)
                logger.info(f"ì‹¤ì œ SHAP ê°’ ê³„ì‚° ì™„ë£Œ: {len(shap_values)}ê°œ í”¼ì²˜")
            except Exception as e:
                logger.warning(f"SHAP ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                shap_values = {}
        else:
            logger.warning("SHAP explainerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            if not SHAP_AVAILABLE:
                logger.warning("SHAP ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì¹˜ ëª…ë ¹: pip install shap>=0.42.0")
            elif not self.shap_explainer:
                logger.warning("SHAP explainer ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
        
        # LIME ì„¤ëª… - ì‹¤ì œ ê³„ì‚°ë§Œ ìˆ˜í–‰
        lime_explanation = {}
        if self.lime_explainer and len(df_processed) > 0:
            try:
                def predict_fn(x):
                    return self.model.predict_proba(pd.DataFrame(x, columns=self.feature_columns))
                
                explanation = self.lime_explainer.explain_instance(
                    df_processed[self.feature_columns].values[0],
                    predict_fn,
                    num_features=8
                )
                
                lime_explanation = {
                    'features': [str(item[0]) for item in explanation.as_list()],
                    'values': [float(item[1]) for item in explanation.as_list()],
                    'intercept': float(explanation.intercept[1]) if hasattr(explanation, 'intercept') else 0.0
                }
                logger.info(f"ì‹¤ì œ LIME ì„¤ëª… ìƒì„± ì™„ë£Œ: {len(lime_explanation['features'])}ê°œ í”¼ì²˜")
            except Exception as e:
                logger.warning(f"LIME ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                lime_explanation = {}
        else:
            logger.warning("LIME explainerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            if not LIME_AVAILABLE:
                logger.warning("LIME ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì¹˜ ëª…ë ¹: pip install lime>=0.2.0.1")
            elif not self.lime_explainer:
                logger.warning("LIME explainer ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
        
        # ìƒìœ„ ìœ„í—˜/ë³´í˜¸ ìš”ì¸ ì¶”ì¶œ
        importance_items = list(feature_importance.items()) if feature_importance else []
        importance_items.sort(key=lambda x: abs(x[1]), reverse=True)
        
        # SHAP ê°’ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if shap_values:
            shap_items = list(shap_values.items())
            shap_items.sort(key=lambda x: abs(x[1]), reverse=True)
            
            top_risk_factors = [
                {"feature": feat, "impact": float(val), "type": "risk"}
                for feat, val in shap_items if val > 0
            ][:5]
            top_protective_factors = [
                {"feature": feat, "impact": float(abs(val)), "type": "protective"}
                for feat, val in shap_items if val < 0
            ][:5]
        else:
            # í”¼ì²˜ ì¤‘ìš”ë„ ê¸°ë°˜
            top_risk_factors = [
                {"feature": feat, "impact": float(val), "type": "risk"}
                for feat, val in importance_items[:5]
            ]
            top_protective_factors = []
        
        return ExplainabilityResult(
            employee_id=employee_data.get('EmployeeNumber', None),
            feature_importance=feature_importance,
            shap_values=shap_values,
            lime_explanation=lime_explanation,
            top_risk_factors=top_risk_factors,
            top_protective_factors=top_protective_factors
        )

    def get_feature_importance(self, top_n: int = 20) -> pd.DataFrame:
        """í”¼ì²˜ ì¤‘ìš”ë„ ë°˜í™˜"""
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        importance_df = pd.DataFrame({
            'feature': self.feature_columns,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        return importance_df.head(top_n)

    def save_model(self, filepath: str):
        """ëª¨ë¸ ë° ì„¤ì • ì €ì¥"""
        if self.model is None:
            raise ValueError("ì €ì¥í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        model_data = {
            'model': self.model,
            'feature_columns': self.feature_columns,
            'optimal_threshold': self.optimal_threshold,
            'scale_pos_weight': self.scale_pos_weight,
            'preprocessing_config': {
                'ordinal_cols': getattr(self, 'ordinal_cols', []),
                'nominal_cols': getattr(self, 'nominal_cols', []),
                'numerical_cols': getattr(self, 'numerical_cols', []),
                'low_corr_threshold': getattr(self, 'low_corr_threshold', 0.03),
                'ORDINAL_LABEL_ORDERS': getattr(self, 'ORDINAL_LABEL_ORDERS', {}),
                'encoders': getattr(self, 'encoders', {})
            },
            'X_train_sample': self.X_train_sample
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        logger.info(f"ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
    
    def load_model(self, filepath: str):
        """ëª¨ë¸ ë° ì„¤ì • ë¡œë”©"""
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
        
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.model = model_data['model']
        self.feature_columns = model_data['feature_columns']
        self.optimal_threshold = model_data['optimal_threshold']
        self.scale_pos_weight = model_data['scale_pos_weight']
        
        # ì „ì²˜ë¦¬ ì„¤ì • ë³µì›
        config = model_data['preprocessing_config']
        self.ordinal_cols = config.get('ordinal_cols', [])
        self.nominal_cols = config.get('nominal_cols', [])
        self.numerical_cols = config.get('numerical_cols', [])
        self.low_corr_threshold = config.get('low_corr_threshold', 0.03)
        self.ORDINAL_LABEL_ORDERS = config.get('ORDINAL_LABEL_ORDERS', {})
        self.encoders = config.get('encoders', {})
        
        # xAI ë°ì´í„° ë³µì›
        self.X_train_sample = model_data.get('X_train_sample')
        
        # xAI ì„¤ëª…ê¸° ì¬ì„¤ì •
        if self.model and self.X_train_sample is not None:
            self._setup_explainers_from_saved_data()
        
        logger.info(f"ëª¨ë¸ì´ ë¡œë”©ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
    
    def _setup_explainers_from_saved_data(self):
        """ì €ì¥ëœ ë°ì´í„°ë¡œë¶€í„° xAI ì„¤ëª…ê¸° ì¬ì„¤ì •"""
        # SHAP ì„¤ì •
        if SHAP_AVAILABLE:
            try:
                self.shap_explainer = shap.TreeExplainer(self.model)
            except Exception as e:
                logger.warning(f"SHAP ì¬ì„¤ì • ì‹¤íŒ¨: {str(e)}")
        
        # LIME ì„¤ì •
        if LIME_AVAILABLE and self.X_train_sample is not None:
            try:
                categorical_features = []
                for i, col in enumerate(self.feature_columns):
                    if col in self.nominal_cols:
                        categorical_features.append(i)
                
                self.lime_explainer = lime.lime_tabular.LimeTabularExplainer(
                    self.X_train_sample,
                    feature_names=self.feature_columns,
                    categorical_features=categorical_features,
                    mode='classification',
                    random_state=self.random_state
                )
            except Exception as e:
                logger.warning(f"LIME ì¬ì„¤ì • ì‹¤íŒ¨: {str(e)}")

    def run_full_pipeline(self, optimize_hp: bool = False, use_sampling: bool = True) -> Dict:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë…¸íŠ¸ë¶ ê¸°ë°˜ ìµœì‹  ë²„ì „)"""
        logger.info("Structura ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ë…¸íŠ¸ë¶ ê¸°ë°˜)...")
        
        try:
            # 1. ë°ì´í„° ë¡œë”©
            logger.info("1. ë°ì´í„° ë¡œë”© ì¤‘...")
            df = self.load_data()
            
            # 2. ì „ì²˜ë¦¬
            logger.info("2. ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
            X, y = self.preprocess_data(df)
            
            # 3. í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í•  (ë…¸íŠ¸ë¶ê³¼ ë™ì¼í•˜ê²Œ 30%)
            logger.info("3. ë°ì´í„° ë¶„í•  ì¤‘...")
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.3, random_state=self.random_state, stratify=y
            )
            
            logger.info(f"ë°ì´í„° ë¶„í•  ì™„ë£Œ - í›ˆë ¨: {X_train.shape}, í…ŒìŠ¤íŠ¸: {X_test.shape}")
            logger.info(f"í´ë˜ìŠ¤ ê· í˜• (í›ˆë ¨): {y_train.value_counts(normalize=True).round(3).to_dict()}")
            
            # 4. í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²° (ë…¸íŠ¸ë¶ ê¸°ë°˜)
            if use_sampling:
                logger.info("4. í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²° ì¤‘...")
                X_train_balanced, y_train_balanced = self._apply_sampling(X_train, y_train)
                logger.info(f"ìƒ˜í”Œë§ ì™„ë£Œ: {len(X_train)} â†’ {len(X_train_balanced)}")
            else:
                X_train_balanced, y_train_balanced = X_train, y_train
            
            # 5. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ë…¸íŠ¸ë¶ ê¸°ë°˜)
            logger.info("5. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì¤‘...")
            if optimize_hp and OPTUNA_AVAILABLE:
                hyperparams = self._optimize_hyperparameters_enhanced(X_train_balanced, y_train_balanced)
            else:
                # ë…¸íŠ¸ë¶ì—ì„œ ìµœì í™”ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©
                hyperparams = self._get_default_params()
            
            # 6. ëª¨ë¸ í›ˆë ¨
            logger.info("6. ëª¨ë¸ í›ˆë ¨ ì¤‘...")
            self.train_model(X_train_balanced, y_train_balanced, hyperparams)
            
            # 7. ì„ê³„ê°’ ìµœì í™” (ì¬í˜„ìœ¨ 70% ëª©í‘œ)
            logger.info("7. ì„ê³„ê°’ ìµœì í™” ì¤‘...")
            self.optimal_threshold = self._optimize_threshold(X_test, y_test, target_recall=0.7)
            logger.info(f"ìµœì  ì„ê³„ê°’: {self.optimal_threshold:.3f}")
            
            # 8. ì„±ëŠ¥ í‰ê°€
            logger.info("8. ì„±ëŠ¥ í‰ê°€ ì¤‘...")
            y_pred_proba = self.predict(X_test, return_proba=True)
            y_pred = self.predict(X_test, return_proba=False)
            
            # 9. ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = {
                'roc_auc': roc_auc_score(y_test, y_pred_proba),
                'accuracy': accuracy_score(y_test, y_pred),
                'precision': precision_score(y_test, y_pred, zero_division=0),
                'recall': recall_score(y_test, y_pred, zero_division=0),
                'f1_score': f1_score(y_test, y_pred, zero_division=0),
                'avg_precision': average_precision_score(y_test, y_pred_proba),
                'optimal_threshold': self.optimal_threshold,
                'training_samples': len(X_train_balanced),
                'test_samples': len(X_test)
            }
            
            logger.info(f"âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
            logger.info(f"ì„±ëŠ¥ ì§€í‘œ - AUC: {metrics['roc_auc']:.3f}, F1: {metrics['f1_score']:.3f}, ì¬í˜„ìœ¨: {metrics['recall']:.3f}")
            return metrics
            
        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            raise

    def predict_single_employee(self, employee_data: Dict, employee_number: str) -> Dict:
        """ë‹¨ì¼ ì§ì› ì˜ˆì¸¡ (ë§ˆìŠ¤í„° ì„œë²„ í˜¸í™˜) - ì‹¤ì œ XAI ë°ì´í„° í¬í•¨"""
        try:
            logger.info(f"ğŸ” ì§ì› {employee_number} ì˜ˆì¸¡ ì‹œì‘...")
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            prediction_result = self.predict_single(employee_data)
            explanation_result = self.explain_prediction(employee_data)
            
            logger.info(f"ğŸ“Š ì§ì› {employee_number} ì˜ˆì¸¡ ê²°ê³¼: í™•ë¥ ={prediction_result.attrition_probability:.3f}, ìœ„í—˜ë„={prediction_result.risk_category}")
            
            # XAI ë°ì´í„° ë¡œê¹…
            logger.info(f"ğŸ” XAI ë°ì´í„° ìƒì„± ê²°ê³¼:")
            logger.info(f"  - Feature Importance: {len(explanation_result.feature_importance)}ê°œ í”¼ì²˜")
            logger.info(f"  - SHAP Values: {len(explanation_result.shap_values) if explanation_result.shap_values else 0}ê°œ í”¼ì²˜")
            logger.info(f"  - LIME Explanation: {'ìˆìŒ' if explanation_result.lime_explanation else 'ì—†ìŒ'}")
            
            # í†µí•© ê²°ê³¼ êµ¬ì„± - XAI ë°ì´í„°ë¥¼ ìµœìƒìœ„ ë ˆë²¨ì—ë„ í¬í•¨
            result = {
                'employee_number': employee_number,
                'attrition_probability': prediction_result.attrition_probability,
                'attrition_prediction': prediction_result.attrition_prediction,
                'risk_category': prediction_result.risk_category,
                'confidence_score': prediction_result.confidence_score,
                'prediction_timestamp': prediction_result.prediction_timestamp,
                
                # XAI ë°ì´í„°ë¥¼ ìµœìƒìœ„ ë ˆë²¨ì— í¬í•¨ (ê¸°ì¡´ í˜¸í™˜ì„±)
                'feature_importance': explanation_result.feature_importance or {},
                'shap_values': explanation_result.shap_values or {},
                'lime_explanation': explanation_result.lime_explanation or {},
                'model_interpretation': {
                    'top_risk_factors': explanation_result.top_risk_factors,
                    'top_protective_factors': explanation_result.top_protective_factors
                },
                'xai_explanation': {
                    'feature_importance': explanation_result.feature_importance or {},
                    'shap_values': explanation_result.shap_values or {},
                    'lime_explanation': explanation_result.lime_explanation or {},
                    'interpretation_method': 'XGBoost + SHAP + LIME'
                },
                
                # explanation êµ¬ì¡° (ìƒˆë¡œìš´ êµ¬ì¡°)
                'explanation': {
                    'feature_importance': explanation_result.feature_importance or {},
                    'shap_values': explanation_result.shap_values or {},
                    'lime_explanation': explanation_result.lime_explanation or {},
                    'top_risk_factors': explanation_result.top_risk_factors,
                    'top_protective_factors': explanation_result.top_protective_factors,
                    'individual_explanation': {
                        'top_risk_factors': explanation_result.top_risk_factors,
                        'top_protective_factors': explanation_result.top_protective_factors
                    }
                },
                'recommendations': self._generate_recommendations({
                    'risk_category': prediction_result.risk_category,
                    'attrition_probability': prediction_result.attrition_probability,
                    'explanation': {
                        'individual_explanation': {
                            'top_risk_factors': explanation_result.top_risk_factors
                        }
                    }
                })
            }
            
            return result
            
        except Exception as e:
            logger.error(f"ì§ì› {employee_number} ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
            raise

    def _apply_sampling(self, X_train: pd.DataFrame, y_train: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:
        """í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²° (ë…¸íŠ¸ë¶ ê¸°ë°˜)"""
        try:
            # ì—¬ëŸ¬ ìƒ˜í”Œë§ ê¸°ë²• ë¹„êµ í…ŒìŠ¤íŠ¸
            from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE
            from imblearn.combine import SMOTETomek, SMOTEENN
            from collections import Counter
            
            logger.info("í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²° ê¸°ë²• ë¹„êµ ì¤‘...")
            
            sampling_methods = {
                'SMOTE': SMOTE(random_state=self.random_state),
                'ADASYN': ADASYN(random_state=self.random_state),
                'BorderlineSMOTE': BorderlineSMOTE(random_state=self.random_state),
                'SMOTETomek': SMOTETomek(random_state=self.random_state)
            }
            
            sampling_results = {}
            for name, sampler in sampling_methods.items():
                try:
                    X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)
                    
                    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ëª¨ë¸ë¡œ ì„±ëŠ¥ í™•ì¸
                    quick_model = XGBClassifier(
                        n_estimators=100, 
                        random_state=self.random_state,
                        scale_pos_weight=5,
                        verbosity=0
                    )
                    
                    cv_f1 = cross_val_score(quick_model, X_resampled, y_resampled, 
                                           cv=3, scoring='f1', n_jobs=-1).mean()
                    
                    sampling_results[name] = {
                        'cv_f1': cv_f1,
                        'size': len(y_resampled),
                        'class_dist': Counter(y_resampled)
                    }
                    
                    logger.info(f"  {name}: F1={cv_f1:.3f}, Size={len(y_resampled)}")
                    
                except Exception as e:
                    logger.warning(f"  {name}: ì‹¤íŒ¨ ({str(e)[:50]})")
                    continue
            
            # ê°€ì¥ ì¢‹ì€ ìƒ˜í”Œë§ ë°©ë²• ì„ íƒ
            if sampling_results:
                best_sampling_name = max(sampling_results.items(), key=lambda x: x[1]['cv_f1'])[0]
                best_sampler = sampling_methods[best_sampling_name]
                X_train_balanced, y_train_balanced = best_sampler.fit_resample(X_train, y_train)
                
                logger.info(f"ì„ íƒëœ ìƒ˜í”Œë§: {best_sampling_name}")
                logger.info(f"CV F1 Score: {sampling_results[best_sampling_name]['cv_f1']:.3f}")
                
                balanced_dist = pd.Series(y_train_balanced).value_counts()
                logger.info(f"ê· í˜• í›„ ë¶„í¬: No={balanced_dist[0]}, Yes={balanced_dist[1]}")
                
                return pd.DataFrame(X_train_balanced, columns=X_train.columns), pd.Series(y_train_balanced)
            else:
                # ë°±ì—…: ê¸°ë³¸ SMOTE ì‚¬ìš©
                logger.warning("ëª¨ë“  ìƒ˜í”Œë§ ì‹¤íŒ¨, ê¸°ë³¸ SMOTE ì‚¬ìš©")
                smote = SMOTE(random_state=self.random_state)
                X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
                return pd.DataFrame(X_train_balanced, columns=X_train.columns), pd.Series(y_train_balanced)
                
        except ImportError:
            logger.warning("imblearn ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œë§ ìƒëµ")
            return X_train, y_train
        except Exception as e:
            logger.error(f"ìƒ˜í”Œë§ ì‹¤íŒ¨: {str(e)}")
            return X_train, y_train
    
    def _optimize_threshold(self, X_test: pd.DataFrame, y_test: pd.Series, target_recall: float = 0.7) -> float:
        """ì„ê³„ê°’ ìµœì í™” (ì¬í˜„ìœ¨ ëª©í‘œ ê¸°ë°˜)"""
        try:
            y_pred_proba = self.model.predict_proba(X_test)[:, 1]
            
            # Precision-Recall ê³¡ì„  ê³„ì‚°
            precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)
            
            # ë°°ì—´ ê¸¸ì´ ì¡°ì •
            if len(precision) > len(thresholds):
                precision = precision[:-1]
                recall = recall[:-1]
            
            # F1 ì ìˆ˜ ê³„ì‚°
            with np.errstate(divide='ignore', invalid='ignore'):
                f1_scores = 2 * (precision * recall) / (precision + recall)
                f1_scores = np.nan_to_num(f1_scores)
            
            # ëª©í‘œ ì¬í˜„ìœ¨ ì´ìƒì¸ ì¸ë±ìŠ¤ ì°¾ê¸°
            valid_indices = recall >= target_recall
            
            if np.any(valid_indices):
                valid_f1 = f1_scores[valid_indices]
                valid_thresholds = thresholds[valid_indices]
                
                if len(valid_f1) > 0 and len(valid_thresholds) > 0:
                    best_idx = np.argmax(valid_f1)
                    optimal_threshold = valid_thresholds[best_idx]
                    logger.info(f"ëª©í‘œ ì¬í˜„ìœ¨ {target_recall:.1%} ë‹¬ì„± ì„ê³„ê°’: {optimal_threshold:.3f}")
                    return float(optimal_threshold)
            
            # ëª©í‘œ ë‹¬ì„± ë¶ˆê°€ëŠ¥í•œ ê²½ìš°, F1 ìµœì í™” ì„ê³„ê°’ ì‚¬ìš©
            if len(f1_scores) > 0:
                best_f1_idx = np.argmax(f1_scores)
                optimal_threshold = thresholds[best_f1_idx]
                logger.warning(f"ëª©í‘œ ì¬í˜„ìœ¨ ë¯¸ë‹¬ì„±, F1 ìµœì í™” ì„ê³„ê°’ ì‚¬ìš©: {optimal_threshold:.3f}")
                return float(optimal_threshold)
            
            # ìµœì¢… ë°±ì—…
            logger.warning("ì„ê³„ê°’ ìµœì í™” ì‹¤íŒ¨, ê¸°ë³¸ê°’ 0.5 ì‚¬ìš©")
            return 0.5
            
        except Exception as e:
            logger.error(f"ì„ê³„ê°’ ìµœì í™” ì‹¤íŒ¨: {str(e)}")
            return 0.5
    
    def _optimize_hyperparameters_enhanced(self, X_train: pd.DataFrame, y_train: pd.Series) -> Dict:
        """í–¥ìƒëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (F1 Score ì¤‘ì‹¬)"""
        if not OPTUNA_AVAILABLE:
            return self._get_default_params()
        
        def objective(trial):
            # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ë²”ìœ„ í™•ëŒ€
            pos_weight = trial.suggest_float('scale_pos_weight', 3.0, 15.0)
            
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 300, 1200),
                'max_depth': trial.suggest_int('max_depth', 4, 12),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.25),
                'subsample': trial.suggest_float('subsample', 0.7, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),
                'reg_alpha': trial.suggest_float('reg_alpha', 0, 20),
                'reg_lambda': trial.suggest_float('reg_lambda', 0, 20),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),
                'scale_pos_weight': pos_weight
            }
            
            # F1 Score ê¸°ë°˜ êµì°¨ê²€ì¦
            kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
            
            model = XGBClassifier(
                objective='binary:logistic',
                eval_metric='auc',
                tree_method='hist',
                enable_categorical=True,
                n_jobs=-1,
                random_state=self.random_state,
                verbosity=0,
                **params
            )
            
            cv_scores = cross_val_score(
                model, X_train, y_train, 
                cv=kfold,
                scoring='f1',
                n_jobs=-1
            )
            
            return cv_scores.mean()
        
        study = optuna.create_study(
            direction='maximize',
            sampler=optuna.samplers.TPESampler(seed=self.random_state)
        )
        
        logger.info("í–¥ìƒëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ (150íšŒ ì‹œí–‰)...")
        study.optimize(objective, n_trials=150, show_progress_bar=False)
        
        logger.info(f"ìµœì  F1 ì ìˆ˜: {study.best_value:.4f}")
        logger.info(f"ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {study.best_params}")
        return study.best_params

    def _generate_recommendations(self, prediction_result):
        """ì˜ˆì¸¡ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        risk_category = prediction_result['risk_category']
        probability = prediction_result['attrition_probability']
        
        if risk_category == 'HIGH':
            recommendations.extend([
                "ì¦‰ì‹œ ë©´ë‹´ì„ í†µí•œ ì´ì§ ì˜ë„ íŒŒì•… í•„ìš”",
                "ì—…ë¬´ í™˜ê²½ ë° ë§Œì¡±ë„ ê°œì„  ë°©ì•ˆ ë…¼ì˜",
                "ê²½ë ¥ ê°œë°œ ë° ìŠ¹ì§„ ê¸°íšŒ ì œê³µ ê²€í† ",
                "ë³´ìƒ ì²´ê³„ ì¬ê²€í†  ë° ì¡°ì • ê³ ë ¤"
            ])
        elif risk_category == 'MEDIUM':
            recommendations.extend([
                "ì •ê¸°ì ì¸ 1:1 ë©´ë‹´ì„ í†µí•œ ìƒíƒœ ëª¨ë‹ˆí„°ë§",
                "ì—…ë¬´ ë§Œì¡±ë„ í–¥ìƒì„ ìœ„í•œ ê°œì„  ë°©ì•ˆ ëª¨ìƒ‰",
                "êµìœ¡ ë° ê°œë°œ ê¸°íšŒ ì œê³µ",
                "íŒ€ ë‚´ ì—­í•  ë° ì±…ì„ ì¬ì¡°ì • ê²€í† "
            ])
        else:  # LOW
            recommendations.extend([
                "í˜„ì¬ ìƒíƒœ ìœ ì§€ë¥¼ ìœ„í•œ ì§€ì†ì  ê´€ë¦¬",
                "ì„±ê³¼ ì¸ì • ë° í”¼ë“œë°± ì œê³µ",
                "ì¥ê¸° ê²½ë ¥ ê°œë°œ ê³„íš ìˆ˜ë¦½ ì§€ì›"
            ])
        
        # XAI ì„¤ëª… ê¸°ë°˜ ì¶”ê°€ ê¶Œì¥ì‚¬í•­
        if 'explanation' in prediction_result and 'individual_explanation' in prediction_result['explanation']:
            exp = prediction_result['explanation']['individual_explanation']
            if 'top_risk_factors' in exp:
                for factor in exp['top_risk_factors'][:2]:  # ìƒìœ„ 2ê°œ ìœ„í—˜ ìš”ì¸
                    feature = factor['feature']
                    if 'Satisfaction' in feature:
                        recommendations.append(f"{feature} ê°œì„ ì„ ìœ„í•œ êµ¬ì²´ì  ì•¡ì…˜ í”Œëœ ìˆ˜ë¦½")
                    elif 'WorkLifeBalance' in feature:
                        recommendations.append("ì›Œë¼ë°¸ ê°œì„ ì„ ìœ„í•œ ìœ ì—°ê·¼ë¬´ì œ ë„ì… ê²€í† ")
                    elif 'OverTime' in feature:
                        recommendations.append("ì—…ë¬´ëŸ‰ ì¡°ì • ë° ì´ˆê³¼ê·¼ë¬´ ìµœì†Œí™” ë°©ì•ˆ ë§ˆë ¨")
        
        return recommendations

# ------------------------------------------------------
# Flask ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„± ë° ì„¤ì •
# ------------------------------------------------------

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def get_data_path_by_analysis_type(analysis_type='batch'):
    """ë¶„ì„ íƒ€ì…ì— ë”°ë¥¸ ë°ì´í„° ê²½ë¡œ ë°˜í™˜"""
    # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ê²½ë¡œ ì‹œë„
    possible_paths = [
        f"../uploads/Structura/{analysis_type}",  # ëŒ€ë¬¸ì S
        f"../uploads/structura/{analysis_type}",  # ì†Œë¬¸ì s
        f"app/uploads/Structura/{analysis_type}",  # ì ˆëŒ€ ê²½ë¡œ ìŠ¤íƒ€ì¼
        f"app/uploads/structura/{analysis_type}"   # ì ˆëŒ€ ê²½ë¡œ ìŠ¤íƒ€ì¼ ì†Œë¬¸ì
    ]
    
    for uploads_dir in possible_paths:
        if os.path.exists(uploads_dir):
            files = [f for f in os.listdir(uploads_dir) if f.endswith('.csv')]
            if files:
                files.sort(reverse=True)  # ìµœì‹  íŒŒì¼ ìš°ì„ 
                logger.info(f"âœ… ë°ì´í„° íŒŒì¼ ë°œê²¬: {uploads_dir}/{files[0]}")
                return os.path.join(uploads_dir, files[0])
    
    logger.error(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œë„í•œ ê²½ë¡œë“¤: {possible_paths}")
    return None

def create_app():
    """Flask ì• í”Œë¦¬ì¼€ì´ì…˜ íŒ©í† ë¦¬"""
    
    app = Flask(__name__)
    
    # CORS ì„¤ì • (React ì—°ë™) - Chronos/Sentioì™€ ë™ì¼í•œ ë‹¨ìˆœ ì„¤ì •
    CORS(app)
    
    # ì„¤ì •
    app.config['JSON_AS_ASCII'] = False
    app.config['JSONIFY_PRETTYPRINT_REGULAR'] = True
    
    # ëª¨ë¸ ìºì‹œ (ìµœì í™”ëœ ëª¨ë¸ ì¬ì‚¬ìš©)
    app.model_cache = {
        'trained_model': None,
        'model_metadata': None,
        'training_timestamp': None,
        'data_hash': None  # ë°ì´í„° ë³€ê²½ ê°ì§€ìš©
    }
    
    # ------------------------------------------------------
    # ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”
    # ------------------------------------------------------
    
    def initialize_services():
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        try:
            logger.info("Structura HR ì˜ˆì¸¡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
            
            # uploads ë””ë ‰í† ë¦¬ì—ì„œ Structura íŒŒì¼ ì°¾ê¸° (post íŒŒì¼ ìš°ì„ )
            data_path = None
            
            # post ë¶„ì„ìš© íŒŒì¼ í™•ì¸ (ì‚¬í›„ ë¶„ì„ê³¼ ë°°ì¹˜ ë¶„ì„ì˜ í•™ìŠµìš©)
            post_dir = "../uploads/structura/post"
            if os.path.exists(post_dir):
                files = [f for f in os.listdir(post_dir) if f.endswith('.csv')]
                if files:
                    files.sort(reverse=True)  # ìµœì‹  íŒŒì¼ ìš°ì„ 
                    data_path = os.path.join(post_dir, files[0])
                    logger.info(f"Structura post ë°ì´í„° íŒŒì¼ ë°œê²¬: {data_path}")
            
            # post íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë°ì´í„° ì‚¬ìš©
            if data_path is None:
                # ê¸°ë³¸ ë°ì´í„° ê²½ë¡œ (fallback)
                default_data_path = "../data/IBM_HR.csv"
                if os.path.exists(default_data_path):
                    data_path = default_data_path
                    logger.info(f"ê¸°ë³¸ ë°ì´í„° íŒŒì¼ ì‚¬ìš©: {data_path}")
                else:
                    logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # ì˜ˆì¸¡ê¸° ì´ˆê¸°í™”
            predictor = StructuraHRPredictor(data_path=data_path)
            
            # ê¸°ì¡´ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë¡œë”©, ì—†ìœ¼ë©´ ì¦‰ì‹œ í›ˆë ¨
            model_path = "hr_attrition_model.pkl"
            if os.path.exists(model_path):
                try:
                    predictor.load_model(model_path)
                    logger.info("ê¸°ì¡´ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"ê¸°ì¡´ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}, ìƒˆë¡œ í›ˆë ¨í•©ë‹ˆë‹¤")
                    # ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ ìƒˆë¡œ í›ˆë ¨
                    if data_path:
                        logger.info("ğŸš€ ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
                        training_result = predictor.run_full_pipeline(optimize_hp=True, use_sampling=True)
                        predictor.save_model(model_path)
                        logger.info(f"âœ… ì´ˆê¸° ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {training_result}")
            else:
                # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì¦‰ì‹œ í›ˆë ¨
                if data_path:
                    logger.info("ğŸš€ ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
                    training_result = predictor.run_full_pipeline(optimize_hp=True, use_sampling=True)
                    predictor.save_model(model_path)
                    logger.info(f"âœ… ì´ˆê¸° ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {training_result}")
                else:
                    logger.warning("í›ˆë ¨í•  ë°ì´í„°ê°€ ì—†ì–´ì„œ ëª¨ë¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # Flask ì•±ì— ì €ì¥
            app.predictor = predictor
            
            logger.info("Structura HR ì˜ˆì¸¡ ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise
    
    # ì•± ìƒì„± ì‹œ ì¦‰ì‹œ ì´ˆê¸°í™”
    initialize_services()
    
    # ------------------------------------------------------
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
    # ------------------------------------------------------
    
    def get_predictor():
        """ì˜ˆì¸¡ê¸° ê°€ì ¸ì˜¤ê¸°"""
        if not hasattr(app, 'predictor') or app.predictor is None:
            return None
        return app.predictor
    
    # ------------------------------------------------------
    # ì—ëŸ¬ í•¸ë“¤ëŸ¬
    # ------------------------------------------------------
    
    @app.errorhandler(404)
    def not_found(error):
        return jsonify({
            "error": "Not Found",
            "message": "ìš”ì²­í•œ ë¦¬ì†ŒìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "status_code": 404
        }), 404
    
    @app.errorhandler(500)
    def internal_error(error):
        return jsonify({
            "error": "Internal Server Error",
            "message": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
            "status_code": 500
        }), 500
    
    @app.errorhandler(Exception)
    def handle_exception(e):
        if isinstance(e, HTTPException):
            return jsonify({
                "error": e.name,
                "message": e.description,
                "status_code": e.code
            }), e.code
        
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            "error": "Unexpected Error",
            "message": str(e),
            "status_code": 500
        }), 500
    
    # ------------------------------------------------------
    # API ë¼ìš°íŠ¸
    # ------------------------------------------------------
    
    @app.route('/')
    def index():
        """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
        return jsonify({
            "service": "Structura HR ì´ì§ ì˜ˆì¸¡ Flask API",
            "version": "1.0.0",
            "status": "running",
            "description": "XGBoost + xAI ê¸°ë°˜ ì„¤ëª… ê°€ëŠ¥í•œ HR ì´ì§ ì˜ˆì¸¡ ì„œë¹„ìŠ¤",
            "features": {
                "machine_learning": "XGBoost",
                "explainable_ai": ["SHAP", "LIME", "Feature Importance"],
                "react_integration": True
            },
            "endpoints": {
                "health": "/api/health",
                "train_model": "/api/train",
                "predict": "/api/predict",
                "explain": "/api/explain",
                "feature_importance": "/api/feature-importance",
                "model_info": "/api/model/info",
                "predict_batch": "/api/predict/batch",
                "employee_analysis": "/api/employee/analysis"
            }
        })
    
    @app.route('/api/health')
    def health_check():
        """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
        
        predictor = get_predictor()
        
        model_status = "loaded" if (predictor and predictor.model) else "not_loaded"
        xai_status = {
            "shap_available": SHAP_AVAILABLE and (predictor and predictor.shap_explainer is not None),
            "lime_available": LIME_AVAILABLE and (predictor and predictor.lime_explainer is not None)
        }
        
        return jsonify({
            "status": "healthy",
            "model_status": model_status,
            "xai_status": xai_status,
            "dependencies": {
                "shap": SHAP_AVAILABLE,
                "lime": LIME_AVAILABLE,
                "optuna": OPTUNA_AVAILABLE
            },
            "timestamp": datetime.now().isoformat()
        })
    
    @app.route('/api/upload/data', methods=['POST'])
    def upload_data():
        """HR ë°ì´í„° CSV íŒŒì¼ ì—…ë¡œë“œ"""
        try:
            # íŒŒì¼ í™•ì¸
            if 'file' not in request.files:
                return jsonify({
                    "success": False,
                    "error": "íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                }), 400
            
            file = request.files['file']
            if file.filename == '':
                return jsonify({
                    "success": False,
                    "error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                }), 400
            
            # íŒŒì¼ í™•ì¥ì í™•ì¸
            if not file.filename.lower().endswith('.csv'):
                return jsonify({
                    "success": False,
                    "error": "CSV íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
                }), 400
            
            # íŒŒì¼ ì €ì¥
            filename = secure_filename(file.filename)
            upload_dir = os.path.join(os.path.dirname(__file__), '..', 'uploads', 'Structura')
            os.makedirs(upload_dir, exist_ok=True)
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•œ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_name = os.path.splitext(filename)[0]
            new_filename = f"{base_name}_{timestamp}.csv"
            file_path = os.path.join(upload_dir, new_filename)
            
            # ìµœì‹  íŒŒì¼ ë§í¬ë„ ìƒì„± (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±ì„ ìœ„í•´)
            latest_link = os.path.join(upload_dir, 'latest_hr_data.csv')
            file.save(file_path)
            
            # ìµœì‹  íŒŒì¼ ë§í¬ ìƒì„± (ì‹¬ë³¼ë¦­ ë§í¬ ë˜ëŠ” ë³µì‚¬)
            try:
                if os.path.exists(latest_link):
                    os.remove(latest_link)
                # Windowsì—ì„œëŠ” ì‹¬ë³¼ë¦­ ë§í¬ ëŒ€ì‹  ë³µì‚¬ ì‚¬ìš©
                import shutil
                shutil.copy2(file_path, latest_link)
            except Exception as e:
                logger.warning(f"ìµœì‹  íŒŒì¼ ë§í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # ë°ì´í„° ê²€ì¦
            try:
                df = pd.read_csv(file_path)
                
                # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
                required_columns = ['Age', 'JobSatisfaction', 'OverTime', 'MonthlyIncome', 'WorkLifeBalance']
                missing_columns = [col for col in required_columns if col not in df.columns]
                
                if missing_columns:
                    return jsonify({
                        "success": False,
                        "error": f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing_columns)}",
                        "required_columns": required_columns,
                        "found_columns": list(df.columns)
                    }), 400
                
                # ê¸°ì¡´ predictor ì´ˆê¸°í™” (ìƒˆ ë°ì´í„°ë¡œ ì¬í›ˆë ¨ í•„ìš”)
                global predictor_instance
                predictor_instance = None
                
                return jsonify({
                    "success": True,
                    "message": "ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.",
                    "file_info": {
                        "original_filename": filename,
                        "saved_filename": new_filename,
                        "upload_path": upload_dir,
                        "file_path": file_path,
                        "latest_link": latest_link,
                        "rows": len(df),
                        "columns": len(df.columns),
                        "column_names": list(df.columns)
                    },
                    "note": "ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ì¬í›ˆë ¨í•´ì£¼ì„¸ìš”."
                })
                
            except Exception as e:
                return jsonify({
                    "success": False,
                    "error": f"ë°ì´í„° íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}"
                }), 400
                
        except Exception as e:
            return jsonify({
                "success": False,
                "error": f"íŒŒì¼ ì—…ë¡œë“œ ì˜¤ë¥˜: {str(e)}"
            }), 500
    
    @app.route('/api/train', methods=['POST'])
    def train_model():
        """ëª¨ë¸ í›ˆë ¨ ì—”ë“œí¬ì¸íŠ¸ (ë…¸íŠ¸ë¶ ê¸°ë°˜ ìµœì‹  ë²„ì „)"""
        
        predictor = get_predictor()
        if not predictor:
            return jsonify({"error": "ì˜ˆì¸¡ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}), 503
        
        try:
            # ìš”ì²­ ë°ì´í„° íŒŒì‹±
            data = request.get_json() or {}
            optimize_hp = data.get('optimize_hyperparameters', True)
            use_sampling = data.get('use_sampling', True)
            
            logger.info(f"ëª¨ë¸ í›ˆë ¨ ì‹œì‘ - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”: {optimize_hp}, ìƒ˜í”Œë§: {use_sampling}")
            
            # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            metrics = predictor.run_full_pipeline(
                optimize_hp=optimize_hp,
                use_sampling=use_sampling
            )
            
            # ëª¨ë¸ ì €ì¥
            predictor.save_model("hr_attrition_model_xai.pkl")
            
            # í”¼ì²˜ ì¤‘ìš”ë„ ê°€ì ¸ì˜¤ê¸°
            importance_df = predictor.get_feature_importance(10)
            feature_importance = [
                {"feature": row['feature'], "importance": float(row['importance'])}
                for _, row in importance_df.iterrows()
            ]
            
            return jsonify({
                "status": "success",
                "message": "ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ (XAI í¬í•¨)",
                "metrics": {k: float(v) if isinstance(v, (int, float)) else v for k, v in metrics.items()},
                "feature_importance": feature_importance,
                "xai_enabled": {
                    "shap": predictor.shap_explainer is not None,
                    "feature_importance": True
                },
                "model_config": {
                    "optimal_threshold": predictor.optimal_threshold,
                    "features_count": len(predictor.feature_columns) if predictor.feature_columns else 0,
                    "sampling_used": use_sampling
                },
                "timestamp": datetime.now().isoformat()
            })
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {str(e)}")
            return jsonify({"error": f"ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {str(e)}"}), 500
    
    @app.route('/api/predict', methods=['POST'])
    def predict_attrition():
        """ì´ì§ ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸ (Probability ì¤‘ì‹¬ + XAI)"""
        
        predictor = get_predictor()
        if not predictor or not predictor.model:
            return jsonify({"error": "ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ê±°ë‚˜ /api/train ì—”ë“œí¬ì¸íŠ¸ë¡œ ëª¨ë¸ì„ í›ˆë ¨í•´ì£¼ì„¸ìš”."}), 503
        
        try:
            # ìš”ì²­ ë°ì´í„° íŒŒì‹±
            data = request.get_json()
            if not data:
                return jsonify({"error": "ì˜ˆì¸¡í•  ì§ì› ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400
            
            logger.info(f"ğŸ”® Structura ì˜ˆì¸¡ ìš”ì²­ ë°›ìŒ: {type(data)} íƒ€ì…")
            
            # employee_idë§Œ ìˆëŠ” ê²½ìš° batch ë°ì´í„°ì—ì„œ ì°¾ê¸°
            if 'employee_id' in data and len(data) == 1:
                employee_id = data['employee_id']
                logger.info(f"ğŸ‘¤ ë‹¨ì¼ ì§ì› IDë¡œ ì˜ˆì¸¡ ìš”ì²­: {employee_id}")
                
                # batch ë°ì´í„°ì—ì„œ í•´ë‹¹ ì§ì› ì°¾ê¸°
                batch_dir = "../uploads/structura/batch"
                if os.path.exists(batch_dir):
                    files = [f for f in os.listdir(batch_dir) if f.endswith('.csv')]
                    if files:
                        files.sort(reverse=True)  # ìµœì‹  íŒŒì¼ ìš°ì„ 
                        batch_file = os.path.join(batch_dir, files[0])
                        logger.info(f"ğŸ“‚ ë°°ì¹˜ íŒŒì¼ì—ì„œ ì§ì› ê²€ìƒ‰: {batch_file}")
                        
                        import pandas as pd
                        df = pd.read_csv(batch_file)
                        
                        # employee_id ë˜ëŠ” EmployeeNumberë¡œ ì°¾ê¸°
                        employee_row = None
                        if 'employee_id' in df.columns:
                            employee_row = df[df['employee_id'] == int(employee_id)]
                        elif 'EmployeeNumber' in df.columns:
                            employee_row = df[df['EmployeeNumber'] == int(employee_id)]
                        
                        if employee_row is not None and len(employee_row) > 0:
                            # ì²« ë²ˆì§¸ ë§¤ì¹­ í–‰ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                            employee_data = employee_row.iloc[0].to_dict()
                            employee_number = employee_data.get('EmployeeNumber', employee_id)
                            logger.info(f"âœ… ì§ì› {employee_number} ë°ì´í„° ë°œê²¬, ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
                            result = predictor.predict_single_employee(employee_data, str(employee_number))
                            logger.info(f"ğŸ¯ ì§ì› {employee_number} ì˜ˆì¸¡ ì™„ë£Œ: ìœ„í—˜ë„ {result.get('attrition_probability', 0):.3f}")
                            return jsonify(result)
                        else:
                            logger.warning(f"âŒ ì§ì› ID {employee_id}ë¥¼ batch ë°ì´í„°ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                            return jsonify({"error": f"ì§ì› ID {employee_id}ë¥¼ batch ë°ì´í„°ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}), 404
                    else:
                        logger.error("âŒ batch ë°ì´í„° íŒŒì¼ì´ ì—†ìŒ")
                        return jsonify({"error": "batch ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤"}), 404
                else:
                    logger.error("âŒ batch ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŒ")
                    return jsonify({"error": "batch ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤"}), 404
            
            # ë‹¨ì¼ ì§ì› ë°ì´í„°ì¸ì§€ í™•ì¸
            elif isinstance(data, list):
                # ë°°ì¹˜ ì˜ˆì¸¡
                logger.info(f"ğŸ“Š ë°°ì¹˜ ì˜ˆì¸¡ ìš”ì²­: {len(data)}ëª…")
                results = []
                for i, employee_data in enumerate(data):
                    employee_number = employee_data.get('EmployeeNumber', str(i+1))
                    logger.info(f"ğŸ”® ë°°ì¹˜ ì˜ˆì¸¡ {i+1}/{len(data)}: ì§ì› {employee_number}")
                    result = predictor.predict_single_employee(employee_data, employee_number)
                    logger.info(f"âœ… ì§ì› {employee_number} ì˜ˆì¸¡ ì™„ë£Œ: ìœ„í—˜ë„ {result.get('attrition_probability', 0):.3f}")
                    results.append(result)
                
                logger.info(f"ğŸ¯ ë°°ì¹˜ ì˜ˆì¸¡ ì „ì²´ ì™„ë£Œ: {len(results)}ëª…")
                return jsonify({
                    "predictions": results,
                    "batch_size": len(results),
                    "timestamp": datetime.now().isoformat()
                })
            else:
                # ë‹¨ì¼ ì˜ˆì¸¡
                employee_number = data.get('EmployeeNumber', 'SINGLE_001')
                logger.info(f"ğŸ‘¤ ë‹¨ì¼ ì§ì› ì˜ˆì¸¡ ìš”ì²­: {employee_number}")
                result = predictor.predict_single_employee(data, employee_number)
                logger.info(f"ğŸ¯ ì§ì› {employee_number} ì˜ˆì¸¡ ì™„ë£Œ: ìœ„í—˜ë„ {result.get('attrition_probability', 0):.3f}")
                return jsonify(result)
                
        except Exception as e:
            logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
            return jsonify({"error": f"ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}"}), 500
    
    @app.route('/api/explain', methods=['POST'])
    def explain_prediction():
        """ì˜ˆì¸¡ ì„¤ëª… ì—”ë“œí¬ì¸íŠ¸ (SHAP ê¸°ë°˜ XAI)"""
        
        predictor = get_predictor()
        if not predictor or not predictor.model:
            return jsonify({"error": "ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}), 503
        
        try:
            # ìš”ì²­ ë°ì´í„° íŒŒì‹±
            data = request.get_json()
            if not data:
                return jsonify({"error": "ì„¤ëª…í•  ì§ì› ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400
            
            # EmployeeNumber ì¶”ì¶œ
            employee_number = data.get('EmployeeNumber', 'EXPLAIN_001')
            
            # DataFrameìœ¼ë¡œ ë³€í™˜
            df = pd.DataFrame([data])
            
            # ì˜ˆì¸¡ ì„¤ëª… ìƒì„±
            explanation = predictor.explain_prediction(df, employee_number)
            
            return jsonify(explanation)
            
        except Exception as e:
            logger.error(f"ì˜ˆì¸¡ ì„¤ëª… ì‹¤íŒ¨: {str(e)}")
            return jsonify({"error": f"ì˜ˆì¸¡ ì„¤ëª… ì‹¤íŒ¨: {str(e)}"}), 500
    
    @app.route('/api/feature-importance')
    def get_feature_importance():
        """í”¼ì²˜ ì¤‘ìš”ë„ ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸"""
        
        predictor = get_predictor()
        if not predictor or not predictor.model:
            return jsonify({"error": "ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}), 503
        
        try:
            top_n = request.args.get('top_n', 20, type=int)
            
            importance_df = predictor.get_feature_importance(top_n)
            
            # DataFrameì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            importance_data = [
                {
                    "feature": row['feature'],
                    "importance": float(row['importance']),
                    "rank": i + 1
                }
                for i, (_, row) in enumerate(importance_df.iterrows())
            ]
            
            return jsonify({
                "feature_importance": importance_data,
                "total_features": len(predictor.feature_columns) if predictor.feature_columns else 0,
                "top_n": top_n,
                "timestamp": datetime.now().isoformat()
            })
            
        except Exception as e:
            logger.error(f"í”¼ì²˜ ì¤‘ìš”ë„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return jsonify({"error": f"í”¼ì²˜ ì¤‘ìš”ë„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"}), 500
    
    @app.route('/api/model/info')
    def get_model_info():
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸"""
        
        predictor = get_predictor()
        if not predictor:
            return jsonify({"error": "ì˜ˆì¸¡ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}), 503
        
        model_info = {
            "model_loaded": predictor.model is not None,
            "feature_count": len(predictor.feature_columns) if predictor.feature_columns else 0,
            "feature_columns": predictor.feature_columns,
            "optimal_threshold": predictor.optimal_threshold,
            "scale_pos_weight": predictor.scale_pos_weight,
            "xai_capabilities": {
                "shap_available": predictor.shap_explainer is not None,
                "lime_available": predictor.lime_explainer is not None,
                "feature_importance": True
            },
            "preprocessing_config": {
                "dropped_columns": predictor.DROP_COLS,
                "ordinal_columns": list(predictor.ORDINAL_MAPS.keys()),
                "nominal_columns": predictor.NOMINAL_COLS
            }
        }
        
        return jsonify(model_info)
    
    @app.route('/api/predict/batch', methods=['POST', 'OPTIONS'])
    def predict_batch():
        """ë°°ì¹˜ ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸ (ì—¬ëŸ¬ ì§ì› ë™ì‹œ ì²˜ë¦¬)"""
        
        # OPTIONS ìš”ì²­ ì²˜ë¦¬
        if request.method == 'OPTIONS':
            response = jsonify({})
            response.headers.add("Access-Control-Allow-Origin", "*")
            response.headers.add('Access-Control-Allow-Headers', "*")
            response.headers.add('Access-Control-Allow-Methods', "*")
            return response
        
        logger.info("ğŸš€ Structura ë°°ì¹˜ ì˜ˆì¸¡ API í˜¸ì¶œë¨")
        
        predictor = get_predictor()
        if not predictor:
            logger.error("âŒ ì˜ˆì¸¡ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return jsonify({"error": "ì˜ˆì¸¡ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}), 503
        
        try:
            # ìš”ì²­ ë°ì´í„° íŒŒì‹±
            request_data = request.get_json()
            if not request_data:
                logger.error("âŒ ìš”ì²­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return jsonify({"error": "ìš”ì²­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}), 400
            
            # ë¶„ì„ íƒ€ì… í™•ì¸ (ê¸°ë³¸ê°’: batch)
            analysis_type = request_data.get('analysis_type', 'batch')
            logger.info(f"ğŸ“Š ë¶„ì„ íƒ€ì…: {analysis_type}")
            
            # ì‚¬í›„ ë¶„ì„(post)ì¸ ê²½ìš°: post ë°ì´í„°ë§Œ ì‚¬ìš©
            if analysis_type == 'post':
                logger.info("ğŸ” ì‚¬í›„ ë¶„ì„ ëª¨ë“œ: post ë°ì´í„° í™•ì¸ ì¤‘...")
                post_data_path = get_data_path_by_analysis_type('post')
                if not post_data_path:
                    logger.error("âŒ Post ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                    return jsonify({"error": "Post ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì‚¬í›„ ë¶„ì„ìš© ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."}), 400
                
                if post_data_path != predictor.data_path:
                    logger.info(f"ğŸ”„ ì‚¬í›„ ë¶„ì„ìš© ë°ì´í„°ë¡œ ì˜ˆì¸¡ê¸° ì—…ë°ì´íŠ¸: {post_data_path}")
                    predictor.data_path = post_data_path
                    try:
                        predictor.load_data()
                        logger.info(f"âœ… Post ë°ì´í„° ì¬ë¡œë“œ ì™„ë£Œ")
                    except Exception as e:
                        logger.error(f"âŒ Post ë°ì´í„° ì¬ë¡œë“œ ì‹¤íŒ¨: {e}")
                        return jsonify({"error": f"Post ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}"}), 500
                else:
                    logger.info(f"âœ… ê¸°ì¡´ Post ë°ì´í„° ê²½ë¡œ ì‚¬ìš©: {predictor.data_path}")
            
            # ë°°ì¹˜ ë¶„ì„(batch)ì¸ ê²½ìš°: ë³„ë„ í•¨ìˆ˜ì—ì„œ ì²˜ë¦¬ (postë¡œ í•™ìŠµ â†’ batchë¡œ ì˜ˆì¸¡)
            elif analysis_type == 'batch':
                logger.info("ğŸ”€ ë°°ì¹˜ ë¶„ì„ì€ ë³„ë„ ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
                return jsonify({"error": "ë°°ì¹˜ ë¶„ì„ì€ /api/batch-analysis ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”."}), 400
            
            # employees í‚¤ê°€ ìˆëŠ” ê²½ìš°ì™€ ì§ì ‘ ë°°ì—´ì¸ ê²½ìš° ëª¨ë‘ ì²˜ë¦¬
            if isinstance(request_data, dict) and 'employees' in request_data:
                data = request_data['employees']
                logger.info(f"ğŸ“‹ employees í‚¤ì—ì„œ {len(data)}ëª… ë°ì´í„° ì¶”ì¶œ")
            elif isinstance(request_data, list):
                data = request_data
                logger.info(f"ğŸ“‹ ì§ì ‘ ë°°ì—´ì—ì„œ {len(data)}ëª… ë°ì´í„° ì¶”ì¶œ")
            else:
                logger.error("âŒ ë°°ì¹˜ ì˜ˆì¸¡ì„ ìœ„í•œ ì§ì› ë°ì´í„° ë¦¬ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤")
                return jsonify({"error": "ë°°ì¹˜ ì˜ˆì¸¡ì„ ìœ„í•œ ì§ì› ë°ì´í„° ë¦¬ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400
            
            if not isinstance(data, list):
                logger.error("âŒ ì§ì› ë°ì´í„°ëŠ” ë°°ì—´ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤")
                return jsonify({"error": "ì§ì› ë°ì´í„°ëŠ” ë°°ì—´ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤"}), 400
            
            # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë°›ì€ ë°ì´í„°ë¡œ ì¦‰ì‹œ í›ˆë ¨
            if not predictor.model:
                logger.info("ëª¨ë¸ì´ ì—†ì–´ì„œ ë°›ì€ ë°ì´í„°ë¡œ ì¦‰ì‹œ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤")
                
                # ë°›ì€ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                import pandas as pd
                df = pd.DataFrame(data)
                
                # Attrition ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                if 'Attrition' not in df.columns:
                    return jsonify({"error": "í›ˆë ¨ì„ ìœ„í•´ì„œëŠ” Attrition ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤"}), 400
                
                # ì„ì‹œë¡œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  í›ˆë ¨
                temp_data_path = "temp_training_data.csv"
                df.to_csv(temp_data_path, index=False)
                
                # ì˜ˆì¸¡ê¸°ì˜ ë°ì´í„° ê²½ë¡œë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ë³€ê²½
                predictor.data_path = temp_data_path
                
                try:
                    # ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰
                    metrics = predictor.run_full_pipeline(optimize_hp=True, use_sampling=True)
                    logger.info(f"ì¦‰ì‹œ í›ˆë ¨ ì™„ë£Œ: {metrics}")
                except Exception as train_error:
                    logger.error(f"ì¦‰ì‹œ í›ˆë ¨ ì‹¤íŒ¨: {str(train_error)}")
                    return jsonify({"error": f"ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {str(train_error)}"}), 500
                finally:
                    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                    import os
                    if os.path.exists(temp_data_path):
                        os.remove(temp_data_path)
            
            results = []
            for i, employee_data in enumerate(data):
                try:
                    employee_number = employee_data.get('EmployeeNumber', str(i+1))
                    result = predictor.predict_single_employee(employee_data, employee_number)
                    results.append(result)
                except Exception as e:
                    # ê°œë³„ ì˜ˆì¸¡ ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ì •ë³´ í¬í•¨
                    results.append({
                        'employee_number': employee_data.get('EmployeeNumber', str(i+1)),
                        'error': str(e),
                        'attrition_probability': None,
                        'risk_category': 'ERROR'
                    })
            
            # í†µê³„ ì •ë³´ ê³„ì‚°
            successful_predictions = [r for r in results if 'error' not in r]
            if successful_predictions:
                probabilities = [r['attrition_probability'] for r in successful_predictions]
                risk_distribution = {}
                for r in successful_predictions:
                    risk_cat = r['risk_category']
                    risk_distribution[risk_cat] = risk_distribution.get(risk_cat, 0) + 1
                
                stats = {
                    'total_employees': len(data),
                    'successful_predictions': len(successful_predictions),
                    'failed_predictions': len(data) - len(successful_predictions),
                    'average_probability': sum(probabilities) / len(probabilities),
                    'risk_distribution': risk_distribution,
                    'high_risk_count': risk_distribution.get('HIGH', 0)
                }
            else:
                stats = {
                    'total_employees': len(data),
                    'successful_predictions': 0,
                    'failed_predictions': len(data),
                    'average_probability': None,
                    'risk_distribution': {},
                    'high_risk_count': 0
                }
            
            return jsonify({
                "predictions": results,
                "statistics": stats,
                "timestamp": datetime.now().isoformat()
            })
            
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
            logger.error(f"âŒ ì˜¤ë¥˜ ìƒì„¸: {type(e).__name__}")
            import traceback
            logger.error(f"âŒ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
            return jsonify({
                "error": f"ë°°ì¹˜ ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}",
                "error_type": type(e).__name__,
                "details": "ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
            }), 500
    
    @app.route('/api/employee/analysis/<employee_number>', methods=['POST'])
    def employee_analysis(employee_number):
        """ê°œë³„ ì§ì› ì‹¬ì¸µ ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸"""
        
        predictor = get_predictor()
        if not predictor or not predictor.model:
            return jsonify({"error": "ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}), 503
        
        try:
            # ìš”ì²­ ë°ì´í„° íŒŒì‹±
            data = request.get_json()
            if not data:
                return jsonify({"error": "ì§ì› ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400
            
            # ì˜ˆì¸¡ ë° ì„¤ëª…
            result = predictor.predict_single_employee(data, employee_number)
            
            # ì¶”ê°€ ë¶„ì„ ì •ë³´
            df = pd.DataFrame([data])
            probability = predictor.predict(df, return_proba=True)[0]
            
            # ìœ„í—˜ë„ ë ˆë²¨ë³„ ì„ê³„ê°’ ì •ë³´
            risk_thresholds = {
                'LOW': 0.4,
                'MEDIUM': 0.7,
                'HIGH': 1.0
            }
            
            # í˜„ì¬ ìœ„í—˜ë„ì™€ ë‹¤ìŒ ë‹¨ê³„ê¹Œì§€ì˜ ê±°ë¦¬
            current_risk = result['risk_category']
            if current_risk == 'LOW':
                next_threshold = risk_thresholds['LOW']
                distance_to_next = next_threshold - probability
            elif current_risk == 'MEDIUM':
                next_threshold = risk_thresholds['MEDIUM']
                distance_to_next = next_threshold - probability
            else:
                next_threshold = None
                distance_to_next = None
            
            # ì‹¬ì¸µ ë¶„ì„ ê²°ê³¼
            analysis = {
                **result,
                'detailed_analysis': {
                    'probability_score': float(probability),
                    'risk_thresholds': risk_thresholds,
                    'current_risk_level': current_risk,
                    'distance_to_next_level': float(distance_to_next) if distance_to_next else None,
                    'percentile_rank': None  # ì „ì²´ ì§ì› ëŒ€ë¹„ ìˆœìœ„ (ì¶”í›„ êµ¬í˜„)
                },
                'recommendations': predictor._generate_recommendations(result),
                'timestamp': datetime.now().isoformat()
            }
            
            return jsonify(analysis)
            
        except Exception as e:
            logger.error(f"ì§ì› ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return jsonify({"error": f"ì§ì› ë¶„ì„ ì‹¤íŒ¨: {str(e)}"}), 500
    
    # ë°°ì¹˜ ë¶„ì„ ë¼ìš°íŠ¸ ì¶”ê°€
    @app.route('/api/batch-analysis', methods=['POST'])
    def batch_analysis_route():
        return batch_analysis()
    
    return app

# ------------------------------------------------------
# ë°°ì¹˜ ë¶„ì„ ì „ìš© ì—”ë“œí¬ì¸íŠ¸
# ------------------------------------------------------

def batch_analysis():
    """ë°°ì¹˜ ë¶„ì„: Post ë°ì´í„°ë¡œ í•™ìŠµ â†’ Batch ë°ì´í„°ë¡œ ì˜ˆì¸¡ (ëª¨ë¸ ìºì‹± ì ìš©)"""
    from flask import current_app
    
    # ëª¨ë¸ ìºì‹œ (í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì •ì˜)
    if not hasattr(current_app, 'model_cache'):
        current_app.model_cache = {
            'trained_model': None,
            'model_metadata': None,
            'training_timestamp': None,
            'data_hash': None  # ë°ì´í„° ë³€ê²½ ê°ì§€ìš©
        }
    model_cache = current_app.model_cache
    
    try:
        logger.info("ğŸš€ Structura ë°°ì¹˜ ë¶„ì„ ì‹œì‘: Post ë°ì´í„° í•™ìŠµ â†’ Batch ë°ì´í„° ì˜ˆì¸¡")
        
        # 1ë‹¨ê³„: Post ë°ì´í„° í™•ì¸
        post_data_path = get_data_path_by_analysis_type('post')
        if not post_data_path:
            return jsonify({"error": "Post ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì‚¬í›„ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”."}), 400
        
        # ë°ì´í„° í•´ì‹œ ê³„ì‚° (ë³€ê²½ ê°ì§€ìš©)
        import hashlib
        import os
        with open(post_data_path, 'rb') as f:
            data_hash = hashlib.md5(f.read()).hexdigest()
        
        # Flask appì˜ predictor ê°€ì ¸ì˜¤ê¸°
        predictor = getattr(current_app, 'predictor', None)
        
        # ìºì‹œëœ ëª¨ë¸ í™•ì¸
        if (model_cache['trained_model'] is not None and 
            model_cache['data_hash'] == data_hash and
            predictor is not None and predictor.model is not None):
            logger.info("ğŸ’¾ ìºì‹œëœ ìµœì í™” ëª¨ë¸ ì‚¬ìš© (ì¬í•™ìŠµ ìƒëµ)")
            training_result = model_cache['model_metadata']
        else:
            logger.info(f"ğŸ“š Post ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ: {post_data_path}")
            
            # ì˜ˆì¸¡ê¸° ì´ˆê¸°í™” (Post ë°ì´í„°ë¡œ)
            predictor = StructuraHRPredictor(data_path=post_data_path)
            predictor.load_data()
            
            # ëª¨ë¸ í•™ìŠµ (Post ë°ì´í„°ë¡œ í•œ ë²ˆë§Œ ìµœì í™” ìˆ˜í–‰í•˜ì—¬ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìƒì„±)
            training_result = predictor.run_full_pipeline(optimize_hp=True, use_sampling=True)
            
            # Flask appì— predictor ì €ì¥
            current_app.predictor = predictor
            
            # ëª¨ë¸ ìºì‹œì— ì €ì¥
            from datetime import datetime
            model_cache['trained_model'] = predictor.model
            model_cache['model_metadata'] = training_result
            model_cache['training_timestamp'] = datetime.now().isoformat()
            model_cache['data_hash'] = data_hash
            
            logger.info(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ë° ìºì‹œ ì €ì¥: {training_result}")
        
        
        # 2ë‹¨ê³„: Batch ë°ì´í„°ë¡œ ì˜ˆì¸¡
        batch_data_path = get_data_path_by_analysis_type('batch')
        if not batch_data_path:
            return jsonify({"error": "Batch ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°°ì¹˜ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."}), 400
        
        logger.info(f"ğŸ”® Batch ë°ì´í„°ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰: {batch_data_path}")
        
        # Batch ë°ì´í„° ë¡œë“œ
        import pandas as pd
        batch_data = pd.read_csv(batch_data_path)
        logger.info(f"ğŸ“Š Batch ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(batch_data)}ëª…")
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        predictions = []
        for idx, row in batch_data.iterrows():
            try:
                # ê°œë³„ ì§ì› ì˜ˆì¸¡
                employee_data = row.to_dict()
                employee_number = employee_data.get('EmployeeNumber', str(idx+1))
                prediction = predictor.predict_single_employee(employee_data, str(employee_number))
                
                predictions.append({
                    'employee_id': employee_data.get('EmployeeNumber', idx),
                    'risk_score': prediction.get('attrition_probability', 0.5),
                    'predicted_attrition': prediction.get('predicted_attrition', 0),
                    'confidence': prediction.get('confidence', 0.7),
                    'feature_importance': prediction.get('feature_importance', {}),
                    'xai_explanation': prediction.get('xai_explanation', {}),
                    'shap_values': prediction.get('shap_values', {}),
                    'lime_explanation': prediction.get('lime_explanation', {}),
                    'model_interpretation': prediction.get('model_interpretation', {}),
                    'employee_data': employee_data
                })
                
            except Exception as e:
                logger.warning(f"ì§ì› {idx} ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
                predictions.append({
                    'employee_id': employee_data.get('EmployeeNumber', idx),
                    'risk_score': 0.5,
                    'predicted_attrition': 0,
                    'confidence': 0.1,
                    'error': str(e),
                    'employee_data': employee_data
                })
        
        logger.info(f"âœ… Structura ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: {len(predictions)}ëª… ì˜ˆì¸¡")
        
        return jsonify({
            "success": True,
            "message": f"Structura ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: Post ë°ì´í„° í•™ìŠµ â†’ {len(predictions)}ëª… Batch ì˜ˆì¸¡",
            "agent": "structura",
            "training_data_path": post_data_path,
            "prediction_data_path": batch_data_path,
            "total_predictions": len(predictions),
            "predictions": predictions,
            "model_info": {
                "training_samples": len(predictor.data) if predictor.data is not None else 0,
                "features_used": list(predictor.feature_columns) if hasattr(predictor, 'feature_columns') else [],
                "model_type": "XGBoost with Hyperparameter Optimization"
            }
        })
        
    except Exception as e:
        logger.error(f"Structura ë°°ì¹˜ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        return jsonify({"error": f"Structura ë°°ì¹˜ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"}), 500

# ------------------------------------------------------
# ì„œë²„ ì‹¤í–‰ í•¨ìˆ˜
# ------------------------------------------------------

def run_server(host='0.0.0.0', port=5001, debug=False):
    """Flask ì„œë²„ ì‹¤í–‰ (XAI í¬í•¨ ìµœì‹  ë²„ì „)"""
    try:
        app = create_app()
        
        print("=" * 70)
        print("ğŸš€ Structura HR ì˜ˆì¸¡ Flask ë°±ì—”ë“œ ì„œë²„ ì‹œì‘ (XAI í¬í•¨)")
        print("=" * 70)
        print(f"ğŸ“¡ ì„œë²„ ì£¼ì†Œ: http://{host}:{port}")
        print(f"ğŸ”— React ì—°ë™: http://localhost:3000ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥")
        print(f"ğŸ¤– XAI ê¸°ëŠ¥: SHAP ê¸°ë°˜ ì„¤ëª… ê°€ëŠ¥í•œ AI")
        print(f"ğŸ”„ ë””ë²„ê·¸ ëª¨ë“œ: {'í™œì„±í™”' if debug else 'ë¹„í™œì„±í™”'}")
        print()
        print("ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸:")
        print(f"  â€¢ í—¬ìŠ¤ì²´í¬: http://{host}:{port}/api/health")
        print(f"  â€¢ ë°ì´í„° ì—…ë¡œë“œ: http://{host}:{port}/api/upload/data")
        print(f"  â€¢ ëª¨ë¸ í›ˆë ¨: http://{host}:{port}/api/train")
        print(f"  â€¢ ì´ì§ ì˜ˆì¸¡: http://{host}:{port}/api/predict")
        print(f"  â€¢ ë°°ì¹˜ ì˜ˆì¸¡: http://{host}:{port}/api/predict/batch")
        print(f"  â€¢ ì˜ˆì¸¡ ì„¤ëª…: http://{host}:{port}/api/explain")
        print(f"  â€¢ ì§ì› ë¶„ì„: http://{host}:{port}/api/employee/analysis/<employee_number>")
        print(f"  â€¢ í”¼ì²˜ ì¤‘ìš”ë„: http://{host}:{port}/api/feature-importance")
        print(f"  â€¢ ëª¨ë¸ ì •ë³´: http://{host}:{port}/api/model/info")
        print()
        print("XAI ê¸°ëŠ¥:")
        print(f"  â€¢ SHAP: {'âœ…' if SHAP_AVAILABLE else 'âŒ'}")
        print(f"  â€¢ Feature Importance: âœ…")
        print(f"  â€¢ Optuna: {'âœ…' if OPTUNA_AVAILABLE else 'âŒ'}")
        print()
        print("ìƒˆë¡œìš´ ê¸°ëŠ¥:")
        print("  â€¢ EmployeeNumberë³„ ê°œë³„ XAI ì„¤ëª…")
        print("  â€¢ Probability ì¤‘ì‹¬ ì˜ˆì¸¡ ê²°ê³¼")
        print("  â€¢ ë°°ì¹˜ ì²˜ë¦¬ ë° í†µê³„ ë¶„ì„")
        print("  â€¢ ìœ„í—˜ë„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ì œê³µ")
        print()
        print("ì„œë²„ë¥¼ ì¤‘ì§€í•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
        print("=" * 70)
        
        app.run(host=host, port=port, debug=debug)
        
    except Exception as e:
        print(f"âŒ ì„œë²„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        raise

if __name__ == '__main__':
    run_server()
