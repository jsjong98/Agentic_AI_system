# -*- coding: utf-8 -*-
"""
Structura HR ì´ì§ ì˜ˆì¸¡ Flask ë°±ì—”ë“œ ì„œë¹„ìŠ¤
XGBoost + xAI (SHAP, LIME) ê¸°ë°˜ ì„¤ëª… ê°€ëŠ¥í•œ AI ì‹œìŠ¤í…œ
React ì—°ë™ì— ìµœì í™”ëœ REST API ì„œë²„
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
from werkzeug.exceptions import HTTPException
from werkzeug.utils import secure_filename
import logging
import os
import json
import pickle
import numpy as np
import pandas as pd
from datetime import datetime
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Tuple, Any
from collections import defaultdict

# ML ê´€ë ¨ imports
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import (
    roc_auc_score, average_precision_score, f1_score,
    precision_score, recall_score, accuracy_score,
    classification_report, confusion_matrix, precision_recall_curve
)
import xgboost as xgb
from xgboost import XGBClassifier

# xAI ê´€ë ¨ imports
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    print("Warning: SHAP not available. Install with: pip install shap")

try:
    import lime
    import lime.lime_tabular
    LIME_AVAILABLE = True
except ImportError:
    LIME_AVAILABLE = False
    print("Warning: LIME not available. Install with: pip install lime")

# Optuna for hyperparameter optimization
try:
    import optuna
    from optuna.pruners import MedianPruner
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    print("Warning: Optuna not available. Using default hyperparameters.")

import warnings
warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ------------------------------------------------------
# ë°ì´í„° ëª¨ë¸ ì •ì˜
# ------------------------------------------------------

@dataclass
class PredictionResult:
    """ì˜ˆì¸¡ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    employee_id: Optional[str]
    attrition_probability: float
    attrition_prediction: int
    risk_category: str
    confidence_score: float
    prediction_timestamp: str = None
    
    def __post_init__(self):
        if self.prediction_timestamp is None:
            self.prediction_timestamp = datetime.now().isoformat()
    
    def to_dict(self):
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ìš©)"""
        return asdict(self)

@dataclass
class ExplainabilityResult:
    """ì„¤ëª… ê°€ëŠ¥ì„± ë¶„ì„ ê²°ê³¼"""
    employee_id: Optional[str]
    feature_importance: Dict[str, float]
    shap_values: Optional[Dict[str, float]]
    lime_explanation: Optional[Dict[str, Any]]
    top_risk_factors: List[Dict[str, Any]]
    top_protective_factors: List[Dict[str, Any]]
    explanation_timestamp: str = None
    
    def __post_init__(self):
        if self.explanation_timestamp is None:
            self.explanation_timestamp = datetime.now().isoformat()
    
    def to_dict(self):
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ìš©)"""
        return asdict(self)

# ------------------------------------------------------
# HR ì´ì§ ì˜ˆì¸¡ê¸° í´ë˜ìŠ¤ (Flask ìµœì í™”)
# ------------------------------------------------------

class StructuraHRPredictor:
    """Structura HR ì´íƒˆ ì˜ˆì¸¡ ì‹œìŠ¤í…œ (xAI í¬í•¨)"""
    
    def __init__(self, data_path: str = "../data/IBM_HR.csv", random_state: int = 42):
        self.data_path = data_path
        self.random_state = random_state
        self.model = None
        self.feature_columns = None
        self.optimal_threshold = 0.5
        self.scale_pos_weight = 1.0
        
        # xAI ê´€ë ¨
        self.shap_explainer = None
        self.lime_explainer = None
        self.X_train_sample = None  # LIMEìš© ë°°ê²½ ë°ì´í„°
        
        # ì „ì²˜ë¦¬ ì„¤ì •
        self.setup_preprocessing_config()
        
    def setup_preprocessing_config(self):
        """ì „ì²˜ë¦¬ ì„¤ì • ì´ˆê¸°í™”"""
        # ì œê±°í•  ì»¬ëŸ¼ë“¤
        self.DROP_COLS = [
            "EmployeeCount", "EmployeeNumber", "Over18", "StandardHours",
            "DailyRate", "HourlyRate", "MonthlyRate",
            "PercentSalaryHike", "YearsSinceLastPromotion", "NumCompaniesWorked",
            "TotalWorkingYears"
        ]
        
        # ìˆœì„œí˜• ë³€ìˆ˜ ë§¤í•‘
        self.ORDINAL_MAPS = {
            "Education": {1: "Below College", 2: "College", 3: "Bachelor", 4: "Master", 5: "Doctor"},
            "JobLevel": {1: "Entry", 2: "Junior", 3: "Mid", 4: "Senior", 5: "Executive"},
            "JobInvolvement": {1: "Low", 2: "Medium", 3: "High", 4: "Very High"},
            "JobSatisfaction": {1: "Low", 2: "Medium", 3: "High", 4: "Very High"},
            "EnvironmentSatisfaction": {1: "Low", 2: "Medium", 3: "High", 4: "Very High"},
            "RelationshipSatisfaction": {1: "Low", 2: "Medium", 3: "High", 4: "Very High"},
            "WorkLifeBalance": {1: "Bad", 2: "Good", 3: "Better", 4: "Best"},
            "PerformanceRating": {1: "Low", 2: "Good", 3: "Excellent", 4: "Outstanding"},
            "StockOptionLevel": {0: "None", 1: "Level 1", 2: "Level 2", 3: "Level 3"},
        }
        
        # ëª…ëª©í˜• ë³€ìˆ˜ë“¤
        self.NOMINAL_COLS = [
            "BusinessTravel", "Department", "EducationField", "Gender",
            "JobRole", "MaritalStatus", "OverTime"
        ]
        
        # ìˆœì„œí˜• ë¼ë²¨ ìˆœì„œ
        self.ORDINAL_LABEL_ORDERS = {
            "JobInvolvement": ["Low", "Medium", "High", "Very High"],
            "JobSatisfaction": ["Low", "Medium", "High", "Very High"],
            "EnvironmentSatisfaction": ["Low", "Medium", "High", "Very High"],
            "RelationshipSatisfaction": ["Low", "Medium", "High", "Very High"],
            "WorkLifeBalance": ["Bad", "Good", "Better", "Best"],
            "PerformanceRating": ["Low", "Good", "Excellent", "Outstanding"],
            "Education": ["Below College", "College", "Bachelor", "Master", "Doctor"],
            "JobLevel": ["Entry", "Junior", "Mid", "Senior", "Executive"],
            "StockOptionLevel": ["None", "Level 1", "Level 2", "Level 3"],
        }

    def load_data(self) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë”©"""
        if not os.path.exists(self.data_path):
            raise FileNotFoundError(f"Data file not found: {self.data_path}")
        
        df = pd.read_csv(self.data_path)
        logger.info(f"ë°ì´í„° ë¡œë”© ì™„ë£Œ: {df.shape}")
        return df
    
    def preprocess_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        logger.info("ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
        
        # 1. ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
        drop_present = [c for c in self.DROP_COLS if c in df.columns]
        df = df.drop(columns=drop_present)
        
        # 2. íƒ€ê²Ÿ ë³€ìˆ˜ ì²˜ë¦¬
        TARGET = "Attrition"
        if TARGET not in df.columns:
            raise ValueError(f"Target column '{TARGET}' not found in data.")
        
        y = df[TARGET].map({"Yes": 1, "No": 0})
        if y.isna().any():
            y = pd.Series(pd.factorize(df[TARGET])[0], index=df.index)
        
        X = df.drop(columns=[TARGET])
        
        # 3. ì»¬ëŸ¼ ê·¸ë£¹ ë¶„ë¥˜
        ordinal_cols = [c for c in self.ORDINAL_MAPS.keys() if c in X.columns]
        nominal_cols = [c for c in self.NOMINAL_COLS if c in X.columns]
        numeric_cols = [c for c in X.columns 
                       if (c not in ordinal_cols + nominal_cols) 
                       and pd.api.types.is_numeric_dtype(X[c])]
        
        # 4. ìˆœì„œí˜• ë³€ìˆ˜ë¥¼ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜
        X = self._coerce_ordinal_to_numeric(X, ordinal_cols)
        
        # 5. ê²°ì¸¡ê°’ ì²˜ë¦¬
        for c in numeric_cols + ordinal_cols:
            X[c] = pd.to_numeric(X[c], errors="coerce")
            X[c] = X[c].fillna(X[c].median())
        
        for c in nominal_cols:
            X[c] = X[c].astype("category")
            if "__UNK__" not in X[c].cat.categories:
                X[c] = X[c].cat.add_categories(["__UNK__"])
            X[c] = X[c].fillna("__UNK__")
        
        logger.info("ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")
        return X, y
    
    def _coerce_ordinal_to_numeric(self, df: pd.DataFrame, ordinal_cols: List[str]) -> pd.DataFrame:
        """ìˆœì„œí˜• ë³€ìˆ˜ë¥¼ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜"""
        df = df.copy()
        
        for c in ordinal_cols:
            s = df[c]
            
            if pd.api.types.is_numeric_dtype(s):
                df[c] = pd.to_numeric(s, errors="coerce")
                continue
            
            if pd.api.types.is_categorical_dtype(s):
                if getattr(s.dtype, "ordered", False):
                    codes = s.cat.codes.replace(-1, np.nan)
                    df[c] = codes.astype(float)
                    continue
                
                try_num = pd.to_numeric(s.astype(str), errors="coerce")
                if try_num.notna().any():
                    df[c] = try_num
                    continue
                
                if c in self.ORDINAL_LABEL_ORDERS:
                    mapped = self._map_labels_to_codes(s, self.ORDINAL_LABEL_ORDERS[c])
                    if mapped.notna().any():
                        df[c] = mapped.astype(float)
                        continue
                
                df[c] = s.cat.codes.replace(-1, np.nan).astype(float)
                continue
            
            try_num = pd.to_numeric(s, errors="coerce")
            if try_num.notna().any():
                df[c] = try_num
                continue
            
            if c in self.ORDINAL_LABEL_ORDERS:
                mapped = self._map_labels_to_codes(s, self.ORDINAL_LABEL_ORDERS[c])
                df[c] = mapped.astype(float)
            else:
                codes, _ = pd.factorize(s, na_sentinel=-1)
                df[c] = pd.Series(codes, index=s.index).replace(-1, np.nan).astype(float)
        
        return df
    
    def _map_labels_to_codes(self, series: pd.Series, labels_in_order: List[str]) -> pd.Series:
        """ë¼ë²¨ì„ ì½”ë“œë¡œ ë§¤í•‘"""
        s = series.astype(str).str.strip()
        start_code = 0 if labels_in_order[0] == "None" else 1
        mapping = {lab.lower(): code for code, lab in enumerate(labels_in_order, start=start_code)}
        return s.str.lower().map(mapping)

    def train_model(self, X_train: pd.DataFrame, y_train: pd.Series, 
                   hyperparams: Optional[Dict] = None) -> XGBClassifier:
        """ëª¨ë¸ í›ˆë ¨ ë° xAI ì„¤ì •"""
        logger.info("ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        if hyperparams is None:
            hyperparams = self._get_default_params()
        
        # í´ë˜ìŠ¤ ë¶ˆê· í˜• ê°€ì¤‘ì¹˜ ê³„ì‚°
        neg = int((y_train == 0).sum())
        pos = int((y_train == 1).sum())
        self.scale_pos_weight = neg / max(pos, 1)
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        params = {
            'n_estimators': 1000,
            'objective': 'binary:logistic',
            'eval_metric': 'auc',
            'tree_method': 'hist',
            'enable_categorical': True,
            'scale_pos_weight': self.scale_pos_weight,
            'n_jobs': -1,
            'random_state': self.random_state,
            'verbosity': 0,
            **hyperparams
        }
        
        self.model = XGBClassifier(**params)
        self.model.fit(X_train, y_train)
        self.feature_columns = X_train.columns.tolist()
        
        # xAI ì„¤ì •
        self._setup_explainers(X_train, y_train)
        
        logger.info("ëª¨ë¸ í›ˆë ¨ ë° xAI ì„¤ì • ì™„ë£Œ")
        return self.model
    
    def _setup_explainers(self, X_train: pd.DataFrame, y_train: pd.Series):
        """SHAP ë° LIME ì„¤ëª…ê¸° ì„¤ì •"""
        
        # SHAP ì„¤ì •
        if SHAP_AVAILABLE:
            try:
                logger.info("SHAP ì„¤ëª…ê¸° ì„¤ì • ì¤‘...")
                self.shap_explainer = shap.TreeExplainer(self.model)
                logger.info("SHAP ì„¤ì • ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"SHAP ì„¤ì • ì‹¤íŒ¨: {str(e)}")
                self.shap_explainer = None
        
        # LIME ì„¤ì •
        if LIME_AVAILABLE:
            try:
                logger.info("LIME ì„¤ëª…ê¸° ì„¤ì • ì¤‘...")
                # ìƒ˜í”Œë§ëœ í›ˆë ¨ ë°ì´í„° (LIME ë°°ê²½ìš©)
                sample_size = min(1000, len(X_train))
                sample_indices = np.random.choice(len(X_train), sample_size, replace=False)
                self.X_train_sample = X_train.iloc[sample_indices].values
                
                # ë²”ì£¼í˜• í”¼ì²˜ ì¸ë±ìŠ¤ ì°¾ê¸°
                categorical_features = []
                for i, col in enumerate(X_train.columns):
                    if col in self.NOMINAL_COLS or pd.api.types.is_categorical_dtype(X_train[col]):
                        categorical_features.append(i)
                
                self.lime_explainer = lime.lime_tabular.LimeTabularExplainer(
                    self.X_train_sample,
                    feature_names=self.feature_columns,
                    categorical_features=categorical_features,
                    mode='classification',
                    random_state=self.random_state
                )
                logger.info("LIME ì„¤ì • ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"LIME ì„¤ì • ì‹¤íŒ¨: {str(e)}")
                self.lime_explainer = None
    
    def _get_default_params(self) -> Dict:
        """ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°"""
        return {
            'learning_rate': 0.1,
            'max_depth': 6,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'min_child_weight': 1,
            'gamma': 0,
            'reg_lambda': 1,
            'reg_alpha': 0,
        }

    def predict(self, X: pd.DataFrame, return_proba: bool = False) -> np.ndarray:
        """ì˜ˆì¸¡"""
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if self.feature_columns is not None:
            missing_cols = set(self.feature_columns) - set(X.columns)
            if missing_cols:
                raise ValueError(f"í•„ìš”í•œ í”¼ì²˜ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_cols}")
            X = X[self.feature_columns]
        
        if return_proba:
            return self.model.predict_proba(X)[:, 1]
        else:
            proba = self.model.predict_proba(X)[:, 1]
            return (proba >= self.optimal_threshold).astype(int)

    def predict_single(self, employee_data: Dict) -> PredictionResult:
        """ë‹¨ì¼ ì§ì› ì˜ˆì¸¡"""
        # ë”•ì…”ë„ˆë¦¬ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame([employee_data])
        
        # ì „ì²˜ë¦¬ (íƒ€ê²Ÿ ì œì™¸)
        if 'Attrition' in df.columns:
            df = df.drop(columns=['Attrition'])
        
        # ì˜ˆì¸¡
        probability = self.predict(df, return_proba=True)[0]
        prediction = int(probability >= self.optimal_threshold)
        
        # ìœ„í—˜ ë²”ì£¼ ê²°ì •
        if probability >= 0.7:
            risk_category = "HIGH"
        elif probability >= 0.4:
            risk_category = "MEDIUM"
        else:
            risk_category = "LOW"
        
        # ì‹ ë¢°ë„ ì ìˆ˜ (í™•ë¥ ê³¼ ì„ê³„ê°’ì˜ ê±°ë¦¬ ê¸°ë°˜)
        confidence_score = abs(probability - 0.5) * 2
        
        return PredictionResult(
            employee_id=employee_data.get('EmployeeNumber', None),
            attrition_probability=float(probability),
            attrition_prediction=prediction,
            risk_category=risk_category,
            confidence_score=float(confidence_score)
        )

    def explain_prediction(self, employee_data: Dict) -> ExplainabilityResult:
        """ì˜ˆì¸¡ ì„¤ëª… (xAI)"""
        # ë”•ì…”ë„ˆë¦¬ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame([employee_data])
        
        if 'Attrition' in df.columns:
            df = df.drop(columns=['Attrition'])
        
        # í”¼ì²˜ ì¤‘ìš”ë„ (ëª¨ë¸ ê¸°ë³¸)
        feature_importance = {}
        if self.model and self.feature_columns:
            for feature, importance in zip(self.feature_columns, self.model.feature_importances_):
                feature_importance[feature] = float(importance)
        
        # SHAP ê°’
        shap_values = None
        if self.shap_explainer:
            try:
                shap_vals = self.shap_explainer.shap_values(df[self.feature_columns])
                shap_values = {}
                for feature, shap_val in zip(self.feature_columns, shap_vals[0]):
                    shap_values[feature] = float(shap_val)
            except Exception as e:
                logger.warning(f"SHAP ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        
        # LIME ì„¤ëª…
        lime_explanation = None
        if self.lime_explainer:
            try:
                def predict_fn(x):
                    return self.model.predict_proba(pd.DataFrame(x, columns=self.feature_columns))
                
                explanation = self.lime_explainer.explain_instance(
                    df[self.feature_columns].values[0],
                    predict_fn,
                    num_features=10
                )
                
                lime_explanation = {
                    'features': [item[0] for item in explanation.as_list()],
                    'values': [item[1] for item in explanation.as_list()],
                    'intercept': float(explanation.intercept[1])
                }
            except Exception as e:
                logger.warning(f"LIME ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        
        # ìƒìœ„ ìœ„í—˜/ë³´í˜¸ ìš”ì¸ ì¶”ì¶œ
        importance_items = list(feature_importance.items()) if feature_importance else []
        importance_items.sort(key=lambda x: abs(x[1]), reverse=True)
        
        # SHAP ê°’ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if shap_values:
            shap_items = list(shap_values.items())
            shap_items.sort(key=lambda x: abs(x[1]), reverse=True)
            
            top_risk_factors = [
                {"feature": feat, "impact": val, "type": "risk"}
                for feat, val in shap_items[:5] if val > 0
            ]
            top_protective_factors = [
                {"feature": feat, "impact": abs(val), "type": "protective"}
                for feat, val in shap_items[:5] if val < 0
            ]
        else:
            # í”¼ì²˜ ì¤‘ìš”ë„ ê¸°ë°˜
            top_risk_factors = [
                {"feature": feat, "impact": val, "type": "risk"}
                for feat, val in importance_items[:5]
            ]
            top_protective_factors = []
        
        return ExplainabilityResult(
            employee_id=employee_data.get('EmployeeNumber', None),
            feature_importance=feature_importance,
            shap_values=shap_values,
            lime_explanation=lime_explanation,
            top_risk_factors=top_risk_factors,
            top_protective_factors=top_protective_factors
        )

    def get_feature_importance(self, top_n: int = 20) -> pd.DataFrame:
        """í”¼ì²˜ ì¤‘ìš”ë„ ë°˜í™˜"""
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        importance_df = pd.DataFrame({
            'feature': self.feature_columns,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        return importance_df.head(top_n)

    def save_model(self, filepath: str):
        """ëª¨ë¸ ë° ì„¤ì • ì €ì¥"""
        if self.model is None:
            raise ValueError("ì €ì¥í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        model_data = {
            'model': self.model,
            'feature_columns': self.feature_columns,
            'optimal_threshold': self.optimal_threshold,
            'scale_pos_weight': self.scale_pos_weight,
            'preprocessing_config': {
                'DROP_COLS': self.DROP_COLS,
                'ORDINAL_MAPS': self.ORDINAL_MAPS,
                'NOMINAL_COLS': self.NOMINAL_COLS,
                'ORDINAL_LABEL_ORDERS': self.ORDINAL_LABEL_ORDERS
            },
            'X_train_sample': self.X_train_sample
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        logger.info(f"ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
    
    def load_model(self, filepath: str):
        """ëª¨ë¸ ë° ì„¤ì • ë¡œë”©"""
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
        
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.model = model_data['model']
        self.feature_columns = model_data['feature_columns']
        self.optimal_threshold = model_data['optimal_threshold']
        self.scale_pos_weight = model_data['scale_pos_weight']
        
        # ì „ì²˜ë¦¬ ì„¤ì • ë³µì›
        config = model_data['preprocessing_config']
        self.DROP_COLS = config['DROP_COLS']
        self.ORDINAL_MAPS = config['ORDINAL_MAPS']
        self.NOMINAL_COLS = config['NOMINAL_COLS']
        self.ORDINAL_LABEL_ORDERS = config['ORDINAL_LABEL_ORDERS']
        
        # xAI ë°ì´í„° ë³µì›
        self.X_train_sample = model_data.get('X_train_sample')
        
        # xAI ì„¤ëª…ê¸° ì¬ì„¤ì •
        if self.model and self.X_train_sample is not None:
            self._setup_explainers_from_saved_data()
        
        logger.info(f"ëª¨ë¸ì´ ë¡œë”©ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
    
    def _setup_explainers_from_saved_data(self):
        """ì €ì¥ëœ ë°ì´í„°ë¡œë¶€í„° xAI ì„¤ëª…ê¸° ì¬ì„¤ì •"""
        # SHAP ì„¤ì •
        if SHAP_AVAILABLE:
            try:
                self.shap_explainer = shap.TreeExplainer(self.model)
            except Exception as e:
                logger.warning(f"SHAP ì¬ì„¤ì • ì‹¤íŒ¨: {str(e)}")
        
        # LIME ì„¤ì •
        if LIME_AVAILABLE and self.X_train_sample is not None:
            try:
                categorical_features = []
                for i, col in enumerate(self.feature_columns):
                    if col in self.NOMINAL_COLS:
                        categorical_features.append(i)
                
                self.lime_explainer = lime.lime_tabular.LimeTabularExplainer(
                    self.X_train_sample,
                    feature_names=self.feature_columns,
                    categorical_features=categorical_features,
                    mode='classification',
                    random_state=self.random_state
                )
            except Exception as e:
                logger.warning(f"LIME ì¬ì„¤ì • ì‹¤íŒ¨: {str(e)}")

# ------------------------------------------------------
# Flask ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„± ë° ì„¤ì •
# ------------------------------------------------------

def create_app():
    """Flask ì• í”Œë¦¬ì¼€ì´ì…˜ íŒ©í† ë¦¬"""
    
    app = Flask(__name__)
    
    # CORS ì„¤ì • (React ì—°ë™)
    CORS(app, resources={
        r"/api/*": {
            "origins": ["http://localhost:3000", "http://127.0.0.1:3000"],
            "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"],
            "allow_headers": ["Content-Type", "Authorization"]
        }
    })
    
    # ì„¤ì •
    app.config['JSON_AS_ASCII'] = False
    app.config['JSONIFY_PRETTYPRINT_REGULAR'] = True
    
    # ì „ì—­ ë³€ìˆ˜
    predictor = None
    
    # ------------------------------------------------------
    # ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”
    # ------------------------------------------------------
    
    @app.before_first_request
    def initialize_services():
        """ì²« ìš”ì²­ ì „ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        nonlocal predictor
        
        try:
            logger.info("Structura HR ì˜ˆì¸¡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
            
            # ì˜ˆì¸¡ê¸° ì´ˆê¸°í™”
            predictor = StructuraHRPredictor()
            
            # ê¸°ì¡´ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë¡œë”©
            model_path = "hr_attrition_model.pkl"
            if os.path.exists(model_path):
                predictor.load_model(model_path)
                logger.info("ê¸°ì¡´ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            else:
                logger.info("ìƒˆ ëª¨ë¸ í›ˆë ¨ì´ í•„ìš”í•©ë‹ˆë‹¤")
            
            # Flask ì•±ì— ì €ì¥
            app.predictor = predictor
            
            logger.info("Structura HR ì˜ˆì¸¡ ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise
    
    # ------------------------------------------------------
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
    # ------------------------------------------------------
    
    def get_predictor():
        """ì˜ˆì¸¡ê¸° ê°€ì ¸ì˜¤ê¸°"""
        if not hasattr(app, 'predictor') or app.predictor is None:
            return None
        return app.predictor
    
    # ------------------------------------------------------
    # ì—ëŸ¬ í•¸ë“¤ëŸ¬
    # ------------------------------------------------------
    
    @app.errorhandler(404)
    def not_found(error):
        return jsonify({
            "error": "Not Found",
            "message": "ìš”ì²­í•œ ë¦¬ì†ŒìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "status_code": 404
        }), 404
    
    @app.errorhandler(500)
    def internal_error(error):
        return jsonify({
            "error": "Internal Server Error",
            "message": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
            "status_code": 500
        }), 500
    
    @app.errorhandler(Exception)
    def handle_exception(e):
        if isinstance(e, HTTPException):
            return jsonify({
                "error": e.name,
                "message": e.description,
                "status_code": e.code
            }), e.code
        
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            "error": "Unexpected Error",
            "message": str(e),
            "status_code": 500
        }), 500
    
    # ------------------------------------------------------
    # API ë¼ìš°íŠ¸
    # ------------------------------------------------------
    
    @app.route('/')
    def index():
        """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
        return jsonify({
            "service": "Structura HR ì´ì§ ì˜ˆì¸¡ Flask API",
            "version": "1.0.0",
            "status": "running",
            "description": "XGBoost + xAI ê¸°ë°˜ ì„¤ëª… ê°€ëŠ¥í•œ HR ì´ì§ ì˜ˆì¸¡ ì„œë¹„ìŠ¤",
            "features": {
                "machine_learning": "XGBoost",
                "explainable_ai": ["SHAP", "LIME", "Feature Importance"],
                "react_integration": True
            },
            "endpoints": {
                "health": "/api/health",
                "train_model": "/api/train",
                "predict": "/api/predict",
                "explain": "/api/explain",
                "feature_importance": "/api/feature-importance",
                "model_info": "/api/model/info",
                "predict_batch": "/api/predict/batch",
                "employee_analysis": "/api/employee/analysis"
            }
        })
    
    @app.route('/api/health')
    def health_check():
        """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
        
        predictor = get_predictor()
        
        model_status = "loaded" if (predictor and predictor.model) else "not_loaded"
        xai_status = {
            "shap_available": SHAP_AVAILABLE and (predictor and predictor.shap_explainer is not None),
            "lime_available": LIME_AVAILABLE and (predictor and predictor.lime_explainer is not None)
        }
        
        return jsonify({
            "status": "healthy",
            "model_status": model_status,
            "xai_status": xai_status,
            "dependencies": {
                "shap": SHAP_AVAILABLE,
                "lime": LIME_AVAILABLE,
                "optuna": OPTUNA_AVAILABLE
            },
            "timestamp": datetime.now().isoformat()
        })
    
    @app.route('/api/upload/data', methods=['POST'])
    def upload_data():
        """HR ë°ì´í„° CSV íŒŒì¼ ì—…ë¡œë“œ"""
        try:
            # íŒŒì¼ í™•ì¸
            if 'file' not in request.files:
                return jsonify({
                    "success": False,
                    "error": "íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                }), 400
            
            file = request.files['file']
            if file.filename == '':
                return jsonify({
                    "success": False,
                    "error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                }), 400
            
            # íŒŒì¼ í™•ì¥ì í™•ì¸
            if not file.filename.lower().endswith('.csv'):
                return jsonify({
                    "success": False,
                    "error": "CSV íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
                }), 400
            
            # íŒŒì¼ ì €ì¥
            filename = secure_filename(file.filename)
            upload_dir = os.path.join(os.path.dirname(__file__), '..', 'uploads', 'Structura')
            os.makedirs(upload_dir, exist_ok=True)
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•œ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_name = os.path.splitext(filename)[0]
            new_filename = f"{base_name}_{timestamp}.csv"
            file_path = os.path.join(upload_dir, new_filename)
            
            # ìµœì‹  íŒŒì¼ ë§í¬ë„ ìƒì„± (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±ì„ ìœ„í•´)
            latest_link = os.path.join(upload_dir, 'latest_hr_data.csv')
            file.save(file_path)
            
            # ìµœì‹  íŒŒì¼ ë§í¬ ìƒì„± (ì‹¬ë³¼ë¦­ ë§í¬ ë˜ëŠ” ë³µì‚¬)
            try:
                if os.path.exists(latest_link):
                    os.remove(latest_link)
                # Windowsì—ì„œëŠ” ì‹¬ë³¼ë¦­ ë§í¬ ëŒ€ì‹  ë³µì‚¬ ì‚¬ìš©
                import shutil
                shutil.copy2(file_path, latest_link)
            except Exception as e:
                logger.warning(f"ìµœì‹  íŒŒì¼ ë§í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # ë°ì´í„° ê²€ì¦
            try:
                df = pd.read_csv(file_path)
                
                # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
                required_columns = ['Age', 'JobSatisfaction', 'OverTime', 'MonthlyIncome', 'WorkLifeBalance']
                missing_columns = [col for col in required_columns if col not in df.columns]
                
                if missing_columns:
                    return jsonify({
                        "success": False,
                        "error": f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing_columns)}",
                        "required_columns": required_columns,
                        "found_columns": list(df.columns)
                    }), 400
                
                # ê¸°ì¡´ predictor ì´ˆê¸°í™” (ìƒˆ ë°ì´í„°ë¡œ ì¬í›ˆë ¨ í•„ìš”)
                global predictor_instance
                predictor_instance = None
                
                return jsonify({
                    "success": True,
                    "message": "ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.",
                    "file_info": {
                        "original_filename": filename,
                        "saved_filename": new_filename,
                        "upload_path": upload_dir,
                        "file_path": file_path,
                        "latest_link": latest_link,
                        "rows": len(df),
                        "columns": len(df.columns),
                        "column_names": list(df.columns)
                    },
                    "note": "ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ì¬í›ˆë ¨í•´ì£¼ì„¸ìš”."
                })
                
            except Exception as e:
                return jsonify({
                    "success": False,
                    "error": f"ë°ì´í„° íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}"
                }), 400
                
        except Exception as e:
            return jsonify({
                "success": False,
                "error": f"íŒŒì¼ ì—…ë¡œë“œ ì˜¤ë¥˜: {str(e)}"
            }), 500
    
    @app.route('/api/train', methods=['POST'])
    def train_model():
        """ëª¨ë¸ í›ˆë ¨ ì—”ë“œí¬ì¸íŠ¸ (ë…¸íŠ¸ë¶ ê¸°ë°˜ ìµœì‹  ë²„ì „)"""
        
        predictor = get_predictor()
        if not predictor:
            return jsonify({"error": "ì˜ˆì¸¡ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}), 503
        
        try:
            # ìš”ì²­ ë°ì´í„° íŒŒì‹±
            data = request.get_json() or {}
            optimize_hp = data.get('optimize_hyperparameters', False)
            use_sampling = data.get('use_sampling', True)
            
            logger.info(f"ëª¨ë¸ í›ˆë ¨ ì‹œì‘ - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”: {optimize_hp}, ìƒ˜í”Œë§: {use_sampling}")
            
            # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            metrics = predictor.run_full_pipeline(
                optimize_hp=optimize_hp,
                use_sampling=use_sampling
            )
            
            # ëª¨ë¸ ì €ì¥
            predictor.save_model("hr_attrition_model_xai.pkl")
            
            # í”¼ì²˜ ì¤‘ìš”ë„ ê°€ì ¸ì˜¤ê¸°
            importance_df = predictor.get_feature_importance(10)
            feature_importance = [
                {"feature": row['feature'], "importance": float(row['importance'])}
                for _, row in importance_df.iterrows()
            ]
            
            return jsonify({
                "status": "success",
                "message": "ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ (XAI í¬í•¨)",
                "metrics": {k: float(v) if isinstance(v, (int, float)) else v for k, v in metrics.items()},
                "feature_importance": feature_importance,
                "xai_enabled": {
                    "shap": predictor.shap_explainer is not None,
                    "feature_importance": True
                },
                "model_config": {
                    "optimal_threshold": predictor.optimal_threshold,
                    "features_count": len(predictor.feature_columns) if predictor.feature_columns else 0,
                    "sampling_used": use_sampling
                },
                "timestamp": datetime.now().isoformat()
            })
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {str(e)}")
            return jsonify({"error": f"ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {str(e)}"}), 500
    
    @app.route('/api/predict', methods=['POST'])
    def predict_attrition():
        """ì´ì§ ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸ (Probability ì¤‘ì‹¬ + XAI)"""
        
        predictor = get_predictor()
        if not predictor or not predictor.model:
            return jsonify({"error": "ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}), 503
        
        try:
            # ìš”ì²­ ë°ì´í„° íŒŒì‹±
            data = request.get_json()
            if not data:
                return jsonify({"error": "ì˜ˆì¸¡í•  ì§ì› ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400
            
            # ë‹¨ì¼ ì§ì› ë°ì´í„°ì¸ì§€ í™•ì¸
            if isinstance(data, list):
                # ë°°ì¹˜ ì˜ˆì¸¡
                results = []
                for i, employee_data in enumerate(data):
                    employee_number = employee_data.get('EmployeeNumber', f'BATCH_{i+1:03d}')
                    result = predictor.predict_single_employee(employee_data, employee_number)
                    results.append(result)
                
                return jsonify({
                    "predictions": results,
                    "batch_size": len(results),
                    "timestamp": datetime.now().isoformat()
                })
            else:
                # ë‹¨ì¼ ì˜ˆì¸¡
                employee_number = data.get('EmployeeNumber', 'SINGLE_001')
                result = predictor.predict_single_employee(data, employee_number)
                return jsonify(result)
                
        except Exception as e:
            logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
            return jsonify({"error": f"ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}"}), 500
    
    @app.route('/api/explain', methods=['POST'])
    def explain_prediction():
        """ì˜ˆì¸¡ ì„¤ëª… ì—”ë“œí¬ì¸íŠ¸ (SHAP ê¸°ë°˜ XAI)"""
        
        predictor = get_predictor()
        if not predictor or not predictor.model:
            return jsonify({"error": "ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}), 503
        
        try:
            # ìš”ì²­ ë°ì´í„° íŒŒì‹±
            data = request.get_json()
            if not data:
                return jsonify({"error": "ì„¤ëª…í•  ì§ì› ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400
            
            # EmployeeNumber ì¶”ì¶œ
            employee_number = data.get('EmployeeNumber', 'EXPLAIN_001')
            
            # DataFrameìœ¼ë¡œ ë³€í™˜
            df = pd.DataFrame([data])
            
            # ì˜ˆì¸¡ ì„¤ëª… ìƒì„±
            explanation = predictor.explain_prediction(df, employee_number)
            
            return jsonify(explanation)
            
        except Exception as e:
            logger.error(f"ì˜ˆì¸¡ ì„¤ëª… ì‹¤íŒ¨: {str(e)}")
            return jsonify({"error": f"ì˜ˆì¸¡ ì„¤ëª… ì‹¤íŒ¨: {str(e)}"}), 500
    
    @app.route('/api/feature-importance')
    def get_feature_importance():
        """í”¼ì²˜ ì¤‘ìš”ë„ ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸"""
        
        predictor = get_predictor()
        if not predictor or not predictor.model:
            return jsonify({"error": "ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}), 503
        
        try:
            top_n = request.args.get('top_n', 20, type=int)
            
            importance_df = predictor.get_feature_importance(top_n)
            
            # DataFrameì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            importance_data = [
                {
                    "feature": row['feature'],
                    "importance": float(row['importance']),
                    "rank": i + 1
                }
                for i, (_, row) in enumerate(importance_df.iterrows())
            ]
            
            return jsonify({
                "feature_importance": importance_data,
                "total_features": len(predictor.feature_columns) if predictor.feature_columns else 0,
                "top_n": top_n,
                "timestamp": datetime.now().isoformat()
            })
            
        except Exception as e:
            logger.error(f"í”¼ì²˜ ì¤‘ìš”ë„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return jsonify({"error": f"í”¼ì²˜ ì¤‘ìš”ë„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"}), 500
    
    @app.route('/api/model/info')
    def get_model_info():
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸"""
        
        predictor = get_predictor()
        if not predictor:
            return jsonify({"error": "ì˜ˆì¸¡ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}), 503
        
        model_info = {
            "model_loaded": predictor.model is not None,
            "feature_count": len(predictor.feature_columns) if predictor.feature_columns else 0,
            "feature_columns": predictor.feature_columns,
            "optimal_threshold": predictor.optimal_threshold,
            "scale_pos_weight": predictor.scale_pos_weight,
            "xai_capabilities": {
                "shap_available": predictor.shap_explainer is not None,
                "lime_available": predictor.lime_explainer is not None,
                "feature_importance": True
            },
            "preprocessing_config": {
                "dropped_columns": predictor.DROP_COLS,
                "ordinal_columns": list(predictor.ORDINAL_MAPS.keys()),
                "nominal_columns": predictor.NOMINAL_COLS
            }
        }
        
        return jsonify(model_info)
    
    @app.route('/api/predict/batch', methods=['POST'])
    def predict_batch():
        """ë°°ì¹˜ ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸ (ì—¬ëŸ¬ ì§ì› ë™ì‹œ ì²˜ë¦¬)"""
        
        predictor = get_predictor()
        if not predictor or not predictor.model:
            return jsonify({"error": "ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}), 503
        
        try:
            # ìš”ì²­ ë°ì´í„° íŒŒì‹±
            data = request.get_json()
            if not data or not isinstance(data, list):
                return jsonify({"error": "ë°°ì¹˜ ì˜ˆì¸¡ì„ ìœ„í•œ ì§ì› ë°ì´í„° ë¦¬ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400
            
            results = []
            for i, employee_data in enumerate(data):
                try:
                    employee_number = employee_data.get('EmployeeNumber', f'BATCH_{i+1:03d}')
                    result = predictor.predict_single_employee(employee_data, employee_number)
                    results.append(result)
                except Exception as e:
                    # ê°œë³„ ì˜ˆì¸¡ ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ì •ë³´ í¬í•¨
                    results.append({
                        'employee_number': employee_data.get('EmployeeNumber', f'BATCH_{i+1:03d}'),
                        'error': str(e),
                        'attrition_probability': None,
                        'risk_category': 'ERROR'
                    })
            
            # í†µê³„ ì •ë³´ ê³„ì‚°
            successful_predictions = [r for r in results if 'error' not in r]
            if successful_predictions:
                probabilities = [r['attrition_probability'] for r in successful_predictions]
                risk_distribution = {}
                for r in successful_predictions:
                    risk_cat = r['risk_category']
                    risk_distribution[risk_cat] = risk_distribution.get(risk_cat, 0) + 1
                
                stats = {
                    'total_employees': len(data),
                    'successful_predictions': len(successful_predictions),
                    'failed_predictions': len(data) - len(successful_predictions),
                    'average_probability': sum(probabilities) / len(probabilities),
                    'risk_distribution': risk_distribution,
                    'high_risk_count': risk_distribution.get('HIGH', 0)
                }
            else:
                stats = {
                    'total_employees': len(data),
                    'successful_predictions': 0,
                    'failed_predictions': len(data),
                    'average_probability': None,
                    'risk_distribution': {},
                    'high_risk_count': 0
                }
            
            return jsonify({
                "predictions": results,
                "statistics": stats,
                "timestamp": datetime.now().isoformat()
            })
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
            return jsonify({"error": f"ë°°ì¹˜ ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}"}), 500
    
    @app.route('/api/employee/analysis/<employee_number>', methods=['POST'])
    def employee_analysis(employee_number):
        """ê°œë³„ ì§ì› ì‹¬ì¸µ ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸"""
        
        predictor = get_predictor()
        if not predictor or not predictor.model:
            return jsonify({"error": "ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}), 503
        
        try:
            # ìš”ì²­ ë°ì´í„° íŒŒì‹±
            data = request.get_json()
            if not data:
                return jsonify({"error": "ì§ì› ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400
            
            # ì˜ˆì¸¡ ë° ì„¤ëª…
            result = predictor.predict_single_employee(data, employee_number)
            
            # ì¶”ê°€ ë¶„ì„ ì •ë³´
            df = pd.DataFrame([data])
            probability = predictor.predict(df, return_proba=True)[0]
            
            # ìœ„í—˜ë„ ë ˆë²¨ë³„ ì„ê³„ê°’ ì •ë³´
            risk_thresholds = {
                'LOW': 0.4,
                'MEDIUM': 0.7,
                'HIGH': 1.0
            }
            
            # í˜„ì¬ ìœ„í—˜ë„ì™€ ë‹¤ìŒ ë‹¨ê³„ê¹Œì§€ì˜ ê±°ë¦¬
            current_risk = result['risk_category']
            if current_risk == 'LOW':
                next_threshold = risk_thresholds['LOW']
                distance_to_next = next_threshold - probability
            elif current_risk == 'MEDIUM':
                next_threshold = risk_thresholds['MEDIUM']
                distance_to_next = next_threshold - probability
            else:
                next_threshold = None
                distance_to_next = None
            
            # ì‹¬ì¸µ ë¶„ì„ ê²°ê³¼
            analysis = {
                **result,
                'detailed_analysis': {
                    'probability_score': float(probability),
                    'risk_thresholds': risk_thresholds,
                    'current_risk_level': current_risk,
                    'distance_to_next_level': float(distance_to_next) if distance_to_next else None,
                    'percentile_rank': None  # ì „ì²´ ì§ì› ëŒ€ë¹„ ìˆœìœ„ (ì¶”í›„ êµ¬í˜„)
                },
                'recommendations': self._generate_recommendations(result),
                'timestamp': datetime.now().isoformat()
            }
            
            return jsonify(analysis)
            
        except Exception as e:
            logger.error(f"ì§ì› ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return jsonify({"error": f"ì§ì› ë¶„ì„ ì‹¤íŒ¨: {str(e)}"}), 500
    
    def _generate_recommendations(self, prediction_result):
        """ì˜ˆì¸¡ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        risk_category = prediction_result['risk_category']
        probability = prediction_result['attrition_probability']
        
        if risk_category == 'HIGH':
            recommendations.extend([
                "ì¦‰ì‹œ ë©´ë‹´ì„ í†µí•œ ì´ì§ ì˜ë„ íŒŒì•… í•„ìš”",
                "ì—…ë¬´ í™˜ê²½ ë° ë§Œì¡±ë„ ê°œì„  ë°©ì•ˆ ë…¼ì˜",
                "ê²½ë ¥ ê°œë°œ ë° ìŠ¹ì§„ ê¸°íšŒ ì œê³µ ê²€í† ",
                "ë³´ìƒ ì²´ê³„ ì¬ê²€í†  ë° ì¡°ì • ê³ ë ¤"
            ])
        elif risk_category == 'MEDIUM':
            recommendations.extend([
                "ì •ê¸°ì ì¸ 1:1 ë©´ë‹´ì„ í†µí•œ ìƒíƒœ ëª¨ë‹ˆí„°ë§",
                "ì—…ë¬´ ë§Œì¡±ë„ í–¥ìƒì„ ìœ„í•œ ê°œì„  ë°©ì•ˆ ëª¨ìƒ‰",
                "êµìœ¡ ë° ê°œë°œ ê¸°íšŒ ì œê³µ",
                "íŒ€ ë‚´ ì—­í•  ë° ì±…ì„ ì¬ì¡°ì • ê²€í† "
            ])
        else:  # LOW
            recommendations.extend([
                "í˜„ì¬ ìƒíƒœ ìœ ì§€ë¥¼ ìœ„í•œ ì§€ì†ì  ê´€ë¦¬",
                "ì„±ê³¼ ì¸ì • ë° í”¼ë“œë°± ì œê³µ",
                "ì¥ê¸° ê²½ë ¥ ê°œë°œ ê³„íš ìˆ˜ë¦½ ì§€ì›"
            ])
        
        # XAI ì„¤ëª… ê¸°ë°˜ ì¶”ê°€ ê¶Œì¥ì‚¬í•­
        if 'explanation' in prediction_result and 'individual_explanation' in prediction_result['explanation']:
            exp = prediction_result['explanation']['individual_explanation']
            if 'top_risk_factors' in exp:
                for factor in exp['top_risk_factors'][:2]:  # ìƒìœ„ 2ê°œ ìœ„í—˜ ìš”ì¸
                    feature = factor['feature']
                    if 'Satisfaction' in feature:
                        recommendations.append(f"{feature} ê°œì„ ì„ ìœ„í•œ êµ¬ì²´ì  ì•¡ì…˜ í”Œëœ ìˆ˜ë¦½")
                    elif 'WorkLifeBalance' in feature:
                        recommendations.append("ì›Œë¼ë°¸ ê°œì„ ì„ ìœ„í•œ ìœ ì—°ê·¼ë¬´ì œ ë„ì… ê²€í† ")
                    elif 'OverTime' in feature:
                        recommendations.append("ì—…ë¬´ëŸ‰ ì¡°ì • ë° ì´ˆê³¼ê·¼ë¬´ ìµœì†Œí™” ë°©ì•ˆ ë§ˆë ¨")
        
        return recommendations
    
    return app

# ------------------------------------------------------
# ì„œë²„ ì‹¤í–‰ í•¨ìˆ˜
# ------------------------------------------------------

def run_server(host='0.0.0.0', port=5001, debug=True):
    """Flask ì„œë²„ ì‹¤í–‰ (XAI í¬í•¨ ìµœì‹  ë²„ì „)"""
    app = create_app()
    
    print("=" * 70)
    print("ğŸš€ Structura HR ì˜ˆì¸¡ Flask ë°±ì—”ë“œ ì„œë²„ ì‹œì‘ (XAI í¬í•¨)")
    print("=" * 70)
    print(f"ğŸ“¡ ì„œë²„ ì£¼ì†Œ: http://{host}:{port}")
    print(f"ğŸ”— React ì—°ë™: http://localhost:3000ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥")
    print(f"ğŸ¤– XAI ê¸°ëŠ¥: SHAP ê¸°ë°˜ ì„¤ëª… ê°€ëŠ¥í•œ AI")
    print(f"ğŸ”„ ë””ë²„ê·¸ ëª¨ë“œ: {'í™œì„±í™”' if debug else 'ë¹„í™œì„±í™”'}")
    print()
    print("ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸:")
    print(f"  â€¢ í—¬ìŠ¤ì²´í¬: http://{host}:{port}/api/health")
    print(f"  â€¢ ë°ì´í„° ì—…ë¡œë“œ: http://{host}:{port}/api/upload/data")
    print(f"  â€¢ ëª¨ë¸ í›ˆë ¨: http://{host}:{port}/api/train")
    print(f"  â€¢ ì´ì§ ì˜ˆì¸¡: http://{host}:{port}/api/predict")
    print(f"  â€¢ ë°°ì¹˜ ì˜ˆì¸¡: http://{host}:{port}/api/predict/batch")
    print(f"  â€¢ ì˜ˆì¸¡ ì„¤ëª…: http://{host}:{port}/api/explain")
    print(f"  â€¢ ì§ì› ë¶„ì„: http://{host}:{port}/api/employee/analysis/<employee_number>")
    print(f"  â€¢ í”¼ì²˜ ì¤‘ìš”ë„: http://{host}:{port}/api/feature-importance")
    print(f"  â€¢ ëª¨ë¸ ì •ë³´: http://{host}:{port}/api/model/info")
    print()
    print("XAI ê¸°ëŠ¥:")
    print(f"  â€¢ SHAP: {'âœ…' if SHAP_AVAILABLE else 'âŒ'}")
    print(f"  â€¢ Feature Importance: âœ…")
    print(f"  â€¢ Optuna: {'âœ…' if OPTUNA_AVAILABLE else 'âŒ'}")
    print()
    print("ìƒˆë¡œìš´ ê¸°ëŠ¥:")
    print("  â€¢ EmployeeNumberë³„ ê°œë³„ XAI ì„¤ëª…")
    print("  â€¢ Probability ì¤‘ì‹¬ ì˜ˆì¸¡ ê²°ê³¼")
    print("  â€¢ ë°°ì¹˜ ì²˜ë¦¬ ë° í†µê³„ ë¶„ì„")
    print("  â€¢ ìœ„í—˜ë„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ì œê³µ")
    print()
    print("ì„œë²„ë¥¼ ì¤‘ì§€í•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
    print("=" * 70)
    
    app.run(host=host, port=port, debug=debug)

if __name__ == '__main__':
    run_server()
