# ============================================================================
# Chronos Flask ë°±ì—”ë“œ API
# ============================================================================

from flask import Flask, request, jsonify, render_template_string
from flask_cors import CORS
from werkzeug.utils import secure_filename
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
import joblib
import os
import json
from datetime import datetime
import traceback
from typing import Dict, List, Any
import warnings
import logging
warnings.filterwarnings('ignore')

def safe_json_serialize(obj):
    """
    NaN, Infinity ê°’ì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” JSON ì§ë ¬í™” í•¨ìˆ˜
    """
    if isinstance(obj, dict):
        return {k: safe_json_serialize(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [safe_json_serialize(item) for item in obj]
    elif isinstance(obj, (np.ndarray,)):
        return safe_json_serialize(obj.tolist())
    elif isinstance(obj, (np.integer, np.int64, np.int32)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj):
            return None
        return float(obj)
    elif isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj):
            return None
        return obj
    else:
        return obj

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìœ„í•œ Optuna import
try:
    import optuna
    from optuna.samplers import TPESampler
    OPTUNA_AVAILABLE = True
    print("âœ… Optuna ì‚¬ìš© ê°€ëŠ¥ - ë² ì´ì§€ì•ˆ ìµœì í™” í™œì„±í™”")
except ImportError:
    OPTUNA_AVAILABLE = False
    print("âš ï¸ Optuna ë¯¸ì„¤ì¹˜ - ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©")

# ë¡œì»¬ ëª¨ë“ˆ import
from chronos_models import GRU_CNN_HybridModel, ChronosModelTrainer, create_hybrid_model, create_attention_model
from chronos_processor_fixed import ProperTimeSeriesProcessor, ChronosVisualizer, employee_based_train_test_split

def time_series_cross_validation(X, y, employee_ids, n_splits=3):
    """
    ì‹œê³„ì—´ êµì°¨ ê²€ì¦ - ì‹œê°„ ìˆœì„œë¥¼ ê³ ë ¤í•œ ì§ì›ë³„ ë¶„í• 
    ê° foldì—ì„œ ì´ì „ ì‹œì ì˜ ì§ì›ë“¤ë¡œ í•™ìŠµí•˜ê³  ì´í›„ ì‹œì ì˜ ì§ì›ë“¤ë¡œ ê²€ì¦
    """
    unique_employees = np.unique(employee_ids)
    n_employees = len(unique_employees)
    
    # ì§ì›ì„ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬ (ê°€ì •: employee_idê°€ ì‹œê°„ ìˆœì„œì™€ ì—°ê´€)
    # ì‹¤ì œë¡œëŠ” ê° ì§ì›ì˜ ì²« ì‹œì ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•´ì•¼ í•¨
    sorted_employees = np.sort(unique_employees)
    
    fold_size = n_employees // n_splits
    cv_results = []
    
    for fold in range(n_splits):
        # ì‹œê°„ ê¸°ë°˜ ë¶„í• : ì´ì „ ì‹œì  ì§ì›ë“¤ë¡œ í•™ìŠµ, ì´í›„ ì‹œì  ì§ì›ë“¤ë¡œ ê²€ì¦
        if fold == n_splits - 1:  # ë§ˆì§€ë§‰ fold
            train_employees = sorted_employees[:fold * fold_size]
            val_employees = sorted_employees[fold * fold_size:]
        else:
            train_employees = sorted_employees[:fold * fold_size + fold_size]
            val_employees = sorted_employees[fold * fold_size + fold_size:(fold + 1) * fold_size + fold_size]
        
        if len(train_employees) == 0 or len(val_employees) == 0:
            continue
            
        # ë§ˆìŠ¤í¬ ìƒì„±
        train_mask = np.isin(employee_ids, train_employees)
        val_mask = np.isin(employee_ids, val_employees)
        
        cv_results.append({
            'train_indices': np.where(train_mask)[0],
            'val_indices': np.where(val_mask)[0],
            'train_employees': train_employees,
            'val_employees': val_employees
        })
    
    return cv_results

app = Flask(__name__)
CORS(app)

# íŒŒì¼ ì—…ë¡œë“œ í¬ê¸° ì œí•œì„ 300MBë¡œ ì„¤ì •
app.config['MAX_CONTENT_LENGTH'] = 300 * 1024 * 1024  # 300MB

# ë¡œê¹… ì„¤ì • - íŒŒì¼ê³¼ ì½˜ì†” ëª¨ë‘ ì¶œë ¥
log_dir = "../../logs"
os.makedirs(log_dir, exist_ok=True)
log_file = os.path.join(log_dir, "chronos_server.log")

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# íŒŒì¼ í•¸ë“¤ëŸ¬
file_handler = logging.FileHandler(log_file, encoding='utf-8')
file_handler.setLevel(logging.INFO)

# ì½˜ì†” í•¸ë“¤ëŸ¬  
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)

# í¬ë§·í„°
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)
console_handler.setFormatter(formatter)

# í•¸ë“¤ëŸ¬ ì¶”ê°€
logger.addHandler(file_handler)
logger.addHandler(console_handler)

# ì „ì—­ ë³€ìˆ˜
processor = None
model = None
visualizer = None
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ëª¨ë“ˆ import ì‹œ ìë™ ì´ˆê¸°í™” (í•¨ìˆ˜ ì •ì˜ í›„ì— ì‹¤í–‰í•˜ê¸° ìœ„í•´ ë‚˜ì¤‘ì— í˜¸ì¶œ)

# ë°ì´í„° ê²½ë¡œ ì„¤ì • - uploads ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
def get_chronos_data_paths(analysis_type='batch'):
    """uploads ë””ë ‰í† ë¦¬ì—ì„œ Chronos ë°ì´í„° íŒŒì¼ ì°¾ê¸°"""
    uploads_dir = "app/uploads/Chronos"
    post_dir = "app/uploads/Chronos/post"
    batch_dir = "app/uploads/Chronos/batch"
    
    data_paths = {
        'timeseries': None,
        'personas': None
    }
    
    logger.info(f"ğŸ” Chronos ë°ì´í„° ê²½ë¡œ íƒìƒ‰ ì¤‘... (ë¶„ì„ íƒ€ì…: {analysis_type})")
    
    # ë¶„ì„ íƒ€ì…ì— ë”°ë¥¸ ë””ë ‰í† ë¦¬ ìš°ì„ ìˆœìœ„ ì„¤ì •
    if analysis_type == 'post':
        search_dirs = [post_dir, uploads_dir]
    elif analysis_type == 'batch':
        search_dirs = [batch_dir, post_dir, uploads_dir]
    else:
        search_dirs = [uploads_dir, post_dir, batch_dir]
    
    # ê° ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ ì°¾ê¸°
    for search_dir in search_dirs:
        if os.path.exists(search_dir):
            files = [f for f in os.listdir(search_dir) if f.endswith('.csv')]
            if files:
                # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì‚¬ìš© (ìˆ˜ì • ì‹œê°„ ê¸°ì¤€)
                files_with_time = [(f, os.path.getmtime(os.path.join(search_dir, f))) for f in files]
                files_with_time.sort(key=lambda x: x[1], reverse=True)  # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ
                latest_file = os.path.join(search_dir, files_with_time[0][0])
                data_paths['timeseries'] = latest_file
                logger.info(f"ğŸ“ {search_dir}ì—ì„œ ìµœì‹  íŒŒì¼ ë°œê²¬: {files_with_time[0][0]}")
                break
    
    # latest íŒŒì¼ í™•ì¸
    if data_paths['timeseries'] is None and os.path.exists(uploads_dir):
        latest_timeseries = os.path.join(uploads_dir, 'latest_timeseries.csv')
        if os.path.exists(latest_timeseries):
            data_paths['timeseries'] = latest_timeseries
            print(f"ğŸ“ latest íŒŒì¼ ì‚¬ìš©: {latest_timeseries}")
    
    # ê¸°ë³¸ê°’ìœ¼ë¡œ fallback
    if data_paths['timeseries'] is None:
        data_paths['timeseries'] = 'data/IBM_HR_timeseries.csv'
        print(f"âš ï¸ ì—…ë¡œë“œëœ íŒŒì¼ ì—†ìŒ, ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©: {data_paths['timeseries']}")
    
    if data_paths['personas'] is None:
        data_paths['personas'] = 'data/IBM_HR_personas_assigned.csv'
        
    return data_paths

DATA_PATH = get_chronos_data_paths('post')

MODEL_PATH = 'app/Chronos/models'
os.makedirs(MODEL_PATH, exist_ok=True)

def load_model():
    """
    ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ
    """
    global model
    
    try:
        model_file = os.path.join(MODEL_PATH, 'chronos_attention_model.pth')
        scaler_file = os.path.join(MODEL_PATH, 'chronos_scaler.joblib')
        
        if os.path.exists(model_file) and os.path.exists(scaler_file):
            # ëª¨ë¸ ë¡œë“œ
            checkpoint = torch.load(model_file, map_location=device)
            model = create_hybrid_model(
                input_size=checkpoint['input_size'],
                gru_hidden=checkpoint.get('gru_hidden', 32),
                cnn_filters=checkpoint.get('cnn_filters', 16),
                kernel_sizes=checkpoint.get('kernel_sizes', [2, 3]),
                dropout=checkpoint.get('dropout', 0.2)
            )
            model.load_state_dict(checkpoint['model_state_dict'])
            model.to(device)
            model.eval()
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
            if processor:
                processor.scaler = joblib.load(scaler_file)
                processor.feature_columns = checkpoint.get('feature_columns', [])
            
            print("âœ… ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
            return True
        else:
            print("âš ï¸ ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return False

def initialize_system():
    """
    ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    """
    global processor, model, visualizer, DATA_PATH
    
    try:
        logger.info("ğŸš€ Chronos ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # ë°ì´í„° ê²½ë¡œ ì—…ë°ì´íŠ¸ (ì—…ë¡œë“œëœ íŒŒì¼ í™•ì¸ - post ìš°ì„ )
        DATA_PATH = get_chronos_data_paths('post')
        logger.info(f"ğŸ“ ë°ì´í„° ê²½ë¡œ: {DATA_PATH}")
        
        # ê°œì„ ëœ í”„ë¡œì„¸ì„œ ë° ì‹œê°í™” ë„êµ¬ ì´ˆê¸°í™”
        processor = ProperTimeSeriesProcessor(sequence_length=50, aggregation_unit='week')
        visualizer = ChronosVisualizer()
        
        # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        if os.path.exists(DATA_PATH['timeseries']):
            # personas íŒŒì¼ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ None
            personas_path = DATA_PATH['personas'] if DATA_PATH['personas'] and os.path.exists(DATA_PATH['personas']) else None
            processor.load_data(DATA_PATH['timeseries'], personas_path)
            processor.detect_columns()
            processor.preprocess_data()
            processor.identify_features()
            
            # processed_data ì†ì„±ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ì‹œí€€ìŠ¤ ìƒì„± í˜¸ì¶œ
            try:
                processor.create_proper_sequences()
                logger.info("âœ… ë°ì´í„° ë¡œë“œ, ì „ì²˜ë¦¬ ë° ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ ì‹œí€€ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
                # processed_data ì†ì„±ì„ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •
                if hasattr(processor, 'ts_data') and processor.ts_data is not None:
                    processor.processed_data = processor.ts_data
                logger.info("âœ… ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì™„ë£Œ (ì‹œí€€ìŠ¤ ìƒì„± ì œì™¸)")
        else:
            logger.warning("âš ï¸ ì‹œê³„ì—´ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµëœ ëª¨ë¸ë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹œë„
        model_file = os.path.join(MODEL_PATH, 'chronos_attention_model.pth')
        if os.path.exists(model_file):
            load_model()
            print("âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        else:
            print("â„¹ï¸ ê¸°ì¡´ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ëª¨ë¸ì„ í•™ìŠµí•´ì£¼ì„¸ìš”.")
            
        logger.info("ğŸ‰ Chronos ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        traceback.print_exc()
        return False

# ëª¨ë“ˆ import ì‹œ ìë™ ì´ˆê¸°í™” ì‹¤í–‰
def _auto_initialize():
    """ëª¨ë“ˆì´ importë  ë•Œ ìë™ìœ¼ë¡œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global processor, model, visualizer
    if processor is None:
        try:
            print("ğŸ”„ Chronos ëª¨ë“ˆ import ì‹œ ìë™ ì´ˆê¸°í™” ì¤‘...")
            initialize_system()
        except Exception as e:
            print(f"âš ï¸ ìë™ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ìë™ ì´ˆê¸°í™” ì‹¤í–‰
_auto_initialize()


def save_model(model, processor, additional_info=None):
    """
    ëª¨ë¸ ì €ì¥
    """
    try:
        model_file = os.path.join(MODEL_PATH, 'chronos_attention_model.pth')
        scaler_file = os.path.join(MODEL_PATH, 'chronos_scaler.joblib')
        
        # ëª¨ë¸ ì €ì¥
        checkpoint = {
            'model_state_dict': model.state_dict(),
            'input_size': model.input_size,
            'gru_hidden': model.gru_hidden,
            'cnn_filters': model.cnn_filters,
            'feature_columns': processor.feature_columns,
            'sequence_length': processor.sequence_length,
            'aggregation_unit': processor.aggregation_unit,
            'timestamp': datetime.now().isoformat()
        }
        
        if additional_info:
            checkpoint.update(additional_info)
            
        torch.save(checkpoint, model_file)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        joblib.dump(processor.scaler, scaler_file)
        
        print("âœ… ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ")
        return True
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return False

@app.route('/')
def home():
    """
    í™ˆí˜ì´ì§€
    """
    html_template = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Chronos - Employee Attrition Prediction</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; background-color: #f5f5f5; }
            .container { max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
            h1 { color: #2c3e50; text-align: center; }
            .api-section { margin: 20px 0; padding: 20px; background: #ecf0f1; border-radius: 5px; }
            .endpoint { margin: 10px 0; padding: 10px; background: white; border-left: 4px solid #3498db; }
            .method { font-weight: bold; color: #e74c3c; }
            .status { padding: 10px; margin: 20px 0; border-radius: 5px; }
            .status.ready { background-color: #d4edda; color: #155724; border: 1px solid #c3e6cb; }
            .status.warning { background-color: #fff3cd; color: #856404; border: 1px solid #ffeaa7; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>ğŸ•’ Chronos - Employee Attrition Prediction System</h1>
            
            <div class="status {{ status_class }}">
                <strong>ì‹œìŠ¤í…œ ìƒíƒœ:</strong> {{ status_message }}
            </div>
            
            <div class="api-section">
                <h2>ğŸ“‹ API ì—”ë“œí¬ì¸íŠ¸</h2>
                
                <div class="endpoint">
                    <span class="method">GET</span> <strong>/api/status</strong>
                    <p>ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸</p>
                </div>
                
                <div class="endpoint">
                    <span class="method">POST</span> <strong>/api/train</strong>
                    <p>ëª¨ë¸ í•™ìŠµ (JSON: {"sequence_length": 6, "epochs": 50})</p>
                </div>
                
                <div class="endpoint">
                    <span class="method">POST</span> <strong>/api/predict</strong>
                    <p>ì˜ˆì¸¡ ìˆ˜í–‰ (JSON: {"employee_ids": [1, 2, 3]})</p>
                </div>
                
                <div class="endpoint">
                    <span class="method">GET</span> <strong>/api/feature_importance</strong>
                    <p>ì „ì²´ í‰ê·  Feature importance ì‹œê°í™”</p>
                </div>
                
                <div class="endpoint">
                    <span class="method">GET</span> <strong>/api/employee_feature_importance</strong>
                    <p>ì§ì›ë³„ Attention & Gradient Feature Importance ë¹„êµ</p>
                </div>
                
                <div class="endpoint">
                    <span class="method">GET</span> <strong>/api/employee_attention_heatmap</strong>
                    <p>ì§ì›ë³„ Attention Weights íˆíŠ¸ë§µ</p>
                </div>
                
                <div class="endpoint">
                    <span class="method">GET</span> <strong>/api/feature_importance_distribution</strong>
                    <p>Feature Importance ë¶„í¬ ë¶„ì„</p>
                </div>
                
                <div class="endpoint">
                    <span class="method">GET</span> <strong>/api/model_analysis</strong>
                    <p>ëª¨ë¸ ë¶„ì„ ëŒ€ì‹œë³´ë“œ</p>
                </div>
                
                <div class="endpoint">
                    <span class="method">GET</span> <strong>/api/employee_timeline/{employee_id}</strong>
                    <p>ê°œë³„ ì§ì› ì‹œê³„ì—´ ë¶„ì„</p>
                </div>
            </div>
            
            <div class="api-section">
                <h2>ğŸš€ ì‚¬ìš© ë°©ë²•</h2>
                <ol>
                    <li><strong>ëª¨ë¸ í•™ìŠµ:</strong> POST /api/trainìœ¼ë¡œ ëª¨ë¸ì„ ë¨¼ì € í•™ìŠµì‹œí‚¤ì„¸ìš”</li>
                    <li><strong>ì˜ˆì¸¡ ìˆ˜í–‰:</strong> POST /api/predictë¡œ ì§ì›ë“¤ì˜ í‡´ì‚¬ í™•ë¥ ì„ ì˜ˆì¸¡í•˜ì„¸ìš”</li>
                    <li><strong>ì „ì²´ ë¶„ì„:</strong> GET /api/feature_importanceë¡œ ì „ì²´ í‰ê·  ì¤‘ìš” í”¼ì²˜ë“¤ì„ í™•ì¸í•˜ì„¸ìš”</li>
                    <li><strong>ì§ì›ë³„ ë¶„ì„:</strong> GET /api/employee_feature_importanceë¡œ ê° ì§ì›ë³„ Attention & Gradient ì¤‘ìš”ë„ë¥¼ ë¹„êµí•˜ì„¸ìš”</li>
                    <li><strong>ì‹œê°„ë³„ íŒ¨í„´:</strong> GET /api/employee_attention_heatmapìœ¼ë¡œ ì§ì›ë³„ ì‹œê°„ëŒ€ë³„ ì§‘ì¤‘ë„ë¥¼ í™•ì¸í•˜ì„¸ìš”</li>
                    <li><strong>ë¶„í¬ ë¶„ì„:</strong> GET /api/feature_importance_distributionìœ¼ë¡œ í”¼ì²˜ ì¤‘ìš”ë„ ë¶„í¬ë¥¼ ë¶„ì„í•˜ì„¸ìš”</li>
                    <li><strong>ê°œë³„ ë¶„ì„:</strong> GET /api/employee_timeline/{id}ë¡œ ê°œë³„ ì§ì›ì„ ìƒì„¸ ë¶„ì„í•˜ì„¸ìš”</li>
                </ol>
            </div>
        </div>
    </body>
    </html>
    """
    
    # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    if model is not None and processor is not None:
        status_class = "ready"
        status_message = "âœ… ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë“  ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    elif processor is not None:
        status_class = "warning"
        status_message = "âš ï¸ ë°ì´í„°ëŠ” ë¡œë“œë˜ì—ˆì§€ë§Œ ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”."
    else:
        status_class = "warning"
        status_message = "âš ï¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë°ì´í„° íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
    
    return render_template_string(html_template, 
                                status_class=status_class, 
                                status_message=status_message)

@app.route('/api/status')
def get_status():
    """
    ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    """
    try:
        status = {
            'system_initialized': processor is not None,
            'model_loaded': model is not None,
            'data_available': processor is not None and processor.ts_data is not None,
            'device': str(device),
            'timestamp': datetime.now().isoformat(),
            'data_paths': DATA_PATH,
            'timeseries_exists': os.path.exists(DATA_PATH['timeseries']) if DATA_PATH['timeseries'] else False,
            'personas_exists': os.path.exists(DATA_PATH['personas']) if DATA_PATH['personas'] else False
        }
        
        if processor:
            status.update({
                'sequence_length': processor.sequence_length,
                'aggregation_unit': processor.aggregation_unit,
                'feature_count': len(processor.feature_columns),
                'data_shape': processor.processed_data.shape if processor.processed_data is not None else None
            })
        
        return jsonify(status)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/upload/timeseries', methods=['POST'])
def upload_timeseries_data():
    """ì‹œê³„ì—´ ë°ì´í„° CSV íŒŒì¼ ì—…ë¡œë“œ"""
    try:
        # íŒŒì¼ í™•ì¸
        if 'file' not in request.files:
            return jsonify({
                "success": False,
                "error": "ì‹œê³„ì—´ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({
                "success": False,
                "error": "ì‹œê³„ì—´ íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
            
        # personas íŒŒì¼ í™•ì¸ (ì„ íƒì‚¬í•­)
        personas_file = None
        if 'personas_file' in request.files:
            personas_file = request.files['personas_file']
            if personas_file.filename == '':
                personas_file = None
        
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        if not file.filename.lower().endswith('.csv'):
            return jsonify({
                "success": False,
                "error": "CSV íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            }), 400
        
        # íŒŒì¼ ì €ì¥
        filename = secure_filename(file.filename)
        upload_dir = os.path.join(os.path.dirname(__file__), '..', 'uploads', 'Chronos')
        os.makedirs(upload_dir, exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•œ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = os.path.splitext(filename)[0]
        new_filename = f"{base_name}_{timestamp}.csv"
        file_path = os.path.join(upload_dir, new_filename)
        
        # ìµœì‹  íŒŒì¼ ë§í¬ë„ ìƒì„±
        latest_link = os.path.join(upload_dir, 'latest_timeseries.csv')
        file.save(file_path)
        
        # ìµœì‹  íŒŒì¼ ë§í¬ ìƒì„±
        try:
            if os.path.exists(latest_link):
                os.remove(latest_link)
            import shutil
            shutil.copy2(file_path, latest_link)
        except Exception as e:
            print(f"ìµœì‹  íŒŒì¼ ë§í¬ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # íŒŒì¼ í¬ê¸° í™•ì¸ (300MB ì œí•œ)
        file.seek(0, 2)  # íŒŒì¼ ëìœ¼ë¡œ ì´ë™
        file_size = file.tell()
        file.seek(0)  # íŒŒì¼ ì‹œì‘ìœ¼ë¡œ ë˜ëŒë¦¬ê¸°
        
        max_size = 300 * 1024 * 1024  # 300MB
        if file_size > max_size:
            return jsonify({
                "success": False,
                "error": f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ìµœëŒ€ 300MBê¹Œì§€ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤. (í˜„ì¬: {file_size / (1024*1024):.1f}MB)"
            }), 413
        
        # ë°ì´í„° ê²€ì¦
        try:
            df = pd.read_csv(file_path)
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ ë° ìë™ ìƒì„±
            required_columns = ['employee_id']
            
            # employee_id ì»¬ëŸ¼ í™•ì¸
            if 'employee_id' not in df.columns:
                return jsonify({
                    "success": False,
                    "error": "í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: employee_id",
                    "required_columns": ['employee_id', 'week ë˜ëŠ” date'],
                    "found_columns": list(df.columns)
                }), 400
            
            # week ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ date ì»¬ëŸ¼ì—ì„œ ìƒì„±
            if 'week' not in df.columns:
                if 'date' in df.columns:
                    print("ğŸ“… date ì»¬ëŸ¼ì—ì„œ week ì»¬ëŸ¼ ìë™ ìƒì„± ì¤‘...")
                    df['date'] = pd.to_datetime(df['date'])
                    df['year'] = df['date'].dt.year
                    df['week'] = df['date'].dt.isocalendar().week
                    df['week'] = df['year'].astype(str) + '-W' + df['week'].astype(str).str.zfill(2)
                    print(f"âœ… week ì»¬ëŸ¼ ìƒì„± ì™„ë£Œ")
                else:
                    return jsonify({
                        "success": False,
                        "error": "week ë˜ëŠ” date ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤",
                        "required_columns": ['employee_id', 'week ë˜ëŠ” date'],
                        "found_columns": list(df.columns)
                    }), 400
            
            # ì‹œê³„ì—´ ë°ì´í„° í˜•ì‹ í™•ì¸
            unique_employees = df['employee_id'].nunique()
            weeks_per_employee = df.groupby('employee_id')['week'].count()
            min_weeks = weeks_per_employee.min()
            max_weeks = weeks_per_employee.max()
            
            # ê¸°ì¡´ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ìƒˆ ë°ì´í„°ë¡œ ì¬ì²˜ë¦¬ í•„ìš”)
            global processor, model
            processor = None
            model = None
            
            return jsonify({
                "success": True,
                "message": "ì‹œê³„ì—´ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "file_info": {
                    "original_filename": filename,
                    "saved_filename": new_filename,
                    "upload_path": upload_dir,
                    "file_path": file_path,
                    "latest_link": latest_link,
                    "total_rows": len(df),
                    "unique_employees": unique_employees,
                    "weeks_range": f"{min_weeks}-{max_weeks}ì£¼",
                    "columns": len(df.columns),
                    "column_names": list(df.columns)
                },
                "note": "ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì‹œìŠ¤í…œì„ ì¬ì´ˆê¸°í™”í•˜ê³  ëª¨ë¸ì„ ì¬í›ˆë ¨í•´ì£¼ì„¸ìš”."
            })
            
        except Exception as e:
            return jsonify({
                "success": False,
                "error": f"ë°ì´í„° íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}"
            }), 400
            
    except Exception as e:
        return jsonify({
            "success": False,
            "error": f"íŒŒì¼ ì—…ë¡œë“œ ì˜¤ë¥˜: {str(e)}"
        }), 500

@app.route('/api/train', methods=['POST'])
def train_model():
    """
    ëª¨ë¸ í•™ìŠµ
    """
    global model
    
    try:
        # í”„ë¡œì„¸ì„œê°€ ì—†ê±°ë‚˜ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë‹¤ì‹œ ì´ˆê¸°í™”
        if processor is None or not hasattr(processor, 'ts_data') or processor.ts_data is None:
            print("ğŸ”„ ë°ì´í„°ê°€ ì—†ì–´ì„œ ì‹œìŠ¤í…œì„ ë‹¤ì‹œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
            success = initialize_system()
            if not success or processor is None or not hasattr(processor, 'ts_data') or processor.ts_data is None:
                return jsonify({'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.'}), 400
        
        # ìš”ì²­ íŒŒë¼ë¯¸í„° íŒŒì‹±
        params = request.get_json() or {}
        sequence_length = params.get('sequence_length', 6)
        epochs = params.get('epochs', 50)
        batch_size = params.get('batch_size', 32)
        learning_rate = params.get('learning_rate', 0.001)
        optimize_hyperparameters_flag = params.get('optimize_hyperparameters', True)  # ê¸°ë³¸ì ìœ¼ë¡œ ìµœì í™” ì‚¬ìš©
        
        print(f"ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘ - Epochs: {epochs}, Batch Size: {batch_size}")
        if optimize_hyperparameters_flag and OPTUNA_AVAILABLE:
            print("ğŸ”§ ë² ì´ì§€ì•ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í™œì„±í™”")
        else:
            print("âš™ï¸ ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©")
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ ì—…ë°ì´íŠ¸
        processor.sequence_length = sequence_length
        
        # ê°œì„ ëœ ì‹œí€€ìŠ¤ ìƒì„±
        X, y, employee_ids = processor.create_proper_sequences()
        
        # ì§ì› ê¸°ë°˜ ë¶„í•  (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ - ì‹œê³„ì—´ íŠ¹ì„± ê³ ë ¤)
        X_train, X_test, y_train, y_test = employee_based_train_test_split(
            X, y, employee_ids, test_ratio=0.2
        )
        
        print(f"ğŸ“Š ì‹œê³„ì—´ ê²€ì¦ ë°©ì‹:")
        print(f"   - ì§ì›ë³„ ë¶„í• : ë™ì¼ ì§ì› ë°ì´í„°ê°€ train/test ë™ì‹œ í¬í•¨ ë°©ì§€")
        print(f"   - ì‹œê°„ ìˆœì„œ ìœ ì§€: ê° ì§ì›ì˜ ê³¼ê±°â†’í˜„ì¬ ì‹œí€€ìŠ¤ ë³´ì¡´")
        print(f"   - ì˜ˆì¸¡ ë°©í–¥: ê³¼ê±° 6ì£¼ ë°ì´í„°ë¡œ ë¯¸ë˜ í‡´ì‚¬ ì—¬ë¶€ ì˜ˆì¸¡")
        
        # í…ì„œ ë³€í™˜
        X_train_tensor = torch.FloatTensor(X_train)
        X_test_tensor = torch.FloatTensor(X_test)
        y_train_tensor = torch.LongTensor(y_train)
        y_test_tensor = torch.LongTensor(y_test)
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        # ëª¨ë¸ ìƒì„± (ìµœì í™” ì—¬ë¶€ì— ë”°ë¼)
        input_size = len(processor.feature_columns)
        
        if optimize_hyperparameters_flag and OPTUNA_AVAILABLE:
            # ë² ì´ì§€ì•ˆ ìµœì í™”ë¥¼ í†µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
            print("ğŸ”§ ë² ì´ì§€ì•ˆ ìµœì í™”ë¡œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ì¤‘...")
            
            def objective(trial):
                # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ
                gru_hidden = trial.suggest_categorical('gru_hidden', [16, 32, 64, 128])
                cnn_filters = trial.suggest_categorical('cnn_filters', [8, 16, 32, 64])
                dropout = trial.suggest_float('dropout', 0.1, 0.5)
                trial_lr = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)
                trial_batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])
                
                try:
                    # ëª¨ë¸ ìƒì„±
                    trial_model = create_hybrid_model(
                        input_size=input_size,
                        gru_hidden=gru_hidden,
                        cnn_filters=cnn_filters,
                        kernel_sizes=[2, 3],
                        dropout=dropout
                    )
                    trial_model.to(device)
                    
                    # ë°ì´í„° ë¡œë” (ë°°ì¹˜ í¬ê¸° ì ìš©)
                    trial_train_loader = DataLoader(train_dataset, batch_size=trial_batch_size, shuffle=True)
                    trial_test_loader = DataLoader(test_dataset, batch_size=trial_batch_size, shuffle=False)
                    
                    # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
                    trial_trainer = ChronosModelTrainer(trial_model, device)
                    trial_optimizer = optim.Adam(trial_model.parameters(), lr=trial_lr)
                    trial_criterion = nn.CrossEntropyLoss()
                    
                    # ì§§ì€ í•™ìŠµ (ìµœì í™”ìš©)
                    best_val_acc = 0
                    patience = 5
                    patience_counter = 0
                    
                    for epoch in range(min(epochs, 30)):  # ìµœëŒ€ 30 ì—í¬í¬ë¡œ ì œí•œ
                        train_loss, train_acc = trial_trainer.train_epoch(trial_train_loader, trial_optimizer, trial_criterion)
                        test_results = trial_trainer.evaluate(trial_test_loader, trial_criterion)
                        val_acc = test_results['accuracy']
                        
                        if val_acc > best_val_acc:
                            best_val_acc = val_acc
                            patience_counter = 0
                        else:
                            patience_counter += 1
                            if patience_counter >= patience:
                                break
                        
                        # Optuna pruning
                        trial.report(val_acc, epoch)
                        if trial.should_prune():
                            raise optuna.TrialPruned()
                    
                    return best_val_acc
                    
                except Exception as e:
                    print(f"Trial ì‹¤íŒ¨: {str(e)}")
                    return 0.0
            
            # Optuna Study ì‹¤í–‰
            study = optuna.create_study(
                direction='maximize',
                sampler=TPESampler(seed=42),
                pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=5)
            )
            
            n_optimization_trials = 50  # ë² ì´ì§€ì•ˆ ìµœì í™” 50íšŒë¡œ ì„¤ì •
            print(f"ğŸš€ {n_optimization_trials}íšŒ ë² ì´ì§€ì•ˆ ìµœì í™” ì‹œí–‰...")
            study.optimize(objective, n_trials=n_optimization_trials, timeout=1800)  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
            
            # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ìƒì„±
            best_params = study.best_params
            print(f"âœ… ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_params}")
            
            model = create_hybrid_model(
                input_size=input_size,
                gru_hidden=best_params['gru_hidden'],
                cnn_filters=best_params['cnn_filters'],
                kernel_sizes=[2, 3],
                dropout=best_params['dropout']
            )
            
            # ìµœì  ì„¤ì • ì ìš©
            learning_rate = best_params['learning_rate']
            batch_size = best_params['batch_size']
            
            # ë°ì´í„°ë¡œë” ì¬ìƒì„± (ìµœì  ë°°ì¹˜ í¬ê¸°ë¡œ)
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
            
        else:
            # ê¸°ì¡´ ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©
            print("âš™ï¸ ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ìƒì„±...")
            model = create_hybrid_model(
                input_size=input_size,
                gru_hidden=32,
                cnn_filters=16,
                kernel_sizes=[2, 3],
                dropout=0.2
            )
        
        model.to(device)
        
        # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        trainer = ChronosModelTrainer(model, device)
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()
        
        # í•™ìŠµ ì§„í–‰
        train_losses = []
        train_accuracies = []
        test_losses = []
        test_accuracies = []
        
        for epoch in range(epochs):
            # í•™ìŠµ
            train_loss, train_acc = trainer.train_epoch(train_loader, optimizer, criterion)
            train_losses.append(train_loss)
            train_accuracies.append(train_acc)
            
            # í‰ê°€
            test_results = trainer.evaluate(test_loader, criterion)
            test_losses.append(test_results['loss'])
            test_accuracies.append(test_results['accuracy'])
            
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs} - Train Acc: {train_acc:.4f}, Test Acc: {test_results['accuracy']:.4f}")
        
        # ìµœì¢… í‰ê°€
        final_results = trainer.evaluate(test_loader, criterion)
        
        # ëª¨ë¸ ì €ì¥
        training_info = {
            'epochs': epochs,
            'batch_size': batch_size,
            'learning_rate': learning_rate,
            'final_accuracy': final_results['accuracy'],
            'train_losses': train_losses,
            'test_losses': test_losses,
            'train_accuracies': train_accuracies,
            'test_accuracies': test_accuracies
        }
        
        save_model(model, processor, training_info)
        
        return jsonify({
            'message': 'ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'results': {
                'final_accuracy': final_results['accuracy'],
                'final_loss': final_results['loss'],
                'training_epochs': epochs,
                'data_size': len(X),
                'feature_count': input_size
            }
        })
        
    except Exception as e:
        print(f"âŒ í•™ìŠµ ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/api/optimize_hyperparameters', methods=['POST'])
def optimize_hyperparameters():
    """
    Optunaë¥¼ ì‚¬ìš©í•œ ë² ì´ì§€ì•ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
    """
    global model, processor
    
    if not OPTUNA_AVAILABLE:
        return jsonify({
            'error': 'Optunaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install optunaë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.'
        }), 400
    
    try:
        if processor is None or processor.ts_data is None:
            return jsonify({'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        # ìš”ì²­ íŒŒë¼ë¯¸í„° íŒŒì‹±
        params = request.get_json() or {}
        n_trials = params.get('n_trials', 50)  # ìµœì í™” ì‹œí–‰ íšŸìˆ˜
        timeout = params.get('timeout', 1800)  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
        
        print(f"ğŸ”§ Chronos ë² ì´ì§€ì•ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")
        print(f"   ì‹œí–‰ íšŸìˆ˜: {n_trials}")
        print(f"   íƒ€ì„ì•„ì›ƒ: {timeout}ì´ˆ")
        
        # ì‹œí€€ìŠ¤ ìƒì„± (ì‹œê°„ ìˆœì„œ ê³ ë ¤)
        X, y, employee_ids = processor.create_proper_sequences()
        
        # ì§ì› ê¸°ë°˜ ë¶„í•  (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ - ê°™ì€ ì§ì›ì˜ ë°ì´í„°ê°€ train/testì— ë™ì‹œ í¬í•¨ë˜ì§€ ì•ŠìŒ)
        X_train, X_test, y_train, y_test = employee_based_train_test_split(
            X, y, employee_ids, test_ratio=0.2
        )
        
        print(f"ğŸ“Š ì‹œê³„ì—´ ë°ì´í„° ê²€ì¦ ë°©ì‹:")
        print(f"   - ì§ì›ë³„ ë¶„í• : ê°™ì€ ì§ì› ë°ì´í„°ê°€ train/testì— ë™ì‹œ í¬í•¨ë˜ì§€ ì•ŠìŒ")
        print(f"   - ì‹œê°„ ìˆœì„œ ë³´ì¡´: ê° ì§ì›ì˜ ì‹œê³„ì—´ ìˆœì„œ ìœ ì§€")
        print(f"   - ë¯¸ë˜ ì˜ˆì¸¡ ë°©ì‹: ê³¼ê±° ì‹œí€€ìŠ¤ë¡œ ë¯¸ë˜ í‡´ì‚¬ ì—¬ë¶€ ì˜ˆì¸¡")
        
        # GPU ì„¤ì •
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        input_size = len(processor.feature_columns)
        
        # ìµœì í™” ëª©ì  í•¨ìˆ˜ ì •ì˜
        def objective(trial):
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ
            gru_hidden = trial.suggest_categorical('gru_hidden', [16, 32, 64, 128])
            cnn_filters = trial.suggest_categorical('cnn_filters', [8, 16, 32, 64])
            dropout = trial.suggest_float('dropout', 0.1, 0.5)
            learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)
            batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])
            epochs = trial.suggest_int('epochs', 20, 100)
            
            try:
                # ëª¨ë¸ ìƒì„±
                trial_model = create_hybrid_model(
                    input_size=input_size,
                    gru_hidden=gru_hidden,
                    cnn_filters=cnn_filters,
                    kernel_sizes=[2, 3],  # ê³ ì •
                    dropout=dropout
                )
                trial_model.to(device)
                
                # ë°ì´í„° ë¡œë” ìƒì„±
                train_dataset = TensorDataset(
                    torch.FloatTensor(X_train), 
                    torch.LongTensor(y_train)
                )
                test_dataset = TensorDataset(
                    torch.FloatTensor(X_test), 
                    torch.LongTensor(y_test)
                )
                
                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
                test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
                
                # íŠ¸ë ˆì´ë„ˆ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •
                trainer = ChronosModelTrainer(trial_model, device)
                optimizer = optim.Adam(trial_model.parameters(), lr=learning_rate)
                criterion = nn.CrossEntropyLoss()
                
                # ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•œ ë³€ìˆ˜
                best_val_acc = 0
                patience = 10
                patience_counter = 0
                
                # í•™ìŠµ ì§„í–‰
                for epoch in range(epochs):
                    train_loss, train_acc = trainer.train_epoch(train_loader, optimizer, criterion)
                    test_results = trainer.evaluate(test_loader, criterion)
                    val_acc = test_results['accuracy']
                    
                    # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
                    if val_acc > best_val_acc:
                        best_val_acc = val_acc
                        patience_counter = 0
                    else:
                        patience_counter += 1
                        if patience_counter >= patience:
                            break
                    
                    # Optuna pruning (ì¤‘ê°„ ê²°ê³¼ ê¸°ë°˜ ì¡°ê¸° ì¢…ë£Œ)
                    trial.report(val_acc, epoch)
                    if trial.should_prune():
                        raise optuna.TrialPruned()
                
                return best_val_acc
                
            except Exception as e:
                print(f"Trial ì‹¤íŒ¨: {str(e)}")
                return 0.0  # ì‹¤íŒ¨ ì‹œ ìµœì € ì ìˆ˜ ë°˜í™˜
        
        # Optuna Study ìƒì„± ë° ì‹¤í–‰
        study = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(seed=42),
            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)
        )
        
        print("ğŸš€ ë² ì´ì§€ì•ˆ ìµœì í™” ì‹œì‘...")
        study.optimize(objective, n_trials=n_trials, timeout=timeout)
        
        # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
        best_params = study.best_params
        print(f"âœ… ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_params}")
        
        # ìµœì  ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
        optimized_model = create_hybrid_model(
            input_size=input_size,
            gru_hidden=best_params['gru_hidden'],
            cnn_filters=best_params['cnn_filters'],
            kernel_sizes=[2, 3],
            dropout=best_params['dropout']
        )
        optimized_model.to(device)
        
        # ìµœì  ì„¤ì •ìœ¼ë¡œ ìµœì¢… í•™ìŠµ
        train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))
        test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))
        
        train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'], shuffle=False)
        
        trainer = ChronosModelTrainer(optimized_model, device)
        optimizer = optim.Adam(optimized_model.parameters(), lr=best_params['learning_rate'])
        criterion = nn.CrossEntropyLoss()
        
        # ìµœì¢… í•™ìŠµ
        final_epochs = min(best_params['epochs'], 50)  # ìµœëŒ€ 50 ì—í¬í¬ë¡œ ì œí•œ
        for epoch in range(final_epochs):
            train_loss, train_acc = trainer.train_epoch(train_loader, optimizer, criterion)
            if epoch % 10 == 0:
                test_results = trainer.evaluate(test_loader, criterion)
                print(f"Final Training Epoch {epoch+1}/{final_epochs} - Val Acc: {test_results['accuracy']:.4f}")
        
        # ìµœì¢… í‰ê°€
        final_results = trainer.evaluate(test_loader, criterion)
        
        # ìµœì í™”ëœ ëª¨ë¸ ì €ì¥
        model = optimized_model
        
        optimization_info = {
            'best_params': best_params,
            'best_score': study.best_value,
            'n_trials': len(study.trials),
            'optimization_time': datetime.now().isoformat(),
            'final_accuracy': final_results['accuracy'],
            'final_loss': final_results['loss']
        }
        
        save_model(model, processor, optimization_info)
        
        return jsonify({
            'message': 'Chronos ë² ì´ì§€ì•ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì™„ë£Œ',
            'optimization_results': {
                'best_hyperparameters': best_params,
                'best_validation_score': study.best_value,
                'final_test_accuracy': final_results['accuracy'],
                'final_test_loss': final_results['loss'],
                'total_trials': len(study.trials),
                'optimization_method': 'Optuna TPESampler',
                'trials_completed': len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]),
                'trials_pruned': len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])
            },
            'model_info': {
                'input_size': input_size,
                'feature_count': len(processor.feature_columns),
                'training_samples': len(X_train),
                'test_samples': len(X_test)
            }
        })
        
    except Exception as e:
        print(f"âŒ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/api/predict', methods=['POST'])
def predict():
    """
    ì˜ˆì¸¡ ìˆ˜í–‰
    """
    global DATA_PATH, processor, model
    
    try:
        if model is None:
            return jsonify({'error': 'ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.'}), 400
        
        if processor is None or processor.processed_data is None:
            return jsonify({'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        # ìš”ì²­ íŒŒë¼ë¯¸í„° íŒŒì‹±
        params = request.get_json() or {}
        
        # employee_id ë‹¨ì¼ ìš”ì²­ ì²˜ë¦¬
        if 'employee_id' in params:
            employee_ids = [params['employee_id']]
        else:
            employee_ids = params.get('employee_ids', [])
            
        analysis_type = params.get('analysis_type', 'batch')
        
        # ë¶„ì„ íƒ€ì…ì— ë”°ë¥¸ ë°ì´í„° ê²½ë¡œ í™•ì¸ ë° ì¬ë¡œë“œ
        new_data_paths = get_chronos_data_paths(analysis_type)
        current_data_paths = get_chronos_data_paths()
        
        if new_data_paths != current_data_paths:
            logger.info(f"ğŸ”„ Chronos: {analysis_type} ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¬ë¡œë“œ")
            DATA_PATH = new_data_paths
            
            # ë°ì´í„° ì¬ë¡œë“œ í•„ìš” ì‹œ processor ì¬ì´ˆê¸°í™”
            if processor and (new_data_paths['timeseries'] != current_data_paths['timeseries'] or 
                            new_data_paths['personas'] != current_data_paths['personas']):
                try:
                    processor.load_data(new_data_paths['timeseries'], new_data_paths['personas'])
                    processor.detect_columns()
                    processor.preprocess_data()
                    processor.identify_features()
                    logger.info(f"âœ… Chronos {analysis_type} ë°ì´í„° ì¬ë¡œë“œ ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"âš ï¸ Chronos ë°ì´í„° ì¬ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        logger.info(f"ğŸ“Š Chronos {analysis_type} ë¶„ì„ ì‹œì‘")
        
        if not employee_ids:
            # ëª¨ë“  ì§ì› ì˜ˆì¸¡
            if processor.processed_data is not None:
                employee_ids = processor.processed_data['employee_id'].unique().tolist()
            else:
                return jsonify({'error': 'ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.'}), 400
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        predictions = []
        
        for emp_id in employee_ids:
            emp_data = processor.processed_data[
                processor.processed_data['employee_id'] == emp_id
            ].sort_values('period')
            
            if len(emp_data) >= processor.sequence_length:
                # í”¼ì²˜ ë°ì´í„° ì¶”ì¶œ ë° ì •ê·œí™”
                feature_data = emp_data[processor.feature_columns].values
                feature_data_scaled = processor.scaler.transform(feature_data)
                
                # ì‹œí€€ìŠ¤ ìƒì„±
                sequence = feature_data_scaled[-processor.sequence_length:]
                X_tensor = torch.FloatTensor(sequence).unsqueeze(0).to(device)
                
                # ì˜ˆì¸¡ ë° í•´ì„
                model.eval()
                with torch.no_grad():
                    interpretation = model.interpret_prediction(X_tensor, processor.feature_columns)
                    
                    pred_class = np.argmax(interpretation['predictions'][0])
                    pred_prob = interpretation['probabilities'][0][1]  # í‡´ì‚¬ í™•ë¥  (Temperature Scaling ì ìš©ë¨)
                    
                    # NaN ê°’ ì²˜ë¦¬
                    if np.isnan(pred_prob) or np.isinf(pred_prob):
                        pred_prob = 0.0
                    
                    # ì•ˆì „í•œ float ë³€í™˜
                    safe_pred_prob = float(pred_prob)
                    
                    predictions.append({
                        'employee_id': int(emp_id),
                        'attrition_probability': safe_pred_prob,
                        'predicted_class': int(pred_class),
                        'risk_level': 'High' if safe_pred_prob > 0.7 else 'Medium' if safe_pred_prob > 0.3 else 'Low',
                        'feature_importance': {
                            name: float(importance) if not (np.isnan(importance) or np.isinf(importance)) else 0.0
                            for name, importance in zip(
                                processor.feature_columns, 
                                interpretation['feature_importance']
                            )
                        },
                        'temporal_attention': interpretation['temporal_attention'][0].tolist() if interpretation['temporal_attention'].ndim > 1 else interpretation['temporal_attention'].tolist()
                    })
        
        # ê²°ê³¼ ì •ë ¬ (í‡´ì‚¬ í™•ë¥  ë†’ì€ ìˆœ)
        predictions.sort(key=lambda x: x['attrition_probability'], reverse=True)
        
        logger.info(f"ğŸ‰ Chronos {analysis_type} ë¶„ì„ ì™„ë£Œ - ì´ {len(predictions)}ëª… ì˜ˆì¸¡ ì™„ë£Œ")
        
        # ì•ˆì „í•œ í‰ê·  ê³„ì‚° (ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬)
        avg_prob = np.mean([p['attrition_probability'] for p in predictions]) if predictions else 0.0
        
        response_data = {
            'predictions': predictions,
            'summary': {
                'total_employees': len(predictions),
                'high_risk_count': sum(1 for p in predictions if p['risk_level'] == 'High'),
                'medium_risk_count': sum(1 for p in predictions if p['risk_level'] == 'Medium'),
                'low_risk_count': sum(1 for p in predictions if p['risk_level'] == 'Low'),
                'average_attrition_probability': avg_prob
            }
        }
        
        # NaN ê°’ ì•ˆì „ ì²˜ë¦¬
        safe_response = safe_json_serialize(response_data)
        return jsonify(safe_response)
        
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/api/feature_importance')
def get_feature_importance():
    """
    Feature importance ì‹œê°í™” (ì „ì²´ í‰ê· )
    """
    try:
        if model is None:
            return jsonify({'error': 'ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        # ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ í‰ê·  feature importance ê³„ì‚°
        X, y, employee_ids = processor.create_proper_sequences()
        X_tensor = torch.FloatTensor(X).to(device)
        
        model.eval()
        feature_importances = []
        
        # gradient ê³„ì‚°ì„ ìœ„í•´ torch.no_grad() ì œê±°
        for i in range(0, len(X_tensor), 32):  # ë°°ì¹˜ ì²˜ë¦¬
            batch = X_tensor[i:i+32]
            interpretation = model.interpret_prediction(batch, processor.feature_columns)
            feature_importances.append(interpretation['feature_importance'])
        
        # í‰ê·  ê³„ì‚°
        avg_importance = np.mean(feature_importances, axis=0)
        
        # ì‹œê°í™” ìƒì„±
        html_plot = visualizer.plot_feature_importance(
            avg_importance, 
            processor.feature_columns,
            "Average Feature Importance Across All Employees"
        )
        
        return html_plot, 200, {'Content-Type': 'text/html'}
        
    except Exception as e:
        print(f"âŒ Feature importance ì˜¤ë¥˜: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/employee_feature_importance')
def get_employee_feature_importance():
    """
    ì§ì›ë³„ Attention & Gradient ê¸°ë°˜ Feature Importance ì‹œê°í™”
    """
    try:
        if model is None:
            return jsonify({'error': 'ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        if processor is None or processor.processed_data is None:
            return jsonify({'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        print("ğŸš€ ì§ì›ë³„ Feature Importance ë¶„ì„ ì‹œì‘")
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
        X, y, employee_ids = processor.create_proper_sequences()
        
        # ì§ì›ë³„ Attention ì¤‘ìš”ë„ ê³„ì‚°
        attention_results = visualizer.calculate_attention_importance_per_employee(
            model, X, y, employee_ids, processor.feature_columns, device
        )
        
        # ì§ì›ë³„ Gradient ì¤‘ìš”ë„ ê³„ì‚°
        gradient_results = visualizer.calculate_gradient_importance_per_employee(
            model, X, y, employee_ids, processor.feature_columns, device, max_samples_per_employee=5
        )
        
        # ì§ì›ë³„ ë¹„êµ ì‹œê°í™”
        comparison_html = visualizer.plot_employee_feature_importance_comparison(
            attention_results, gradient_results, processor.feature_columns,
            top_employees=10, top_features=10
        )
        
        return comparison_html, 200, {'Content-Type': 'text/html'}
        
    except Exception as e:
        print(f"âŒ ì§ì›ë³„ Feature importance ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/api/employee_attention_heatmap')
def get_employee_attention_heatmap():
    """
    ì§ì›ë³„ Attention Weights íˆíŠ¸ë§µ
    """
    try:
        if model is None:
            return jsonify({'error': 'ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        if processor is None or processor.processed_data is None:
            return jsonify({'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        print("ğŸš€ ì§ì›ë³„ Attention Heatmap ìƒì„± ì‹œì‘")
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
        X, y, employee_ids = processor.create_proper_sequences()
        
        # ì§ì›ë³„ Attention ì¤‘ìš”ë„ ê³„ì‚°
        attention_results = visualizer.calculate_attention_importance_per_employee(
            model, X, y, employee_ids, processor.feature_columns, device
        )
        
        # Attention íˆíŠ¸ë§µ ìƒì„±
        heatmap_html = visualizer.plot_employee_attention_heatmap(
            attention_results, processor.feature_columns, employee_ids_to_show=25
        )
        
        return heatmap_html, 200, {'Content-Type': 'text/html'}
        
    except Exception as e:
        print(f"âŒ Attention Heatmap ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/api/feature_importance_distribution')
def get_feature_importance_distribution():
    """
    Feature Importance ë¶„í¬ ë¶„ì„
    """
    try:
        if model is None:
            return jsonify({'error': 'ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        if processor is None or processor.processed_data is None:
            return jsonify({'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        print("ğŸš€ Feature Importance ë¶„í¬ ë¶„ì„ ì‹œì‘")
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
        X, y, employee_ids = processor.create_proper_sequences()
        
        # ì§ì›ë³„ Attention ì¤‘ìš”ë„ ê³„ì‚°
        attention_results = visualizer.calculate_attention_importance_per_employee(
            model, X, y, employee_ids, processor.feature_columns, device
        )
        
        # ì§ì›ë³„ Gradient ì¤‘ìš”ë„ ê³„ì‚°
        gradient_results = visualizer.calculate_gradient_importance_per_employee(
            model, X, y, employee_ids, processor.feature_columns, device, max_samples_per_employee=3
        )
        
        # ë¶„í¬ ë¶„ì„ ì‹œê°í™”
        distribution_html = visualizer.plot_feature_importance_distribution(
            attention_results, gradient_results, processor.feature_columns
        )
        
        return distribution_html, 200, {'Content-Type': 'text/html'}
        
    except Exception as e:
        print(f"âŒ Feature Importance ë¶„í¬ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/api/model_analysis')
def get_model_analysis():
    """
    ëª¨ë¸ ë¶„ì„ ëŒ€ì‹œë³´ë“œ
    """
    try:
        if model is None:
            return jsonify({'error': 'ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        # ì „ì²´ ë°ì´í„° ì˜ˆì¸¡
        X, y, employee_ids = processor.create_proper_sequences()
        X_tensor = torch.FloatTensor(X).to(device)
        y_tensor = torch.LongTensor(y)
        
        model.eval()
        all_predictions = []
        all_probabilities = []
        
        with torch.no_grad():
            for i in range(0, len(X_tensor), 32):
                batch_x = X_tensor[i:i+32]
                outputs = model(batch_x)
                probabilities = torch.softmax(outputs, dim=1)
                predictions = torch.argmax(outputs, dim=1)
                
                all_predictions.extend(predictions.cpu().numpy())
                all_probabilities.extend(probabilities.cpu().numpy())
        
        all_predictions = np.array(all_predictions)
        all_probabilities = np.array(all_probabilities)
        
        # ë¶„ì„ ì‹œê°í™” ìƒì„±
        html_plot = visualizer.plot_prediction_analysis(
            all_predictions, all_probabilities, y
        )
        
        return html_plot, 200, {'Content-Type': 'text/html'}
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/employee_timeline/<int:employee_id>')
def get_employee_timeline(employee_id):
    """
    ê°œë³„ ì§ì› ì‹œê³„ì—´ ë¶„ì„
    """
    try:
        if processor is None or processor.processed_data is None:
            return jsonify({'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        # ì§ì› ë°ì´í„° ì¶”ì¶œ
        emp_data = processor.processed_data[
            processor.processed_data['employee_id'] == employee_id
        ].sort_values('period')
        
        if len(emp_data) == 0:
            return jsonify({'error': f'ì§ì› ID {employee_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 404
        
        attention_weights = None
        
        # ëª¨ë¸ì´ ìˆìœ¼ë©´ attention weights ê³„ì‚°
        if model is not None and len(emp_data) >= processor.sequence_length:
            # ì‹œê°„ë³„ ì§‘ê³„ ì²˜ë¦¬
            emp_data['year'] = emp_data[processor.date_column].dt.year
            emp_data['week'] = emp_data[processor.date_column].dt.isocalendar().week
            emp_data['time_period'] = emp_data['year'].astype(str) + '-W' + emp_data['week'].astype(str).str.zfill(2)
            
            agg_data = emp_data.groupby('time_period')[processor.feature_columns].mean().reset_index()
            agg_data = agg_data.sort_values('time_period')
            
            # ìµœê·¼ ì‹œí€€ìŠ¤ ìƒì„±
            if len(agg_data) >= processor.sequence_length:
                sequence_data = agg_data[processor.feature_columns].values[-processor.sequence_length:]
                
                # ì •ê·œí™”
                sequence_scaled = processor.scaler.transform(sequence_data.reshape(-1, len(processor.feature_columns)))
                sequence_scaled = sequence_scaled.reshape(1, processor.sequence_length, -1)
                
                X_tensor = torch.FloatTensor(sequence_scaled).to(device)
                
                model.eval()
                with torch.no_grad():
                    interpretation = model.interpret_prediction(X_tensor, processor.feature_columns)
                    attention_weights = interpretation['temporal_attention'][0] if interpretation['temporal_attention'].ndim > 1 else interpretation['temporal_attention']
        
        # ì‹œê°í™” ìƒì„±
        html_plot = visualizer.create_employee_timeline(emp_data, attention_weights)
        
        return html_plot, 200, {'Content-Type': 'text/html'}
        
    except Exception as e:
        print(f"âŒ ì§ì› íƒ€ì„ë¼ì¸ ì˜¤ë¥˜: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/batch-analysis', methods=['POST'])
def batch_analysis():
    """ë°°ì¹˜ ë¶„ì„: Post ë°ì´í„°ë¡œ í•™ìŠµ â†’ Batch ë°ì´í„°ë¡œ ì˜ˆì¸¡"""
    global DATA_PATH, processor, model
    
    try:
        print("ğŸš€ Chronos ë°°ì¹˜ ë¶„ì„ ì‹œì‘: Post ë°ì´í„° í•™ìŠµ â†’ Batch ë°ì´í„° ì˜ˆì¸¡")
        
        # 1ë‹¨ê³„: Post ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ
        post_data_paths = get_chronos_data_paths('post')
        if not post_data_paths['timeseries'] or not os.path.exists(post_data_paths['timeseries']):
            return jsonify({"error": "Post ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì‚¬í›„ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”."}), 400
        
        print(f"ğŸ“š Post ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ: {post_data_paths['timeseries']}")
        
        # Post ë°ì´í„°ë¡œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        DATA_PATH = post_data_paths
        processor = ProperTimeSeriesProcessor(sequence_length=50, aggregation_unit='week')
        
        # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        processor.load_data(post_data_paths['timeseries'], post_data_paths.get('personas'))
        processor.detect_columns()
        processor.preprocess_data()
        processor.identify_features()
        
        # Post ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ (í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í¬í•¨)
        print("ğŸ”§ Post ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y, employee_ids = processor.create_proper_sequences()
        
        # ì§ì› ê¸°ë°˜ ë¶„í• 
        from chronos_processor_fixed import employee_based_train_test_split
        X_train, X_test, y_train, y_test = employee_based_train_test_split(
            X, y, employee_ids, test_ratio=0.2
        )
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ìˆ˜í–‰
        if OPTUNA_AVAILABLE:
            print("ğŸš€ Post ë°ì´í„° ê¸°ë°˜ ë² ì´ì§€ì•ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")
            
            def objective(trial):
                # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ
                gru_hidden = trial.suggest_categorical('gru_hidden', [16, 32, 64, 128])
                cnn_filters = trial.suggest_categorical('cnn_filters', [8, 16, 32, 64])
                dropout = trial.suggest_float('dropout', 0.1, 0.5)
                trial_lr = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)
                trial_batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])
                
                try:
                    # ëª¨ë¸ ìƒì„±
                    trial_model = create_hybrid_model(
                        input_size=len(processor.feature_columns),
                        gru_hidden=gru_hidden,
                        cnn_filters=cnn_filters,
                        kernel_sizes=[2, 3],
                        dropout=dropout
                    )
                    trial_model.to(device)
                    
                    # ë°ì´í„° ë¡œë” (ë°°ì¹˜ í¬ê¸° ì ìš©)
                    from torch.utils.data import TensorDataset, DataLoader
                    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))
                    test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))
                    
                    trial_train_loader = DataLoader(train_dataset, batch_size=trial_batch_size, shuffle=True)
                    trial_test_loader = DataLoader(test_dataset, batch_size=trial_batch_size, shuffle=False)
                    
                    # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
                    from chronos_models import ChronosModelTrainer
                    import torch.optim as optim
                    import torch.nn as nn
                    
                    trial_trainer = ChronosModelTrainer(trial_model, device)
                    trial_optimizer = optim.Adam(trial_model.parameters(), lr=trial_lr)
                    trial_criterion = nn.CrossEntropyLoss()
                    
                    # í•™ìŠµ (30 ì—í¬í¬ë¡œ ì œí•œ)
                    best_val_acc = 0
                    patience = 5
                    patience_counter = 0
                    
                    for epoch in range(30):
                        train_loss, train_acc = trial_trainer.train_epoch(trial_train_loader, trial_optimizer, trial_criterion)
                        test_results = trial_trainer.evaluate(trial_test_loader, trial_criterion)
                        val_acc = test_results['accuracy']
                        
                        if val_acc > best_val_acc:
                            best_val_acc = val_acc
                            patience_counter = 0
                        else:
                            patience_counter += 1
                            if patience_counter >= patience:
                                break
                        
                        # Optuna pruning
                        trial.report(val_acc, epoch)
                        if trial.should_prune():
                            raise optuna.TrialPruned()
                    
                    return best_val_acc
                    
                except Exception as e:
                    print(f"Trial ì‹¤íŒ¨: {str(e)}")
                    return 0.0
            
            # Optuna Study ì‹¤í–‰
            study = optuna.create_study(
                direction='maximize',
                sampler=optuna.samplers.TPESampler(seed=42),
                pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=5)
            )
            
            n_optimization_trials = 30  # ë°°ì¹˜ ë¶„ì„ìš©ìœ¼ë¡œ 30íšŒë¡œ ì œí•œ
            print(f"ğŸš€ {n_optimization_trials}íšŒ ë² ì´ì§€ì•ˆ ìµœì í™” ì‹œí–‰...")
            study.optimize(objective, n_trials=n_optimization_trials, timeout=1200)  # 20ë¶„ íƒ€ì„ì•„ì›ƒ
            
            # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ìƒì„±
            best_params = study.best_params
            print(f"âœ… ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_params}")
            
            model = create_hybrid_model(
                input_size=len(processor.feature_columns),
                gru_hidden=best_params['gru_hidden'],
                cnn_filters=best_params['cnn_filters'],
                kernel_sizes=[2, 3],
                dropout=best_params['dropout']
            )
            model.to(device)
            
            # ìµœì  ì„¤ì •ìœ¼ë¡œ ìµœì¢… í•™ìŠµ
            from torch.utils.data import TensorDataset, DataLoader
            from chronos_models import ChronosModelTrainer
            import torch.optim as optim
            import torch.nn as nn
            
            train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))
            train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)
            
            trainer = ChronosModelTrainer(model, device)
            optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])
            criterion = nn.CrossEntropyLoss()
            
            # ìµœì¢… í•™ìŠµ
            for epoch in range(50):
                train_loss, train_acc = trainer.train_epoch(train_loader, optimizer, criterion)
                if epoch % 10 == 0:
                    print(f"ìµœì¢… í•™ìŠµ Epoch {epoch+1}/50 - Acc: {train_acc:.4f}")
            
            training_result = {
                "message": "Post ë°ì´í„° ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì™„ë£Œ", 
                "best_params": best_params,
                "best_score": study.best_value,
                "epochs": 50
            }
        else:
            # Optuna ì—†ëŠ” ê²½ìš° ê³ ì • íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµ
            print("âš™ï¸ ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ Post ë°ì´í„° í•™ìŠµ")
            model = create_hybrid_model(
                input_size=len(processor.feature_columns),
                gru_hidden=32,
                cnn_filters=16,
                dropout=0.2
            )
            model.to(device)
            
            from torch.utils.data import TensorDataset, DataLoader
            from chronos_models import ChronosModelTrainer
            import torch.optim as optim
            import torch.nn as nn
            
            train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))
            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
            
            trainer = ChronosModelTrainer(model, device)
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            criterion = nn.CrossEntropyLoss()
            
            for epoch in range(50):
                train_loss, train_acc = trainer.train_epoch(train_loader, optimizer, criterion)
                if epoch % 10 == 0:
                    print(f"í•™ìŠµ Epoch {epoch+1}/50 - Acc: {train_acc:.4f}")
            
            training_result = {"message": "Post ë°ì´í„° ê¸°ë°˜ ê³ ì • íŒŒë¼ë¯¸í„° í•™ìŠµ ì™„ë£Œ", "epochs": 50}
        print(f"âœ… Chronos ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {training_result}")
        
        # 2ë‹¨ê³„: Batch ë°ì´í„°ë¡œ ì˜ˆì¸¡
        batch_data_paths = get_chronos_data_paths('batch')
        if not batch_data_paths['timeseries'] or not os.path.exists(batch_data_paths['timeseries']):
            return jsonify({"error": "Batch ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°°ì¹˜ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."}), 400
        
        print(f"ğŸ”® Batch ë°ì´í„°ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰: {batch_data_paths['timeseries']}")
        
        # Batch ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        batch_processor = ProperTimeSeriesProcessor(sequence_length=50, aggregation_unit='week')
        batch_processor.load_data(batch_data_paths['timeseries'], batch_data_paths.get('personas'))
        batch_processor.detect_columns()
        batch_processor.preprocess_data()
        batch_processor.identify_features()
        
        # Batch ë°ì´í„° ì‹œí€€ìŠ¤ ìƒì„±
        X_batch, y_batch, batch_employee_ids = batch_processor.create_proper_sequences()
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        model.eval()
        X_batch_tensor = torch.FloatTensor(X_batch).to(device)
        
        with torch.no_grad():
            outputs = model(X_batch_tensor)
            probabilities = torch.softmax(outputs, dim=1)
            predictions = probabilities[:, 1].cpu().numpy()  # í‡´ì‚¬ í™•ë¥ 
        
        # ê²°ê³¼ í¬ë§·íŒ…
        results = []
        
        for i, (emp_id, pred) in enumerate(zip(batch_employee_ids, predictions)):
            # XAI ì •ë³´ ìƒì„± (Attention weights)
            xai_info = {}
            try:
                # ê°œë³„ ìƒ˜í”Œì— ëŒ€í•œ attention weights ì¶”ì¶œ
                sample_tensor = X_batch_tensor[i:i+1]
                attention_weights = model.get_attention_weights(sample_tensor)
                xai_info['attention_weights'] = attention_weights.cpu().numpy().tolist()
            except Exception as e:
                print(f"Attention weights ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                xai_info['attention_weights'] = []
            
            # ì‹œí€€ìŠ¤ ì¤‘ìš”ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë°©ì‹)
            sequence_importance = {}
            try:
                sequence_length = X_batch.shape[1]
                for t in range(sequence_length):
                    # ì‹œê°„ ê°€ì¤‘ì¹˜ ì ìš© (ìµœê·¼ ì‹œì ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜)
                    time_weight = (t + 1) / sequence_length
                    sequence_importance[f'timestep_{t}'] = float(pred * time_weight)
            except Exception as e:
                print(f"Sequence importance ê³„ì‚° ì‹¤íŒ¨: {e}")
                sequence_importance = {}
            
            # NaN ê°’ ì²˜ë¦¬
            safe_pred = float(pred) if not (np.isnan(pred) or np.isinf(pred)) else 0.0
            
            results.append({
                'employee_id': str(emp_id),
                'risk_score': safe_pred,
                'predicted_attrition': int(safe_pred > 0.5),
                'confidence': 0.8,  # Chronos ê¸°ë³¸ ì‹ ë¢°ë„
                'sequence_length': processor.sequence_length,
                'features_used': processor.feature_dim,
                'xai_explanation': {
                    'attention_weights': xai_info.get('attention_weights', {}),
                    'sequence_importance': sequence_importance,
                    'trend_analysis': {
                        'prediction_trend': 'increasing' if safe_pred > 0.5 else 'stable',
                        'confidence_level': 'high' if abs(safe_pred - 0.5) > 0.3 else 'medium'
                    }
                },
                'model_metadata': {
                    'model_type': 'GRU_CNN_Hybrid',
                    'sequence_length': processor.sequence_length,
                    'feature_dimensions': len(processor.feature_columns),
                    'training_optimized': True  # Post ë°ì´í„°ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ìˆ˜í–‰
                }
            })
        
        print(f"âœ… Chronos ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: {len(results)}ëª… ì˜ˆì¸¡")
        
        response_data = {
            "success": True,
            "message": f"Chronos ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: Post ë°ì´í„° í•™ìŠµ â†’ {len(results)}ëª… Batch ì˜ˆì¸¡",
            "agent": "chronos",
            "training_data_path": post_data_paths['timeseries'],
            "prediction_data_path": batch_data_paths['timeseries'],
            "total_predictions": len(results),
            "predictions": results,
            "model_info": {
                "training_samples": len(X_train),
                "sequence_length": processor.sequence_length,
                "feature_dim": len(processor.feature_columns),
                "model_type": "GRU_CNN_Hybrid with Post-Data Hyperparameter Optimization"
            }
        }
        
        # NaN ê°’ ì•ˆì „ ì²˜ë¦¬
        safe_response = safe_json_serialize(response_data)
        return jsonify(safe_response)
        
    except Exception as e:
        print(f"âŒ Chronos ë°°ì¹˜ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        return jsonify({"error": f"Chronos ë°°ì¹˜ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"}), 500

if __name__ == '__main__':
    print("ğŸš€ Chronos Flask ì„œë²„ ì‹œì‘ ì¤‘...")
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    if initialize_system():
        print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        print("ğŸŒ ì„œë²„ ì£¼ì†Œ: http://localhost:5003")
        print("ğŸ“‹ API ë¬¸ì„œ: http://localhost:5003")
        app.run(host='0.0.0.0', port=5003, debug=True)
    else:
        print("âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
