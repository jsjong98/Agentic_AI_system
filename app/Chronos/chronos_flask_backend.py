# ============================================================================
# Chronos Flask ë°±ì—”ë“œ API
# ============================================================================

from flask import Flask, request, jsonify, render_template_string
from flask_cors import CORS
from werkzeug.utils import secure_filename
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
import joblib
import os
import json
from datetime import datetime
import traceback
from typing import Dict, List, Any
import warnings
warnings.filterwarnings('ignore')

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìœ„í•œ Optuna import
try:
    import optuna
    from optuna.samplers import TPESampler
    OPTUNA_AVAILABLE = True
    print("âœ… Optuna ì‚¬ìš© ê°€ëŠ¥ - ë² ì´ì§€ì•ˆ ìµœì í™” í™œì„±í™”")
except ImportError:
    OPTUNA_AVAILABLE = False
    print("âš ï¸ Optuna ë¯¸ì„¤ì¹˜ - ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©")

# ë¡œì»¬ ëª¨ë“ˆ import
from chronos_models import GRU_CNN_HybridModel, ChronosModelTrainer, create_hybrid_model, create_attention_model
from chronos_processor_fixed import ProperTimeSeriesProcessor, ChronosVisualizer, employee_based_train_test_split

def time_series_cross_validation(X, y, employee_ids, n_splits=3):
    """
    ì‹œê³„ì—´ êµì°¨ ê²€ì¦ - ì‹œê°„ ìˆœì„œë¥¼ ê³ ë ¤í•œ ì§ì›ë³„ ë¶„í• 
    ê° foldì—ì„œ ì´ì „ ì‹œì ì˜ ì§ì›ë“¤ë¡œ í•™ìŠµí•˜ê³  ì´í›„ ì‹œì ì˜ ì§ì›ë“¤ë¡œ ê²€ì¦
    """
    unique_employees = np.unique(employee_ids)
    n_employees = len(unique_employees)
    
    # ì§ì›ì„ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬ (ê°€ì •: employee_idê°€ ì‹œê°„ ìˆœì„œì™€ ì—°ê´€)
    # ì‹¤ì œë¡œëŠ” ê° ì§ì›ì˜ ì²« ì‹œì ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•´ì•¼ í•¨
    sorted_employees = np.sort(unique_employees)
    
    fold_size = n_employees // n_splits
    cv_results = []
    
    for fold in range(n_splits):
        # ì‹œê°„ ê¸°ë°˜ ë¶„í• : ì´ì „ ì‹œì  ì§ì›ë“¤ë¡œ í•™ìŠµ, ì´í›„ ì‹œì  ì§ì›ë“¤ë¡œ ê²€ì¦
        if fold == n_splits - 1:  # ë§ˆì§€ë§‰ fold
            train_employees = sorted_employees[:fold * fold_size]
            val_employees = sorted_employees[fold * fold_size:]
        else:
            train_employees = sorted_employees[:fold * fold_size + fold_size]
            val_employees = sorted_employees[fold * fold_size + fold_size:(fold + 1) * fold_size + fold_size]
        
        if len(train_employees) == 0 or len(val_employees) == 0:
            continue
            
        # ë§ˆìŠ¤í¬ ìƒì„±
        train_mask = np.isin(employee_ids, train_employees)
        val_mask = np.isin(employee_ids, val_employees)
        
        cv_results.append({
            'train_indices': np.where(train_mask)[0],
            'val_indices': np.where(val_mask)[0],
            'train_employees': train_employees,
            'val_employees': val_employees
        })
    
    return cv_results

app = Flask(__name__)
CORS(app)

# íŒŒì¼ ì—…ë¡œë“œ í¬ê¸° ì œí•œì„ 300MBë¡œ ì„¤ì •
app.config['MAX_CONTENT_LENGTH'] = 300 * 1024 * 1024  # 300MB

# ì „ì—­ ë³€ìˆ˜
processor = None
model = None
visualizer = None
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ë°ì´í„° ê²½ë¡œ ì„¤ì •
DATA_PATH = {
    'timeseries': 'data/IBM_HR_timeseries.csv',
    'personas': 'data/IBM_HR_personas_assigned.csv'  # í˜ë¥´ì†Œë‚˜ ì •ë³´ í¬í•¨
}

MODEL_PATH = 'app/Chronos/models'
os.makedirs(MODEL_PATH, exist_ok=True)

def initialize_system():
    """
    ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    """
    global processor, model, visualizer
    
    try:
        print("ğŸš€ Chronos ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # ê°œì„ ëœ í”„ë¡œì„¸ì„œ ë° ì‹œê°í™” ë„êµ¬ ì´ˆê¸°í™”
        processor = ProperTimeSeriesProcessor(sequence_length=50, aggregation_unit='week')
        visualizer = ChronosVisualizer()
        
        # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        if os.path.exists(DATA_PATH['timeseries']) and os.path.exists(DATA_PATH['personas']):
            processor.load_data(DATA_PATH['timeseries'], DATA_PATH['personas'])
            processor.detect_columns()
            processor.preprocess_data()
            processor.identify_features()
            print("âœ… ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì™„ë£Œ")
        else:
            print("âš ï¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµëœ ëª¨ë¸ë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹œë„
        model_file = os.path.join(MODEL_PATH, 'chronos_attention_model.pth')
        if os.path.exists(model_file):
            load_model()
            print("âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        else:
            print("â„¹ï¸ ê¸°ì¡´ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ëª¨ë¸ì„ í•™ìŠµí•´ì£¼ì„¸ìš”.")
            
        print("ğŸ‰ Chronos ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        traceback.print_exc()
        return False

def load_model():
    """
    ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ
    """
    global model
    
    try:
        model_file = os.path.join(MODEL_PATH, 'chronos_attention_model.pth')
        scaler_file = os.path.join(MODEL_PATH, 'chronos_scaler.joblib')
        
        if os.path.exists(model_file) and os.path.exists(scaler_file):
            # ëª¨ë¸ ë¡œë“œ
            checkpoint = torch.load(model_file, map_location=device)
            model = create_hybrid_model(
                input_size=checkpoint['input_size'],
                gru_hidden=checkpoint.get('gru_hidden', 32),
                cnn_filters=checkpoint.get('cnn_filters', 16),
                kernel_sizes=checkpoint.get('kernel_sizes', [2, 3]),
                dropout=checkpoint.get('dropout', 0.2)
            )
            model.load_state_dict(checkpoint['model_state_dict'])
            model.to(device)
            model.eval()
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
            if processor:
                processor.scaler = joblib.load(scaler_file)
                processor.feature_columns = checkpoint.get('feature_columns', [])
            
            print("âœ… ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
            return True
        else:
            print("âš ï¸ ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return False

def save_model(model, processor, additional_info=None):
    """
    ëª¨ë¸ ì €ì¥
    """
    try:
        model_file = os.path.join(MODEL_PATH, 'chronos_attention_model.pth')
        scaler_file = os.path.join(MODEL_PATH, 'chronos_scaler.joblib')
        
        # ëª¨ë¸ ì €ì¥
        checkpoint = {
            'model_state_dict': model.state_dict(),
            'input_size': model.input_size,
            'gru_hidden': model.gru_hidden,
            'cnn_filters': model.cnn_filters,
            'feature_columns': processor.feature_columns,
            'sequence_length': processor.sequence_length,
            'aggregation_unit': processor.aggregation_unit,
            'timestamp': datetime.now().isoformat()
        }
        
        if additional_info:
            checkpoint.update(additional_info)
            
        torch.save(checkpoint, model_file)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        joblib.save(processor.scaler, scaler_file)
        
        print("âœ… ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ")
        return True
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return False

@app.route('/')
def home():
    """
    í™ˆí˜ì´ì§€
    """
    html_template = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Chronos - Employee Attrition Prediction</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; background-color: #f5f5f5; }
            .container { max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
            h1 { color: #2c3e50; text-align: center; }
            .api-section { margin: 20px 0; padding: 20px; background: #ecf0f1; border-radius: 5px; }
            .endpoint { margin: 10px 0; padding: 10px; background: white; border-left: 4px solid #3498db; }
            .method { font-weight: bold; color: #e74c3c; }
            .status { padding: 10px; margin: 20px 0; border-radius: 5px; }
            .status.ready { background-color: #d4edda; color: #155724; border: 1px solid #c3e6cb; }
            .status.warning { background-color: #fff3cd; color: #856404; border: 1px solid #ffeaa7; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>ğŸ•’ Chronos - Employee Attrition Prediction System</h1>
            
            <div class="status {{ status_class }}">
                <strong>ì‹œìŠ¤í…œ ìƒíƒœ:</strong> {{ status_message }}
            </div>
            
            <div class="api-section">
                <h2>ğŸ“‹ API ì—”ë“œí¬ì¸íŠ¸</h2>
                
                <div class="endpoint">
                    <span class="method">GET</span> <strong>/api/status</strong>
                    <p>ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸</p>
                </div>
                
                <div class="endpoint">
                    <span class="method">POST</span> <strong>/api/train</strong>
                    <p>ëª¨ë¸ í•™ìŠµ (JSON: {"sequence_length": 6, "epochs": 50})</p>
                </div>
                
                <div class="endpoint">
                    <span class="method">POST</span> <strong>/api/predict</strong>
                    <p>ì˜ˆì¸¡ ìˆ˜í–‰ (JSON: {"employee_ids": [1, 2, 3]})</p>
                </div>
                
                <div class="endpoint">
                    <span class="method">GET</span> <strong>/api/feature_importance</strong>
                    <p>Feature importance ì‹œê°í™”</p>
                </div>
                
                <div class="endpoint">
                    <span class="method">GET</span> <strong>/api/model_analysis</strong>
                    <p>ëª¨ë¸ ë¶„ì„ ëŒ€ì‹œë³´ë“œ</p>
                </div>
                
                <div class="endpoint">
                    <span class="method">GET</span> <strong>/api/employee_timeline/{employee_id}</strong>
                    <p>ê°œë³„ ì§ì› ì‹œê³„ì—´ ë¶„ì„</p>
                </div>
            </div>
            
            <div class="api-section">
                <h2>ğŸš€ ì‚¬ìš© ë°©ë²•</h2>
                <ol>
                    <li><strong>ëª¨ë¸ í•™ìŠµ:</strong> POST /api/trainìœ¼ë¡œ ëª¨ë¸ì„ ë¨¼ì € í•™ìŠµì‹œí‚¤ì„¸ìš”</li>
                    <li><strong>ì˜ˆì¸¡ ìˆ˜í–‰:</strong> POST /api/predictë¡œ ì§ì›ë“¤ì˜ í‡´ì‚¬ í™•ë¥ ì„ ì˜ˆì¸¡í•˜ì„¸ìš”</li>
                    <li><strong>ê²°ê³¼ ë¶„ì„:</strong> GET /api/feature_importanceë¡œ ì¤‘ìš”í•œ í”¼ì²˜ë“¤ì„ í™•ì¸í•˜ì„¸ìš”</li>
                    <li><strong>ê°œë³„ ë¶„ì„:</strong> GET /api/employee_timeline/{id}ë¡œ ê°œë³„ ì§ì›ì„ ë¶„ì„í•˜ì„¸ìš”</li>
                </ol>
            </div>
        </div>
    </body>
    </html>
    """
    
    # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    if model is not None and processor is not None:
        status_class = "ready"
        status_message = "âœ… ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë“  ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    elif processor is not None:
        status_class = "warning"
        status_message = "âš ï¸ ë°ì´í„°ëŠ” ë¡œë“œë˜ì—ˆì§€ë§Œ ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”."
    else:
        status_class = "warning"
        status_message = "âš ï¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë°ì´í„° íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
    
    return render_template_string(html_template, 
                                status_class=status_class, 
                                status_message=status_message)

@app.route('/api/status')
def get_status():
    """
    ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    """
    try:
        status = {
            'system_initialized': processor is not None,
            'model_loaded': model is not None,
            'data_available': processor is not None and processor.ts_data is not None,
            'device': str(device),
            'timestamp': datetime.now().isoformat()
        }
        
        if processor:
            status.update({
                'sequence_length': processor.sequence_length,
                'aggregation_unit': processor.aggregation_unit,
                'feature_count': len(processor.feature_columns),
                'data_shape': processor.processed_data.shape if processor.processed_data is not None else None
            })
        
        return jsonify(status)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/upload/timeseries', methods=['POST'])
def upload_timeseries_data():
    """ì‹œê³„ì—´ ë°ì´í„° CSV íŒŒì¼ ì—…ë¡œë“œ"""
    try:
        # íŒŒì¼ í™•ì¸
        if 'file' not in request.files:
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        if not file.filename.lower().endswith('.csv'):
            return jsonify({
                "success": False,
                "error": "CSV íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            }), 400
        
        # íŒŒì¼ ì €ì¥
        filename = secure_filename(file.filename)
        upload_dir = os.path.join(os.path.dirname(__file__), '..', 'uploads', 'Chronos')
        os.makedirs(upload_dir, exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•œ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = os.path.splitext(filename)[0]
        new_filename = f"{base_name}_{timestamp}.csv"
        file_path = os.path.join(upload_dir, new_filename)
        
        # ìµœì‹  íŒŒì¼ ë§í¬ë„ ìƒì„±
        latest_link = os.path.join(upload_dir, 'latest_timeseries.csv')
        file.save(file_path)
        
        # ìµœì‹  íŒŒì¼ ë§í¬ ìƒì„±
        try:
            if os.path.exists(latest_link):
                os.remove(latest_link)
            import shutil
            shutil.copy2(file_path, latest_link)
        except Exception as e:
            print(f"ìµœì‹  íŒŒì¼ ë§í¬ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # íŒŒì¼ í¬ê¸° í™•ì¸ (300MB ì œí•œ)
        file.seek(0, 2)  # íŒŒì¼ ëìœ¼ë¡œ ì´ë™
        file_size = file.tell()
        file.seek(0)  # íŒŒì¼ ì‹œì‘ìœ¼ë¡œ ë˜ëŒë¦¬ê¸°
        
        max_size = 300 * 1024 * 1024  # 300MB
        if file_size > max_size:
            return jsonify({
                "success": False,
                "error": f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ìµœëŒ€ 300MBê¹Œì§€ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤. (í˜„ì¬: {file_size / (1024*1024):.1f}MB)"
            }), 413
        
        # ë°ì´í„° ê²€ì¦
        try:
            df = pd.read_csv(file_path)
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_columns = ['employee_id', 'week']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                return jsonify({
                    "success": False,
                    "error": f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing_columns)}",
                    "required_columns": required_columns,
                    "found_columns": list(df.columns)
                }), 400
            
            # ì‹œê³„ì—´ ë°ì´í„° í˜•ì‹ í™•ì¸
            unique_employees = df['employee_id'].nunique()
            weeks_per_employee = df.groupby('employee_id')['week'].count()
            min_weeks = weeks_per_employee.min()
            max_weeks = weeks_per_employee.max()
            
            # ê¸°ì¡´ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ìƒˆ ë°ì´í„°ë¡œ ì¬ì²˜ë¦¬ í•„ìš”)
            global processor, model
            processor = None
            model = None
            
            return jsonify({
                "success": True,
                "message": "ì‹œê³„ì—´ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "file_info": {
                    "original_filename": filename,
                    "saved_filename": new_filename,
                    "upload_path": upload_dir,
                    "file_path": file_path,
                    "latest_link": latest_link,
                    "total_rows": len(df),
                    "unique_employees": unique_employees,
                    "weeks_range": f"{min_weeks}-{max_weeks}ì£¼",
                    "columns": len(df.columns),
                    "column_names": list(df.columns)
                },
                "note": "ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì‹œìŠ¤í…œì„ ì¬ì´ˆê¸°í™”í•˜ê³  ëª¨ë¸ì„ ì¬í›ˆë ¨í•´ì£¼ì„¸ìš”."
            })
            
        except Exception as e:
            return jsonify({
                "success": False,
                "error": f"ë°ì´í„° íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}"
            }), 400
            
    except Exception as e:
        return jsonify({
            "success": False,
            "error": f"íŒŒì¼ ì—…ë¡œë“œ ì˜¤ë¥˜: {str(e)}"
        }), 500

@app.route('/api/train', methods=['POST'])
def train_model():
    """
    ëª¨ë¸ í•™ìŠµ
    """
    global model
    
    try:
        if processor is None or processor.ts_data is None:
            return jsonify({'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        # ìš”ì²­ íŒŒë¼ë¯¸í„° íŒŒì‹±
        params = request.get_json() or {}
        sequence_length = params.get('sequence_length', 6)
        epochs = params.get('epochs', 50)
        batch_size = params.get('batch_size', 32)
        learning_rate = params.get('learning_rate', 0.001)
        optimize_hyperparameters_flag = params.get('optimize_hyperparameters', True)  # ê¸°ë³¸ì ìœ¼ë¡œ ìµœì í™” ì‚¬ìš©
        
        print(f"ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘ - Epochs: {epochs}, Batch Size: {batch_size}")
        if optimize_hyperparameters_flag and OPTUNA_AVAILABLE:
            print("ğŸ”§ ë² ì´ì§€ì•ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í™œì„±í™”")
        else:
            print("âš™ï¸ ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©")
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ ì—…ë°ì´íŠ¸
        processor.sequence_length = sequence_length
        
        # ê°œì„ ëœ ì‹œí€€ìŠ¤ ìƒì„±
        X, y, employee_ids = processor.create_proper_sequences()
        
        # ì§ì› ê¸°ë°˜ ë¶„í•  (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ - ì‹œê³„ì—´ íŠ¹ì„± ê³ ë ¤)
        X_train, X_test, y_train, y_test = employee_based_train_test_split(
            X, y, employee_ids, test_ratio=0.2
        )
        
        print(f"ğŸ“Š ì‹œê³„ì—´ ê²€ì¦ ë°©ì‹:")
        print(f"   - ì§ì›ë³„ ë¶„í• : ë™ì¼ ì§ì› ë°ì´í„°ê°€ train/test ë™ì‹œ í¬í•¨ ë°©ì§€")
        print(f"   - ì‹œê°„ ìˆœì„œ ìœ ì§€: ê° ì§ì›ì˜ ê³¼ê±°â†’í˜„ì¬ ì‹œí€€ìŠ¤ ë³´ì¡´")
        print(f"   - ì˜ˆì¸¡ ë°©í–¥: ê³¼ê±° 6ì£¼ ë°ì´í„°ë¡œ ë¯¸ë˜ í‡´ì‚¬ ì—¬ë¶€ ì˜ˆì¸¡")
        
        # í…ì„œ ë³€í™˜
        X_train_tensor = torch.FloatTensor(X_train)
        X_test_tensor = torch.FloatTensor(X_test)
        y_train_tensor = torch.LongTensor(y_train)
        y_test_tensor = torch.LongTensor(y_test)
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        # ëª¨ë¸ ìƒì„± (ìµœì í™” ì—¬ë¶€ì— ë”°ë¼)
        input_size = len(processor.feature_columns)
        
        if optimize_hyperparameters_flag and OPTUNA_AVAILABLE:
            # ë² ì´ì§€ì•ˆ ìµœì í™”ë¥¼ í†µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
            print("ğŸ”§ ë² ì´ì§€ì•ˆ ìµœì í™”ë¡œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ì¤‘...")
            
            def objective(trial):
                # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ
                gru_hidden = trial.suggest_categorical('gru_hidden', [16, 32, 64, 128])
                cnn_filters = trial.suggest_categorical('cnn_filters', [8, 16, 32, 64])
                dropout = trial.suggest_float('dropout', 0.1, 0.5)
                trial_lr = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)
                trial_batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])
                
                try:
                    # ëª¨ë¸ ìƒì„±
                    trial_model = create_hybrid_model(
                        input_size=input_size,
                        gru_hidden=gru_hidden,
                        cnn_filters=cnn_filters,
                        kernel_sizes=[2, 3],
                        dropout=dropout
                    )
                    trial_model.to(device)
                    
                    # ë°ì´í„° ë¡œë” (ë°°ì¹˜ í¬ê¸° ì ìš©)
                    trial_train_loader = DataLoader(train_dataset, batch_size=trial_batch_size, shuffle=True)
                    trial_test_loader = DataLoader(test_dataset, batch_size=trial_batch_size, shuffle=False)
                    
                    # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
                    trial_trainer = ChronosModelTrainer(trial_model, device)
                    trial_optimizer = optim.Adam(trial_model.parameters(), lr=trial_lr)
                    trial_criterion = nn.CrossEntropyLoss()
                    
                    # ì§§ì€ í•™ìŠµ (ìµœì í™”ìš©)
                    best_val_acc = 0
                    patience = 5
                    patience_counter = 0
                    
                    for epoch in range(min(epochs, 30)):  # ìµœëŒ€ 30 ì—í¬í¬ë¡œ ì œí•œ
                        train_loss, train_acc = trial_trainer.train_epoch(trial_train_loader, trial_optimizer, trial_criterion)
                        test_results = trial_trainer.evaluate(trial_test_loader, trial_criterion)
                        val_acc = test_results['accuracy']
                        
                        if val_acc > best_val_acc:
                            best_val_acc = val_acc
                            patience_counter = 0
                        else:
                            patience_counter += 1
                            if patience_counter >= patience:
                                break
                        
                        # Optuna pruning
                        trial.report(val_acc, epoch)
                        if trial.should_prune():
                            raise optuna.TrialPruned()
                    
                    return best_val_acc
                    
                except Exception as e:
                    print(f"Trial ì‹¤íŒ¨: {str(e)}")
                    return 0.0
            
            # Optuna Study ì‹¤í–‰
            study = optuna.create_study(
                direction='maximize',
                sampler=TPESampler(seed=42),
                pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=5)
            )
            
            n_optimization_trials = 50  # ë² ì´ì§€ì•ˆ ìµœì í™” 50íšŒë¡œ ì„¤ì •
            print(f"ğŸš€ {n_optimization_trials}íšŒ ë² ì´ì§€ì•ˆ ìµœì í™” ì‹œí–‰...")
            study.optimize(objective, n_trials=n_optimization_trials, timeout=1800)  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
            
            # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ìƒì„±
            best_params = study.best_params
            print(f"âœ… ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_params}")
            
            model = create_hybrid_model(
                input_size=input_size,
                gru_hidden=best_params['gru_hidden'],
                cnn_filters=best_params['cnn_filters'],
                kernel_sizes=[2, 3],
                dropout=best_params['dropout']
            )
            
            # ìµœì  ì„¤ì • ì ìš©
            learning_rate = best_params['learning_rate']
            batch_size = best_params['batch_size']
            
            # ë°ì´í„°ë¡œë” ì¬ìƒì„± (ìµœì  ë°°ì¹˜ í¬ê¸°ë¡œ)
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
            
        else:
            # ê¸°ì¡´ ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©
            print("âš™ï¸ ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ìƒì„±...")
            model = create_hybrid_model(
                input_size=input_size,
                gru_hidden=32,
                cnn_filters=16,
                kernel_sizes=[2, 3],
                dropout=0.2
            )
        
        model.to(device)
        
        # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        trainer = ChronosModelTrainer(model, device)
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()
        
        # í•™ìŠµ ì§„í–‰
        train_losses = []
        train_accuracies = []
        test_losses = []
        test_accuracies = []
        
        for epoch in range(epochs):
            # í•™ìŠµ
            train_loss, train_acc = trainer.train_epoch(train_loader, optimizer, criterion)
            train_losses.append(train_loss)
            train_accuracies.append(train_acc)
            
            # í‰ê°€
            test_results = trainer.evaluate(test_loader, criterion)
            test_losses.append(test_results['loss'])
            test_accuracies.append(test_results['accuracy'])
            
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs} - Train Acc: {train_acc:.4f}, Test Acc: {test_results['accuracy']:.4f}")
        
        # ìµœì¢… í‰ê°€
        final_results = trainer.evaluate(test_loader, criterion)
        
        # ëª¨ë¸ ì €ì¥
        training_info = {
            'epochs': epochs,
            'batch_size': batch_size,
            'learning_rate': learning_rate,
            'final_accuracy': final_results['accuracy'],
            'train_losses': train_losses,
            'test_losses': test_losses,
            'train_accuracies': train_accuracies,
            'test_accuracies': test_accuracies
        }
        
        save_model(model, processor, training_info)
        
        return jsonify({
            'message': 'ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'results': {
                'final_accuracy': final_results['accuracy'],
                'final_loss': final_results['loss'],
                'training_epochs': epochs,
                'data_size': len(X),
                'feature_count': input_size
            }
        })
        
    except Exception as e:
        print(f"âŒ í•™ìŠµ ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/api/optimize_hyperparameters', methods=['POST'])
def optimize_hyperparameters():
    """
    Optunaë¥¼ ì‚¬ìš©í•œ ë² ì´ì§€ì•ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
    """
    global model, processor
    
    if not OPTUNA_AVAILABLE:
        return jsonify({
            'error': 'Optunaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install optunaë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.'
        }), 400
    
    try:
        if processor is None or processor.ts_data is None:
            return jsonify({'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        # ìš”ì²­ íŒŒë¼ë¯¸í„° íŒŒì‹±
        params = request.get_json() or {}
        n_trials = params.get('n_trials', 50)  # ìµœì í™” ì‹œí–‰ íšŸìˆ˜
        timeout = params.get('timeout', 1800)  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
        
        print(f"ğŸ”§ Chronos ë² ì´ì§€ì•ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")
        print(f"   ì‹œí–‰ íšŸìˆ˜: {n_trials}")
        print(f"   íƒ€ì„ì•„ì›ƒ: {timeout}ì´ˆ")
        
        # ì‹œí€€ìŠ¤ ìƒì„± (ì‹œê°„ ìˆœì„œ ê³ ë ¤)
        X, y, employee_ids = processor.create_proper_sequences()
        
        # ì§ì› ê¸°ë°˜ ë¶„í•  (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ - ê°™ì€ ì§ì›ì˜ ë°ì´í„°ê°€ train/testì— ë™ì‹œ í¬í•¨ë˜ì§€ ì•ŠìŒ)
        X_train, X_test, y_train, y_test = employee_based_train_test_split(
            X, y, employee_ids, test_ratio=0.2
        )
        
        print(f"ğŸ“Š ì‹œê³„ì—´ ë°ì´í„° ê²€ì¦ ë°©ì‹:")
        print(f"   - ì§ì›ë³„ ë¶„í• : ê°™ì€ ì§ì› ë°ì´í„°ê°€ train/testì— ë™ì‹œ í¬í•¨ë˜ì§€ ì•ŠìŒ")
        print(f"   - ì‹œê°„ ìˆœì„œ ë³´ì¡´: ê° ì§ì›ì˜ ì‹œê³„ì—´ ìˆœì„œ ìœ ì§€")
        print(f"   - ë¯¸ë˜ ì˜ˆì¸¡ ë°©ì‹: ê³¼ê±° ì‹œí€€ìŠ¤ë¡œ ë¯¸ë˜ í‡´ì‚¬ ì—¬ë¶€ ì˜ˆì¸¡")
        
        # GPU ì„¤ì •
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        input_size = len(processor.feature_columns)
        
        # ìµœì í™” ëª©ì  í•¨ìˆ˜ ì •ì˜
        def objective(trial):
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ
            gru_hidden = trial.suggest_categorical('gru_hidden', [16, 32, 64, 128])
            cnn_filters = trial.suggest_categorical('cnn_filters', [8, 16, 32, 64])
            dropout = trial.suggest_float('dropout', 0.1, 0.5)
            learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)
            batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])
            epochs = trial.suggest_int('epochs', 20, 100)
            
            try:
                # ëª¨ë¸ ìƒì„±
                trial_model = create_hybrid_model(
                    input_size=input_size,
                    gru_hidden=gru_hidden,
                    cnn_filters=cnn_filters,
                    kernel_sizes=[2, 3],  # ê³ ì •
                    dropout=dropout
                )
                trial_model.to(device)
                
                # ë°ì´í„° ë¡œë” ìƒì„±
                train_dataset = TensorDataset(
                    torch.FloatTensor(X_train), 
                    torch.LongTensor(y_train)
                )
                test_dataset = TensorDataset(
                    torch.FloatTensor(X_test), 
                    torch.LongTensor(y_test)
                )
                
                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
                test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
                
                # íŠ¸ë ˆì´ë„ˆ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •
                trainer = ChronosModelTrainer(trial_model, device)
                optimizer = optim.Adam(trial_model.parameters(), lr=learning_rate)
                criterion = nn.CrossEntropyLoss()
                
                # ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•œ ë³€ìˆ˜
                best_val_acc = 0
                patience = 10
                patience_counter = 0
                
                # í•™ìŠµ ì§„í–‰
                for epoch in range(epochs):
                    train_loss, train_acc = trainer.train_epoch(train_loader, optimizer, criterion)
                    test_results = trainer.evaluate(test_loader, criterion)
                    val_acc = test_results['accuracy']
                    
                    # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
                    if val_acc > best_val_acc:
                        best_val_acc = val_acc
                        patience_counter = 0
                    else:
                        patience_counter += 1
                        if patience_counter >= patience:
                            break
                    
                    # Optuna pruning (ì¤‘ê°„ ê²°ê³¼ ê¸°ë°˜ ì¡°ê¸° ì¢…ë£Œ)
                    trial.report(val_acc, epoch)
                    if trial.should_prune():
                        raise optuna.TrialPruned()
                
                return best_val_acc
                
            except Exception as e:
                print(f"Trial ì‹¤íŒ¨: {str(e)}")
                return 0.0  # ì‹¤íŒ¨ ì‹œ ìµœì € ì ìˆ˜ ë°˜í™˜
        
        # Optuna Study ìƒì„± ë° ì‹¤í–‰
        study = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(seed=42),
            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)
        )
        
        print("ğŸš€ ë² ì´ì§€ì•ˆ ìµœì í™” ì‹œì‘...")
        study.optimize(objective, n_trials=n_trials, timeout=timeout)
        
        # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
        best_params = study.best_params
        print(f"âœ… ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_params}")
        
        # ìµœì  ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
        optimized_model = create_hybrid_model(
            input_size=input_size,
            gru_hidden=best_params['gru_hidden'],
            cnn_filters=best_params['cnn_filters'],
            kernel_sizes=[2, 3],
            dropout=best_params['dropout']
        )
        optimized_model.to(device)
        
        # ìµœì  ì„¤ì •ìœ¼ë¡œ ìµœì¢… í•™ìŠµ
        train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))
        test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))
        
        train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'], shuffle=False)
        
        trainer = ChronosModelTrainer(optimized_model, device)
        optimizer = optim.Adam(optimized_model.parameters(), lr=best_params['learning_rate'])
        criterion = nn.CrossEntropyLoss()
        
        # ìµœì¢… í•™ìŠµ
        final_epochs = min(best_params['epochs'], 50)  # ìµœëŒ€ 50 ì—í¬í¬ë¡œ ì œí•œ
        for epoch in range(final_epochs):
            train_loss, train_acc = trainer.train_epoch(train_loader, optimizer, criterion)
            if epoch % 10 == 0:
                test_results = trainer.evaluate(test_loader, criterion)
                print(f"Final Training Epoch {epoch+1}/{final_epochs} - Val Acc: {test_results['accuracy']:.4f}")
        
        # ìµœì¢… í‰ê°€
        final_results = trainer.evaluate(test_loader, criterion)
        
        # ìµœì í™”ëœ ëª¨ë¸ ì €ì¥
        model = optimized_model
        
        optimization_info = {
            'best_params': best_params,
            'best_score': study.best_value,
            'n_trials': len(study.trials),
            'optimization_time': datetime.now().isoformat(),
            'final_accuracy': final_results['accuracy'],
            'final_loss': final_results['loss']
        }
        
        save_model(model, processor, optimization_info)
        
        return jsonify({
            'message': 'Chronos ë² ì´ì§€ì•ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì™„ë£Œ',
            'optimization_results': {
                'best_hyperparameters': best_params,
                'best_validation_score': study.best_value,
                'final_test_accuracy': final_results['accuracy'],
                'final_test_loss': final_results['loss'],
                'total_trials': len(study.trials),
                'optimization_method': 'Optuna TPESampler',
                'trials_completed': len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]),
                'trials_pruned': len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])
            },
            'model_info': {
                'input_size': input_size,
                'feature_count': len(processor.feature_columns),
                'training_samples': len(X_train),
                'test_samples': len(X_test)
            }
        })
        
    except Exception as e:
        print(f"âŒ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/api/predict', methods=['POST'])
def predict():
    """
    ì˜ˆì¸¡ ìˆ˜í–‰
    """
    try:
        if model is None:
            return jsonify({'error': 'ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.'}), 400
        
        if processor is None or processor.processed_data is None:
            return jsonify({'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        # ìš”ì²­ íŒŒë¼ë¯¸í„° íŒŒì‹±
        params = request.get_json() or {}
        employee_ids = params.get('employee_ids', [])
        
        if not employee_ids:
            # ëª¨ë“  ì§ì› ì˜ˆì¸¡
            employee_ids = processor.processed_data['employee_id'].unique().tolist()
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        predictions = []
        
        for emp_id in employee_ids:
            emp_data = processor.processed_data[
                processor.processed_data['employee_id'] == emp_id
            ].sort_values('period')
            
            if len(emp_data) >= processor.sequence_length:
                # í”¼ì²˜ ë°ì´í„° ì¶”ì¶œ ë° ì •ê·œí™”
                feature_data = emp_data[processor.feature_columns].values
                feature_data_scaled = processor.scaler.transform(feature_data)
                
                # ì‹œí€€ìŠ¤ ìƒì„±
                sequence = feature_data_scaled[-processor.sequence_length:]
                X_tensor = torch.FloatTensor(sequence).unsqueeze(0).to(device)
                
                # ì˜ˆì¸¡ ë° í•´ì„
                model.eval()
                with torch.no_grad():
                    interpretation = model.interpret_prediction(X_tensor, processor.feature_columns)
                    
                    pred_class = np.argmax(interpretation['predictions'][0])
                    pred_prob = interpretation['probabilities'][0][1]  # í‡´ì‚¬ í™•ë¥  (Temperature Scaling ì ìš©ë¨)
                    
                    predictions.append({
                        'employee_id': int(emp_id),
                        'attrition_probability': float(pred_prob),
                        'predicted_class': int(pred_class),
                        'risk_level': 'High' if pred_prob > 0.7 else 'Medium' if pred_prob > 0.3 else 'Low',
                        'feature_importance': {
                            name: float(importance) 
                            for name, importance in zip(
                                processor.feature_columns, 
                                interpretation['feature_importance']
                            )
                        },
                        'temporal_attention': interpretation['temporal_attention'][0].tolist() if interpretation['temporal_attention'].ndim > 1 else interpretation['temporal_attention'].tolist()
                    })
        
        # ê²°ê³¼ ì •ë ¬ (í‡´ì‚¬ í™•ë¥  ë†’ì€ ìˆœ)
        predictions.sort(key=lambda x: x['attrition_probability'], reverse=True)
        
        return jsonify({
            'predictions': predictions,
            'summary': {
                'total_employees': len(predictions),
                'high_risk_count': sum(1 for p in predictions if p['risk_level'] == 'High'),
                'medium_risk_count': sum(1 for p in predictions if p['risk_level'] == 'Medium'),
                'low_risk_count': sum(1 for p in predictions if p['risk_level'] == 'Low'),
                'average_attrition_probability': np.mean([p['attrition_probability'] for p in predictions])
            }
        })
        
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/api/feature_importance')
def get_feature_importance():
    """
    Feature importance ì‹œê°í™”
    """
    try:
        if model is None:
            return jsonify({'error': 'ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        # ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ í‰ê·  feature importance ê³„ì‚°
        X, y, employee_ids = processor.create_proper_sequences()
        X_tensor = torch.FloatTensor(X).to(device)
        
        model.eval()
        feature_importances = []
        
        with torch.no_grad():
            for i in range(0, len(X_tensor), 32):  # ë°°ì¹˜ ì²˜ë¦¬
                batch = X_tensor[i:i+32]
                interpretation = model.interpret_prediction(batch, processor.feature_columns)
                feature_importances.append(interpretation['feature_importance'])
        
        # í‰ê·  ê³„ì‚°
        avg_importance = np.mean(feature_importances, axis=0)
        
        # ì‹œê°í™” ìƒì„±
        html_plot = visualizer.plot_feature_importance(
            avg_importance, 
            processor.feature_columns,
            "Average Feature Importance Across All Employees"
        )
        
        return html_plot, 200, {'Content-Type': 'text/html'}
        
    except Exception as e:
        print(f"âŒ Feature importance ì˜¤ë¥˜: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/model_analysis')
def get_model_analysis():
    """
    ëª¨ë¸ ë¶„ì„ ëŒ€ì‹œë³´ë“œ
    """
    try:
        if model is None:
            return jsonify({'error': 'ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        # ì „ì²´ ë°ì´í„° ì˜ˆì¸¡
        X, y, employee_ids = processor.create_proper_sequences()
        X_tensor = torch.FloatTensor(X).to(device)
        y_tensor = torch.LongTensor(y)
        
        model.eval()
        all_predictions = []
        all_probabilities = []
        
        with torch.no_grad():
            for i in range(0, len(X_tensor), 32):
                batch_x = X_tensor[i:i+32]
                outputs = model(batch_x)
                probabilities = torch.softmax(outputs, dim=1)
                predictions = torch.argmax(outputs, dim=1)
                
                all_predictions.extend(predictions.cpu().numpy())
                all_probabilities.extend(probabilities.cpu().numpy())
        
        all_predictions = np.array(all_predictions)
        all_probabilities = np.array(all_probabilities)
        
        # ë¶„ì„ ì‹œê°í™” ìƒì„±
        html_plot = visualizer.plot_prediction_analysis(
            all_predictions, all_probabilities, y
        )
        
        return html_plot, 200, {'Content-Type': 'text/html'}
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/employee_timeline/<int:employee_id>')
def get_employee_timeline(employee_id):
    """
    ê°œë³„ ì§ì› ì‹œê³„ì—´ ë¶„ì„
    """
    try:
        if processor is None or processor.processed_data is None:
            return jsonify({'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        # ì§ì› ë°ì´í„° ì¶”ì¶œ
        emp_data = processor.processed_data[
            processor.processed_data['employee_id'] == employee_id
        ].sort_values('period')
        
        if len(emp_data) == 0:
            return jsonify({'error': f'ì§ì› ID {employee_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 404
        
        attention_weights = None
        
        # ëª¨ë¸ì´ ìˆìœ¼ë©´ attention weights ê³„ì‚°
        if model is not None and len(emp_data) >= processor.sequence_length:
            # ì‹œê°„ë³„ ì§‘ê³„ ì²˜ë¦¬
            emp_data['year'] = emp_data[processor.date_column].dt.year
            emp_data['week'] = emp_data[processor.date_column].dt.isocalendar().week
            emp_data['time_period'] = emp_data['year'].astype(str) + '-W' + emp_data['week'].astype(str).str.zfill(2)
            
            agg_data = emp_data.groupby('time_period')[processor.feature_columns].mean().reset_index()
            agg_data = agg_data.sort_values('time_period')
            
            # ìµœê·¼ ì‹œí€€ìŠ¤ ìƒì„±
            if len(agg_data) >= processor.sequence_length:
                sequence_data = agg_data[processor.feature_columns].values[-processor.sequence_length:]
                
                # ì •ê·œí™”
                sequence_scaled = processor.scaler.transform(sequence_data.reshape(-1, len(processor.feature_columns)))
                sequence_scaled = sequence_scaled.reshape(1, processor.sequence_length, -1)
                
                X_tensor = torch.FloatTensor(sequence_scaled).to(device)
                
                model.eval()
                with torch.no_grad():
                    interpretation = model.interpret_prediction(X_tensor, processor.feature_columns)
                    attention_weights = interpretation['temporal_attention'][0] if interpretation['temporal_attention'].ndim > 1 else interpretation['temporal_attention']
        
        # ì‹œê°í™” ìƒì„±
        html_plot = visualizer.create_employee_timeline(emp_data, attention_weights)
        
        return html_plot, 200, {'Content-Type': 'text/html'}
        
    except Exception as e:
        print(f"âŒ ì§ì› íƒ€ì„ë¼ì¸ ì˜¤ë¥˜: {str(e)}")
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    print("ğŸš€ Chronos Flask ì„œë²„ ì‹œì‘ ì¤‘...")
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    if initialize_system():
        print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        print("ğŸŒ ì„œë²„ ì£¼ì†Œ: http://localhost:5003")
        print("ğŸ“‹ API ë¬¸ì„œ: http://localhost:5003")
        app.run(host='0.0.0.0', port=5003, debug=True)
    else:
        print("âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
