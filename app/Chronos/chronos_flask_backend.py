# ============================================================================
# Chronos Flask ë°±ì—”ë“œ API
# ============================================================================

from flask import Flask, request, jsonify, render_template_string
from flask_cors import CORS
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
import joblib
import os
import json
from datetime import datetime
import traceback
from typing import Dict, List, Any
import warnings
warnings.filterwarnings('ignore')

# ë¡œì»¬ ëª¨ë“ˆ import
from chronos_models import GRU_CNN_AttentionModel, ChronosModelTrainer, create_attention_model
from chronos_processor import ChronosDataProcessor, ChronosVisualizer

app = Flask(__name__)
CORS(app)

# ì „ì—­ ë³€ìˆ˜
processor = None
model = None
visualizer = None
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ë°ì´í„° ê²½ë¡œ ì„¤ì •
DATA_PATH = {
    'timeseries': 'data/IBM_HR_timeseries.csv',
    'personas': 'data/IBM_HR_personas_assigned.csv'
}

MODEL_PATH = 'app/Chronos/models'
os.makedirs(MODEL_PATH, exist_ok=True)

def initialize_system():
    """
    ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    """
    global processor, model, visualizer
    
    try:
        print("ğŸš€ Chronos ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # í”„ë¡œì„¸ì„œ ë° ì‹œê°í™” ë„êµ¬ ì´ˆê¸°í™”
        processor = ChronosDataProcessor(sequence_length=6, aggregation_unit='week')
        visualizer = ChronosVisualizer()
        
        # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        if os.path.exists(DATA_PATH['timeseries']) and os.path.exists(DATA_PATH['personas']):
            processor.load_data(DATA_PATH['timeseries'], DATA_PATH['personas'])
            processor.preprocess_data()
            print("âœ… ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì™„ë£Œ")
        else:
            print("âš ï¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµëœ ëª¨ë¸ë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹œë„
        model_file = os.path.join(MODEL_PATH, 'chronos_attention_model.pth')
        if os.path.exists(model_file):
            load_model()
            print("âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        else:
            print("â„¹ï¸ ê¸°ì¡´ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ëª¨ë¸ì„ í•™ìŠµí•´ì£¼ì„¸ìš”.")
            
        print("ğŸ‰ Chronos ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        traceback.print_exc()
        return False

def load_model():
    """
    ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ
    """
    global model
    
    try:
        model_file = os.path.join(MODEL_PATH, 'chronos_attention_model.pth')
        scaler_file = os.path.join(MODEL_PATH, 'chronos_scaler.joblib')
        
        if os.path.exists(model_file) and os.path.exists(scaler_file):
            # ëª¨ë¸ ë¡œë“œ
            checkpoint = torch.load(model_file, map_location=device)
            model = create_attention_model(
                input_size=checkpoint['input_size'],
                gru_hidden=checkpoint.get('gru_hidden', 64),
                cnn_filters=checkpoint.get('cnn_filters', 32),
                dropout=checkpoint.get('dropout', 0.3)
            )
            model.load_state_dict(checkpoint['model_state_dict'])
            model.to(device)
            model.eval()
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
            if processor:
                processor.scaler = joblib.load(scaler_file)
                processor.feature_columns = checkpoint.get('feature_columns', [])
            
            print("âœ… ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
            return True
        else:
            print("âš ï¸ ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return False

def save_model(model, processor, additional_info=None):
    """
    ëª¨ë¸ ì €ì¥
    """
    try:
        model_file = os.path.join(MODEL_PATH, 'chronos_attention_model.pth')
        scaler_file = os.path.join(MODEL_PATH, 'chronos_scaler.joblib')
        
        # ëª¨ë¸ ì €ì¥
        checkpoint = {
            'model_state_dict': model.state_dict(),
            'input_size': model.input_size,
            'gru_hidden': model.gru_hidden,
            'cnn_filters': model.cnn_filters,
            'feature_columns': processor.feature_columns,
            'sequence_length': processor.sequence_length,
            'aggregation_unit': processor.aggregation_unit,
            'timestamp': datetime.now().isoformat()
        }
        
        if additional_info:
            checkpoint.update(additional_info)
            
        torch.save(checkpoint, model_file)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        joblib.save(processor.scaler, scaler_file)
        
        print("âœ… ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ")
        return True
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return False

@app.route('/')
def home():
    """
    í™ˆí˜ì´ì§€
    """
    html_template = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Chronos - Employee Attrition Prediction</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; background-color: #f5f5f5; }
            .container { max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
            h1 { color: #2c3e50; text-align: center; }
            .api-section { margin: 20px 0; padding: 20px; background: #ecf0f1; border-radius: 5px; }
            .endpoint { margin: 10px 0; padding: 10px; background: white; border-left: 4px solid #3498db; }
            .method { font-weight: bold; color: #e74c3c; }
            .status { padding: 10px; margin: 20px 0; border-radius: 5px; }
            .status.ready { background-color: #d4edda; color: #155724; border: 1px solid #c3e6cb; }
            .status.warning { background-color: #fff3cd; color: #856404; border: 1px solid #ffeaa7; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>ğŸ•’ Chronos - Employee Attrition Prediction System</h1>
            
            <div class="status {{ status_class }}">
                <strong>ì‹œìŠ¤í…œ ìƒíƒœ:</strong> {{ status_message }}
            </div>
            
            <div class="api-section">
                <h2>ğŸ“‹ API ì—”ë“œí¬ì¸íŠ¸</h2>
                
                <div class="endpoint">
                    <span class="method">GET</span> <strong>/api/status</strong>
                    <p>ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸</p>
                </div>
                
                <div class="endpoint">
                    <span class="method">POST</span> <strong>/api/train</strong>
                    <p>ëª¨ë¸ í•™ìŠµ (JSON: {"sequence_length": 6, "epochs": 50})</p>
                </div>
                
                <div class="endpoint">
                    <span class="method">POST</span> <strong>/api/predict</strong>
                    <p>ì˜ˆì¸¡ ìˆ˜í–‰ (JSON: {"employee_ids": [1, 2, 3]})</p>
                </div>
                
                <div class="endpoint">
                    <span class="method">GET</span> <strong>/api/feature_importance</strong>
                    <p>Feature importance ì‹œê°í™”</p>
                </div>
                
                <div class="endpoint">
                    <span class="method">GET</span> <strong>/api/model_analysis</strong>
                    <p>ëª¨ë¸ ë¶„ì„ ëŒ€ì‹œë³´ë“œ</p>
                </div>
                
                <div class="endpoint">
                    <span class="method">GET</span> <strong>/api/employee_timeline/{employee_id}</strong>
                    <p>ê°œë³„ ì§ì› ì‹œê³„ì—´ ë¶„ì„</p>
                </div>
            </div>
            
            <div class="api-section">
                <h2>ğŸš€ ì‚¬ìš© ë°©ë²•</h2>
                <ol>
                    <li><strong>ëª¨ë¸ í•™ìŠµ:</strong> POST /api/trainìœ¼ë¡œ ëª¨ë¸ì„ ë¨¼ì € í•™ìŠµì‹œí‚¤ì„¸ìš”</li>
                    <li><strong>ì˜ˆì¸¡ ìˆ˜í–‰:</strong> POST /api/predictë¡œ ì§ì›ë“¤ì˜ í‡´ì‚¬ í™•ë¥ ì„ ì˜ˆì¸¡í•˜ì„¸ìš”</li>
                    <li><strong>ê²°ê³¼ ë¶„ì„:</strong> GET /api/feature_importanceë¡œ ì¤‘ìš”í•œ í”¼ì²˜ë“¤ì„ í™•ì¸í•˜ì„¸ìš”</li>
                    <li><strong>ê°œë³„ ë¶„ì„:</strong> GET /api/employee_timeline/{id}ë¡œ ê°œë³„ ì§ì›ì„ ë¶„ì„í•˜ì„¸ìš”</li>
                </ol>
            </div>
        </div>
    </body>
    </html>
    """
    
    # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    if model is not None and processor is not None:
        status_class = "ready"
        status_message = "âœ… ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë“  ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    elif processor is not None:
        status_class = "warning"
        status_message = "âš ï¸ ë°ì´í„°ëŠ” ë¡œë“œë˜ì—ˆì§€ë§Œ ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”."
    else:
        status_class = "warning"
        status_message = "âš ï¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë°ì´í„° íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
    
    return render_template_string(html_template, 
                                status_class=status_class, 
                                status_message=status_message)

@app.route('/api/status')
def get_status():
    """
    ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    """
    try:
        status = {
            'system_initialized': processor is not None,
            'model_loaded': model is not None,
            'data_available': processor is not None and processor.ts_data is not None,
            'device': str(device),
            'timestamp': datetime.now().isoformat()
        }
        
        if processor:
            status.update({
                'sequence_length': processor.sequence_length,
                'aggregation_unit': processor.aggregation_unit,
                'feature_count': len(processor.feature_columns),
                'data_shape': processor.processed_data.shape if processor.processed_data is not None else None
            })
        
        return jsonify(status)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/train', methods=['POST'])
def train_model():
    """
    ëª¨ë¸ í•™ìŠµ
    """
    global model
    
    try:
        if processor is None or processor.ts_data is None:
            return jsonify({'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        # ìš”ì²­ íŒŒë¼ë¯¸í„° íŒŒì‹±
        params = request.get_json() or {}
        sequence_length = params.get('sequence_length', 6)
        epochs = params.get('epochs', 50)
        batch_size = params.get('batch_size', 32)
        learning_rate = params.get('learning_rate', 0.001)
        
        print(f"ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘ - Epochs: {epochs}, Batch Size: {batch_size}")
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ ì—…ë°ì´íŠ¸
        processor.sequence_length = sequence_length
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y, employee_ids = processor.create_sequences()
        
        # í…ì„œ ë³€í™˜
        X_tensor = torch.FloatTensor(X)
        y_tensor = torch.LongTensor(y)
        
        # ë°ì´í„°ì…‹ ë¶„í• 
        from sklearn.model_selection import train_test_split
        X_train, X_test, y_train, y_test = train_test_split(
            X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y_tensor
        )
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        train_dataset = TensorDataset(X_train, y_train)
        test_dataset = TensorDataset(X_test, y_test)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        # ëª¨ë¸ ìƒì„±
        input_size = len(processor.feature_columns)
        model = create_attention_model(input_size=input_size)
        model.to(device)
        
        # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        trainer = ChronosModelTrainer(model, device)
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()
        
        # í•™ìŠµ ì§„í–‰
        train_losses = []
        train_accuracies = []
        test_losses = []
        test_accuracies = []
        
        for epoch in range(epochs):
            # í•™ìŠµ
            train_loss, train_acc = trainer.train_epoch(train_loader, optimizer, criterion)
            train_losses.append(train_loss)
            train_accuracies.append(train_acc)
            
            # í‰ê°€
            test_results = trainer.evaluate(test_loader, criterion)
            test_losses.append(test_results['loss'])
            test_accuracies.append(test_results['accuracy'])
            
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs} - Train Acc: {train_acc:.4f}, Test Acc: {test_results['accuracy']:.4f}")
        
        # ìµœì¢… í‰ê°€
        final_results = trainer.evaluate(test_loader, criterion)
        
        # ëª¨ë¸ ì €ì¥
        training_info = {
            'epochs': epochs,
            'batch_size': batch_size,
            'learning_rate': learning_rate,
            'final_accuracy': final_results['accuracy'],
            'train_losses': train_losses,
            'test_losses': test_losses,
            'train_accuracies': train_accuracies,
            'test_accuracies': test_accuracies
        }
        
        save_model(model, processor, training_info)
        
        return jsonify({
            'message': 'ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'results': {
                'final_accuracy': final_results['accuracy'],
                'final_loss': final_results['loss'],
                'training_epochs': epochs,
                'data_size': len(X),
                'feature_count': input_size
            }
        })
        
    except Exception as e:
        print(f"âŒ í•™ìŠµ ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/api/predict', methods=['POST'])
def predict():
    """
    ì˜ˆì¸¡ ìˆ˜í–‰
    """
    try:
        if model is None:
            return jsonify({'error': 'ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.'}), 400
        
        if processor is None or processor.processed_data is None:
            return jsonify({'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        # ìš”ì²­ íŒŒë¼ë¯¸í„° íŒŒì‹±
        params = request.get_json() or {}
        employee_ids = params.get('employee_ids', [])
        
        if not employee_ids:
            # ëª¨ë“  ì§ì› ì˜ˆì¸¡
            employee_ids = processor.processed_data['employee_id'].unique().tolist()
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        predictions = []
        
        for emp_id in employee_ids:
            emp_data = processor.processed_data[
                processor.processed_data['employee_id'] == emp_id
            ].sort_values('period')
            
            if len(emp_data) >= processor.sequence_length:
                # í”¼ì²˜ ë°ì´í„° ì¶”ì¶œ ë° ì •ê·œí™”
                feature_data = emp_data[processor.feature_columns].values
                feature_data_scaled = processor.scaler.transform(feature_data)
                
                # ì‹œí€€ìŠ¤ ìƒì„±
                sequence = feature_data_scaled[-processor.sequence_length:]
                X_tensor = torch.FloatTensor(sequence).unsqueeze(0).to(device)
                
                # ì˜ˆì¸¡ ë° í•´ì„
                model.eval()
                with torch.no_grad():
                    interpretation = model.interpret_prediction(X_tensor, processor.feature_columns)
                    
                    pred_class = np.argmax(interpretation['predictions'][0])
                    pred_prob = interpretation['probabilities'][0][1]  # í‡´ì‚¬ í™•ë¥ 
                    
                    predictions.append({
                        'employee_id': int(emp_id),
                        'attrition_probability': float(pred_prob),
                        'predicted_class': int(pred_class),
                        'risk_level': 'High' if pred_prob > 0.7 else 'Medium' if pred_prob > 0.3 else 'Low',
                        'feature_importance': {
                            name: float(importance) 
                            for name, importance in zip(
                                processor.feature_columns, 
                                interpretation['feature_importance']
                            )
                        },
                        'temporal_attention': interpretation['temporal_attention'][0].tolist() if interpretation['temporal_attention'].ndim > 1 else interpretation['temporal_attention'].tolist()
                    })
        
        # ê²°ê³¼ ì •ë ¬ (í‡´ì‚¬ í™•ë¥  ë†’ì€ ìˆœ)
        predictions.sort(key=lambda x: x['attrition_probability'], reverse=True)
        
        return jsonify({
            'predictions': predictions,
            'summary': {
                'total_employees': len(predictions),
                'high_risk_count': sum(1 for p in predictions if p['risk_level'] == 'High'),
                'medium_risk_count': sum(1 for p in predictions if p['risk_level'] == 'Medium'),
                'low_risk_count': sum(1 for p in predictions if p['risk_level'] == 'Low'),
                'average_attrition_probability': np.mean([p['attrition_probability'] for p in predictions])
            }
        })
        
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/api/feature_importance')
def get_feature_importance():
    """
    Feature importance ì‹œê°í™”
    """
    try:
        if model is None:
            return jsonify({'error': 'ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        # ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ í‰ê·  feature importance ê³„ì‚°
        X, y, employee_ids = processor.create_sequences()
        X_tensor = torch.FloatTensor(X).to(device)
        
        model.eval()
        feature_importances = []
        
        with torch.no_grad():
            for i in range(0, len(X_tensor), 32):  # ë°°ì¹˜ ì²˜ë¦¬
                batch = X_tensor[i:i+32]
                interpretation = model.interpret_prediction(batch, processor.feature_columns)
                feature_importances.append(interpretation['feature_importance'])
        
        # í‰ê·  ê³„ì‚°
        avg_importance = np.mean(feature_importances, axis=0)
        
        # ì‹œê°í™” ìƒì„±
        html_plot = visualizer.plot_feature_importance(
            avg_importance, 
            processor.feature_columns,
            "Average Feature Importance Across All Employees"
        )
        
        return html_plot, 200, {'Content-Type': 'text/html'}
        
    except Exception as e:
        print(f"âŒ Feature importance ì˜¤ë¥˜: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/model_analysis')
def get_model_analysis():
    """
    ëª¨ë¸ ë¶„ì„ ëŒ€ì‹œë³´ë“œ
    """
    try:
        if model is None:
            return jsonify({'error': 'ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        # ì „ì²´ ë°ì´í„° ì˜ˆì¸¡
        X, y, employee_ids = processor.create_sequences()
        X_tensor = torch.FloatTensor(X).to(device)
        y_tensor = torch.LongTensor(y)
        
        model.eval()
        all_predictions = []
        all_probabilities = []
        
        with torch.no_grad():
            for i in range(0, len(X_tensor), 32):
                batch_x = X_tensor[i:i+32]
                outputs = model(batch_x)
                probabilities = torch.softmax(outputs, dim=1)
                predictions = torch.argmax(outputs, dim=1)
                
                all_predictions.extend(predictions.cpu().numpy())
                all_probabilities.extend(probabilities.cpu().numpy())
        
        all_predictions = np.array(all_predictions)
        all_probabilities = np.array(all_probabilities)
        
        # ë¶„ì„ ì‹œê°í™” ìƒì„±
        html_plot = visualizer.plot_prediction_analysis(
            all_predictions, all_probabilities, y
        )
        
        return html_plot, 200, {'Content-Type': 'text/html'}
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/employee_timeline/<int:employee_id>')
def get_employee_timeline(employee_id):
    """
    ê°œë³„ ì§ì› ì‹œê³„ì—´ ë¶„ì„
    """
    try:
        if processor is None or processor.processed_data is None:
            return jsonify({'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        # ì§ì› ë°ì´í„° ì¶”ì¶œ
        emp_data = processor.processed_data[
            processor.processed_data['employee_id'] == employee_id
        ].sort_values('period')
        
        if len(emp_data) == 0:
            return jsonify({'error': f'ì§ì› ID {employee_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 404
        
        attention_weights = None
        
        # ëª¨ë¸ì´ ìˆìœ¼ë©´ attention weights ê³„ì‚°
        if model is not None and len(emp_data) >= processor.sequence_length:
            feature_data = emp_data[processor.feature_columns].values
            feature_data_scaled = processor.scaler.transform(feature_data)
            sequence = feature_data_scaled[-processor.sequence_length:]
            X_tensor = torch.FloatTensor(sequence).unsqueeze(0).to(device)
            
            model.eval()
            with torch.no_grad():
                interpretation = model.interpret_prediction(X_tensor, processor.feature_columns)
                attention_weights = interpretation['temporal_attention'][0] if interpretation['temporal_attention'].ndim > 1 else interpretation['temporal_attention']
        
        # ì‹œê°í™” ìƒì„±
        html_plot = visualizer.create_employee_timeline(emp_data, attention_weights)
        
        return html_plot, 200, {'Content-Type': 'text/html'}
        
    except Exception as e:
        print(f"âŒ ì§ì› íƒ€ì„ë¼ì¸ ì˜¤ë¥˜: {str(e)}")
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    print("ğŸš€ Chronos Flask ì„œë²„ ì‹œì‘ ì¤‘...")
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    if initialize_system():
        print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        print("ğŸŒ ì„œë²„ ì£¼ì†Œ: http://localhost:5002")
        print("ğŸ“‹ API ë¬¸ì„œ: http://localhost:5002")
        app.run(host='0.0.0.0', port=5002, debug=True)
    else:
        print("âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
