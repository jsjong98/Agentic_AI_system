# ============================================================================
# ê°œì„ ëœ Chronos ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ (Chronos_analysis_fixed.pyì—ì„œ ì¶”ì¶œ)
# ============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import torch
from torch.utils.data import Dataset, DataLoader, TensorDataset
# sklearn import with fallback
try:
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
    SKLEARN_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  sklearn import ì‹¤íŒ¨: {e}")
    print("   ì¼ë¶€ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
    SKLEARN_AVAILABLE = False
    
    # ê¸°ë³¸ ëŒ€ì²´ í´ë˜ìŠ¤ë“¤
    class StandardScaler:
        def fit(self, X): return self
        def transform(self, X): return X
        def fit_transform(self, X): return X
    
    def train_test_split(*arrays, **kwargs):
        # ê°„ë‹¨í•œ ë¶„í•  êµ¬í˜„
        test_size = kwargs.get('test_size', 0.2)
        split_idx = int(len(arrays[0]) * (1 - test_size))
        result = []
        for arr in arrays:
            result.extend([arr[:split_idx], arr[split_idx:]])
        return result
    
    def classification_report(*args, **kwargs):
        return "sklearn not available"
    
    def confusion_matrix(*args, **kwargs):
        return [[0, 0], [0, 0]]
    
    def roc_auc_score(*args, **kwargs):
        return 0.5
    
    def roc_curve(*args, **kwargs):
        return [0, 1], [0, 1], [0.5]
import joblib
import io
import base64
import os
from typing import Dict, List, Tuple, Optional
from datetime import datetime, timedelta
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

class ProperTimeSeriesProcessor:
    def __init__(self, sequence_length=6, prediction_horizon=4, aggregation_unit='week'):
        """
        ì˜¬ë°”ë¥¸ ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ìœ„í•œ í”„ë¡œì„¸ì„œ
        
        Args:
            sequence_length (int): ì˜ˆì¸¡ì— ì‚¬ìš©í•  ê³¼ê±° ì‹œí€€ìŠ¤ ê¸¸ì´
            prediction_horizon (int): ì˜ˆì¸¡ ì‹œì  (Nì£¼ í›„ í‡´ì‚¬ ì—¬ë¶€ ì˜ˆì¸¡)
            aggregation_unit (str): ì§‘ê³„ ë‹¨ìœ„ ('day', 'week', 'month')
        """
        self.sequence_length = sequence_length
        self.prediction_horizon = prediction_horizon
        self.aggregation_unit = aggregation_unit
        self.scaler = StandardScaler()
        self.excluded_ratio_columns = [
            'work_focused_ratio', 'meeting_collaboration_ratio', 
            'social_dining_ratio', 'break_relaxation_ratio', 'shared_work_ratio'
        ]
        self.feature_columns = []
        
        print(f"ğŸ”§ ì˜¬ë°”ë¥¸ ì‹œê³„ì—´ ì„¤ì •:")
        print(f"   ì‹œí€€ìŠ¤ ê¸¸ì´: {sequence_length}{aggregation_unit[0]}")
        print(f"   ì˜ˆì¸¡ ì‹œì : {prediction_horizon}{aggregation_unit[0]} í›„")
        print(f"   ì§‘ê³„ ë‹¨ìœ„: {aggregation_unit}")
        
    def load_data(self, timeseries_path, personas_path=None):
        """ë°ì´í„° ë¡œë”©"""
        print("=" * 50)
        print("ë°ì´í„° ë¡œë”© ì¤‘...")
        print("=" * 50)
        
        # ì‹œê³„ì—´ ë°ì´í„° ë¡œë“œ
        self.ts_data = pd.read_csv(timeseries_path)
        print(f"âœ… ì‹œê³„ì—´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {self.ts_data.shape}")
        
        # ì§ì› ì†ì„± ë°ì´í„° ë¡œë“œ
        if personas_path and os.path.exists(personas_path):
            self.personas_data = pd.read_csv(personas_path)
            print(f"âœ… ì§ì› ì†ì„± ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {self.personas_data.shape}")
        else:
            # ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ì§ì› ì •ë³´ ì¶”ì¶œ
            print("ğŸ“‹ ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ì§ì› ì •ë³´ ì¶”ì¶œ ì¤‘...")
            unique_employees = self.ts_data[['employee_id']].drop_duplicates()
            
            # attrition_statusê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
            if 'attrition_status' in self.ts_data.columns:
                # ê° ì§ì›ì˜ attrition_status ì¶”ì¶œ (ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©)
                attrition_info = self.ts_data.groupby('employee_id')['attrition_status'].first().reset_index()
                self.personas_data = pd.merge(unique_employees, attrition_info, on='employee_id')
                # ì»¬ëŸ¼ëª…ì„ í‘œì¤€í™”
                self.personas_data = self.personas_data.rename(columns={'attrition_status': 'Attrition'})
            else:
                # attrition ì •ë³´ê°€ ì—†ìœ¼ë©´ ëª¨ë‘ 0ìœ¼ë¡œ ì„¤ì •
                self.personas_data = unique_employees.copy()
                self.personas_data['Attrition'] = 0
                
            print(f"âœ… ì§ì› ì •ë³´ ì¶”ì¶œ ì™„ë£Œ: {self.personas_data.shape}")
        
        return self.ts_data.head(), self.personas_data.head()

    def detect_columns(self):
        """ì»¬ëŸ¼ ìë™ ê°ì§€ ë° ë§¤ì¹­"""
        print("=" * 50)
        print("ì»¬ëŸ¼ ìë™ ê°ì§€ ì¤‘...")
        print("=" * 50)
        
        # ë‚ ì§œ ì»¬ëŸ¼ ìë™ ê°ì§€
        date_columns = [col for col in self.ts_data.columns 
                       if any(keyword in col.lower() for keyword in ['date', 'time', 'timestamp'])]
        
        if date_columns:
            self.date_column = date_columns[0]
            print(f"ğŸ—“ï¸  ê°ì§€ëœ ë‚ ì§œ ì»¬ëŸ¼: {self.date_column}")
        else:
            raise ValueError("âŒ ë‚ ì§œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        # ì§ì› ID ì»¬ëŸ¼ ìë™ ê°ì§€
        employee_id_candidates = [col for col in self.ts_data.columns 
                                 if any(keyword in col.lower() for keyword in ['employee', 'id', 'number'])]
        
        if employee_id_candidates:
            self.employee_id_col = employee_id_candidates[0]
            print(f"ğŸ‘¤ ê°ì§€ëœ ì‹œê³„ì—´ ì§ì› ID ì»¬ëŸ¼: {self.employee_id_col}")
        else:
            raise ValueError("âŒ ì§ì› ID ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        # ì§ì› ì†ì„± ë°ì´í„°ì—ì„œ ë§¤ì¹­ë˜ëŠ” ID ì»¬ëŸ¼ ì°¾ê¸°
        personas_id_candidates = [col for col in self.personas_data.columns 
                                 if any(keyword in col.lower() for keyword in ['employee', 'id', 'number'])]
        
        max_overlap = 0
        best_match_col = None
        
        for col in personas_id_candidates:
            try:
                overlap = len(set(self.ts_data[self.employee_id_col]).intersection(
                             set(self.personas_data[col])))
                if overlap > max_overlap:
                    max_overlap = overlap
                    best_match_col = col
            except:
                continue
                
        if best_match_col:
            self.personas_id_col = best_match_col
            print(f"âœ… ìµœì  ë§¤ì¹­ ì»¬ëŸ¼: {self.personas_id_col} ({max_overlap}ëª… ê²¹ì¹¨)")
        else:
            raise ValueError("âŒ ì§ì› ì†ì„± ë°ì´í„°ì—ì„œ ë§¤ì¹­ë˜ëŠ” ID ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # Attrition ì»¬ëŸ¼ ì°¾ê¸°
        attrition_cols = [col for col in self.personas_data.columns 
                         if 'attrition' in col.lower()]
        
        if attrition_cols:
            self.attrition_col = attrition_cols[0]
            print(f"ğŸ¯ ê°ì§€ëœ Attrition ì»¬ëŸ¼: {self.attrition_col}")
        else:
            raise ValueError("âŒ Attrition ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    def preprocess_data(self):
        """ë°ì´í„° ì „ì²˜ë¦¬ ë° ì‹œê°„ ë²”ìœ„ ë¶„ì„"""
        print("=" * 50)
        print("ì˜¬ë°”ë¥¸ ì‹œê³„ì—´ ì „ì²˜ë¦¬ ì¤‘...")
        print("=" * 50)
        
        # ë‚ ì§œ ë³€í™˜
        self.ts_data[self.date_column] = pd.to_datetime(self.ts_data[self.date_column])
        
        # 2023-2024ë…„ ë°ì´í„°ë§Œ í•„í„°ë§
        start_date = datetime(2023, 1, 1)
        end_date = datetime(2024, 12, 31)
        
        original_shape = self.ts_data.shape
        self.ts_data = self.ts_data[
            (self.ts_data[self.date_column] >= start_date) & 
            (self.ts_data[self.date_column] <= end_date)
        ].copy()
        
        print(f"ğŸ“… 2023-2024ë…„ í•„í„°ë§: {original_shape} â†’ {self.ts_data.shape}")
        
        # Attrition ë¼ë²¨ì„ 0/1ë¡œ ë³€í™˜
        def convert_attrition(value):
            if pd.isna(value):
                return 0
            value_str = str(value).lower().strip()
            if value_str in ['yes', 'y', 'true', '1', '1.0']:
                return 1
            else:
                return 0
                
        self.personas_data['attrition_binary'] = self.personas_data[self.attrition_col].apply(convert_attrition)
        
        attrition_dist = self.personas_data['attrition_binary'].value_counts()
        print(f"\nğŸ¯ Attrition ë¶„í¬:")
        print(f"   ì¬ì§(0): {attrition_dist.get(0, 0)}ëª…")
        print(f"   í‡´ì‚¬(1): {attrition_dist.get(1, 0)}ëª…")
        print(f"   í‡´ì‚¬ìœ¨: {attrition_dist.get(1, 0) / len(self.personas_data) * 100:.1f}%")

    def identify_features(self):
        """í”¼ì²˜ ì‹ë³„"""
        print("=" * 50)
        print("í”¼ì²˜ ì‹ë³„ ì¤‘...")
        print("=" * 50)
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì‹ë³„
        numeric_columns = self.ts_data.select_dtypes(include=[np.number]).columns.tolist()
        
        # ì œì™¸í•  ì»¬ëŸ¼ë“¤ ì œê±°
        exclude_columns = [self.employee_id_col] + self.excluded_ratio_columns
        exclude_columns = [col for col in exclude_columns if col in numeric_columns]
        
        self.feature_columns = [col for col in numeric_columns if col not in exclude_columns]
        
        print(f"ğŸ” ì „ì²´ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: {len(numeric_columns)}ê°œ")
        print(f"âŒ ì œì™¸ëœ ì»¬ëŸ¼: {exclude_columns}")
        print(f"âœ… ì‚¬ìš©í•  í”¼ì²˜: {len(self.feature_columns)}ê°œ")

    def create_proper_sequences(self):
        """ì˜¬ë°”ë¥¸ ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„± - ì‹œê°„ ìˆœì„œ ê³ ë ¤"""
        print("=" * 50)
        print("ì˜¬ë°”ë¥¸ ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
        print("=" * 50)
        
        # ì‹œê°„ë³„ ì§‘ê³„
        if self.aggregation_unit == 'week':
            self.ts_data['year'] = self.ts_data[self.date_column].dt.year
            self.ts_data['week'] = self.ts_data[self.date_column].dt.isocalendar().week
            self.ts_data['time_period'] = self.ts_data['year'].astype(str) + '-W' + self.ts_data['week'].astype(str).str.zfill(2)
        
        # ì‹œê°„ë³„ ì§‘ê³„
        agg_data = self.ts_data.groupby([self.employee_id_col, 'time_period'])[self.feature_columns].mean().reset_index()
        
        # ì§ì› ì†ì„± ë°ì´í„°ì™€ ë³‘í•©
        merged_data = pd.merge(
            agg_data,
            self.personas_data[[self.personas_id_col, 'attrition_binary']],
            left_on=self.employee_id_col,
            right_on=self.personas_id_col,
            how='inner'
        )
        
        # Flask ë°±ì—”ë“œ í˜¸í™˜ì„±ì„ ìœ„í•œ ì»¬ëŸ¼ëª… í‘œì¤€í™”
        merged_data = merged_data.rename(columns={
            self.employee_id_col: 'employee_id',
            'time_period': 'period'
        })
        
        # ì‹œê°„ ìˆœì„œ ì •ë ¬
        merged_data = merged_data.sort_values(['employee_id', 'period'])
        
        sequences = []
        labels = []
        employee_ids = []
        time_points = []
        
        print("ğŸ”„ ì§ì›ë³„ ì˜¬ë°”ë¥¸ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
        
        for employee_id in tqdm(merged_data['employee_id'].unique(), desc="ì§ì›ë³„ ì²˜ë¦¬"):
            employee_data = merged_data[
                merged_data['employee_id'] == employee_id
            ].sort_values('period').reset_index(drop=True)
            
            attrition_label = employee_data['attrition_binary'].iloc[0]
            
            # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
            min_required_length = self.sequence_length + self.prediction_horizon
            if len(employee_data) >= min_required_length:
                
                # ì „ì²´ ì‹œê³„ì—´ ë°ì´í„° ì‚¬ìš© (ìƒ˜í”Œë§ ì œê±°)
                if len(employee_data) >= self.sequence_length:
                    # ìµœì‹  ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œí€€ìŠ¤ ìƒì„± (ìƒ˜í”Œë§ ëŒ€ì‹  ìµœê·¼ ë°ì´í„° ì‚¬ìš©)
                    sequence_data = employee_data[self.feature_columns].values[-self.sequence_length:]
                    
                    # ê° ì§ì›ë‹¹ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë§Œ ìƒì„±
                    sequences.append(sequence_data)
                    labels.append(attrition_label)
                    employee_ids.append(employee_id)
                    time_points.append(employee_data.iloc[0]['period'])  # ì‹œì‘ ì‹œì 
        
        self.X = np.array(sequences, dtype=np.float32)
        self.y = np.array(labels, dtype=np.int64)
        self.employee_ids_seq = np.array(employee_ids)
        self.time_points = np.array(time_points)
        
        # Scalerë¥¼ í”¼ì²˜ ë°ì´í„°ì— fit
        if len(sequences) > 0:
            # ëª¨ë“  ì‹œí€€ìŠ¤ì˜ í”¼ì²˜ ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì³ì„œ scaler fit
            all_features = np.concatenate([seq for seq in sequences], axis=0)
            self.scaler.fit(all_features)
            print(f"âœ… StandardScaler fit ì™„ë£Œ: {all_features.shape}")
        
        # Flask ë°±ì—”ë“œ í˜¸í™˜ì„±ì„ ìœ„í•œ processed_data ì†ì„± ì¶”ê°€
        self.processed_data = merged_data
        
        print(f"âœ… ì˜¬ë°”ë¥¸ ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ:")
        print(f"   ì´ ì‹œí€€ìŠ¤: {len(self.X)}ê°œ")
        print(f"   ì‹œí€€ìŠ¤ í˜•íƒœ: {self.X.shape}")
        print(f"   í‡´ì‚¬ ë¼ë²¨ ë¹„ìœ¨: {np.mean(self.y) * 100:.1f}%")
        
        return self.X, self.y, self.employee_ids_seq

def employee_based_train_test_split(X, y, employee_ids, test_ratio=0.2):
    """ì§ì› ê¸°ë°˜ train/test ë¶„í•  (ì‹œê°„ ìˆœì„œ ê³ ë ¤í•˜ë©´ì„œ í´ë˜ìŠ¤ ê· í˜• ìœ ì§€)"""
    
    # ì§ì›ë³„ ë¼ë²¨ í™•ì¸
    unique_employees = np.unique(employee_ids)
    employee_labels = []
    
    for emp_id in unique_employees:
        emp_sequences = y[employee_ids == emp_id]
        # í•´ë‹¹ ì§ì›ì˜ ì‹œí€€ìŠ¤ ì¤‘ í•˜ë‚˜ë¼ë„ positiveë©´ í‡´ì‚¬ ì§ì›ìœ¼ë¡œ ë¶„ë¥˜
        emp_label = 1 if np.any(emp_sequences == 1) else 0
        employee_labels.append(emp_label)
    
    employee_labels = np.array(employee_labels)
    
    print(f"ğŸ“Š ì§ì›ë³„ ë¼ë²¨ ë¶„í¬:")
    print(f"   ì´ ì§ì› ìˆ˜: {len(unique_employees)}ëª…")
    print(f"   í‡´ì‚¬ ì§ì›: {np.sum(employee_labels)}ëª…")
    print(f"   ì¬ì§ ì§ì›: {np.sum(employee_labels == 0)}ëª…")
    
    # ì§ì› ë ˆë²¨ì—ì„œ stratified split
    try:
        train_employees, test_employees = train_test_split(
            unique_employees, test_size=test_ratio, random_state=42, 
            stratify=employee_labels
        )
    except ValueError:
        # stratifyê°€ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ì¼ë°˜ ë¶„í• 
        train_employees, test_employees = train_test_split(
            unique_employees, test_size=test_ratio, random_state=42
        )
    
    # ì§ì› ê¸°ë°˜ìœ¼ë¡œ ì‹œí€€ìŠ¤ ë¶„í• 
    train_mask = np.isin(employee_ids, train_employees)
    test_mask = np.isin(employee_ids, test_employees)
    
    X_train = X[train_mask]
    X_test = X[test_mask]
    y_train = y[train_mask]
    y_test = y[test_mask]
    
    print(f"ğŸ‘¥ ì§ì› ê¸°ë°˜ ë¶„í•  ì™„ë£Œ:")
    print(f"   í›ˆë ¨ ì§ì›: {len(train_employees)}ëª…")
    print(f"   í…ŒìŠ¤íŠ¸ ì§ì›: {len(test_employees)}ëª…")
    print(f"   í›ˆë ¨ ì‹œí€€ìŠ¤: {len(X_train)}ê°œ (í‡´ì‚¬ìœ¨: {np.mean(y_train)*100:.1f}%)")
    print(f"   í…ŒìŠ¤íŠ¸ ì‹œí€€ìŠ¤: {len(X_test)}ê°œ (í‡´ì‚¬ìœ¨: {np.mean(y_test)*100:.1f}%)")
    
    return X_train, X_test, y_train, y_test

# ì‹œê°í™” í´ë˜ìŠ¤ëŠ” ê¸°ì¡´ chronos_processor.pyì—ì„œ ê°€ì ¸ì˜´
class ChronosVisualizer:
    """
    Chronos ì‹œê°í™” í´ë˜ìŠ¤
    """
    def __init__(self):
        plt.style.use('default')
        sns.set_palette("husl")
    
    def plot_feature_importance(self, feature_importance: np.ndarray, feature_names: List[str], 
                              title: str = "Feature Importance") -> str:
        """
        Feature importance ì‹œê°í™”
        """
        # Plotlyë¥¼ ì‚¬ìš©í•œ ì¸í„°ë™í‹°ë¸Œ ì°¨íŠ¸
        fig = go.Figure(data=[
            go.Bar(
                x=feature_importance,
                y=feature_names,
                orientation='h',
                marker=dict(
                    color=feature_importance,
                    colorscale='Viridis',
                    showscale=True
                )
            )
        ])
        
        fig.update_layout(
            title=title,
            xaxis_title="Importance Score",
            yaxis_title="Features",
            height=max(400, len(feature_names) * 25),
            template="plotly_white"
        )
        
        return fig.to_html(include_plotlyjs='cdn')
    
    def plot_temporal_attention(self, attention_weights: np.ndarray, 
                              sequence_length: int, title: str = "Temporal Attention") -> str:
        """
        ì‹œê³„ì—´ attention ì‹œê°í™”
        """
        # í‰ê·  attention weights ê³„ì‚°
        avg_attention = np.mean(attention_weights, axis=0) if attention_weights.ndim > 1 else attention_weights
        
        time_steps = [f"Week -{sequence_length-i}" for i in range(sequence_length)]
        
        fig = go.Figure(data=[
            go.Scatter(
                x=time_steps,
                y=avg_attention,
                mode='lines+markers',
                line=dict(width=3, color='#1f77b4'),
                marker=dict(size=8, color='#ff7f0e')
            )
        ])
        
        fig.update_layout(
            title=title,
            xaxis_title="Time Steps",
            yaxis_title="Attention Weight",
            template="plotly_white",
            height=400
        )
        
        return fig.to_html(include_plotlyjs='cdn')
    
    def plot_prediction_analysis(self, predictions: np.ndarray, probabilities: np.ndarray, 
                               labels: np.ndarray) -> str:
        """
        ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„ ì‹œê°í™”
        """
        # ì„œë¸Œí”Œë¡¯ ìƒì„±
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('Confusion Matrix', 'ROC Curve', 
                          'Prediction Distribution', 'Probability Distribution'),
            specs=[[{"type": "heatmap"}, {"type": "scatter"}],
                   [{"type": "histogram"}, {"type": "histogram"}]]
        )
        
        # 1. Confusion Matrix
        cm = confusion_matrix(labels, predictions)
        fig.add_trace(
            go.Heatmap(z=cm, colorscale='Blues', showscale=False),
            row=1, col=1
        )
        
        # 2. ROC Curve
        if len(np.unique(labels)) > 1:
            fpr, tpr, _ = roc_curve(labels, probabilities[:, 1])
            auc_score = roc_auc_score(labels, probabilities[:, 1])
            
            fig.add_trace(
                go.Scatter(x=fpr, y=tpr, mode='lines', 
                          name=f'ROC (AUC = {auc_score:.3f})'),
                row=1, col=2
            )
            fig.add_trace(
                go.Scatter(x=[0, 1], y=[0, 1], mode='lines', 
                          line=dict(dash='dash'), name='Random'),
                row=1, col=2
            )
        
        # 3. Prediction Distribution
        fig.add_trace(
            go.Histogram(x=predictions, name='Predictions', nbinsx=10),
            row=2, col=1
        )
        
        # 4. Probability Distribution
        fig.add_trace(
            go.Histogram(x=probabilities[:, 1], name='Attrition Probability', nbinsx=20),
            row=2, col=2
        )
        
        fig.update_layout(
            title="Prediction Analysis Dashboard",
            height=800,
            template="plotly_white",
            showlegend=False
        )
        
        return fig.to_html(include_plotlyjs='cdn')
    
    def create_employee_timeline(self, employee_data: pd.DataFrame, 
                               attention_weights: np.ndarray = None) -> str:
        """
        ê°œë³„ ì§ì›ì˜ ì‹œê³„ì—´ ë°ì´í„° ì‹œê°í™”
        """
        fig = make_subplots(
            rows=2, cols=1,
            subplot_titles=('Feature Timeline', 'Attention Weights'),
            vertical_spacing=0.1
        )
        
        # Feature timeline
        for i, col in enumerate(employee_data.columns[2:]):  # Skip employee_id and period
            fig.add_trace(
                go.Scatter(
                    x=employee_data['period'],
                    y=employee_data[col],
                    mode='lines+markers',
                    name=col,
                    line=dict(width=2)
                ),
                row=1, col=1
            )
        
        # Attention weights
        if attention_weights is not None:
            fig.add_trace(
                go.Bar(
                    x=employee_data['period'][-len(attention_weights):],
                    y=attention_weights,
                    name='Attention',
                    marker_color='red',
                    opacity=0.7
                ),
                row=2, col=1
            )
        
        fig.update_layout(
            title="Employee Timeline Analysis",
            height=600,
            template="plotly_white"
        )
        
        return fig.to_html(include_plotlyjs='cdn')
    
    def calculate_attention_importance_per_employee(self, model, X_test, y_test, employee_ids, feature_names, device):
        """ì§ì›ë³„ Attention ê¸°ë°˜ Feature Importance ê³„ì‚°"""
        print("ğŸ” ì§ì›ë³„ Attention Feature Importance ê³„ì‚° ì¤‘...")
        
        employee_attention_results = {}
        
        # PyTorch í…ì„œë¡œ ë³€í™˜
        if isinstance(X_test, np.ndarray):
            X_tensor = torch.FloatTensor(X_test).to(device)
        else:
            X_tensor = X_test.to(device)
        
        model.eval()
        with torch.no_grad():
            # ì „ì²´ ë°°ì¹˜ì— ëŒ€í•œ attention weights ì¶”ì¶œ
            attention_weights = model.get_attention_weights(X_tensor)  # (batch_size, sequence_length)
            attention_weights_np = attention_weights.cpu().numpy()
        
        # ì§ì›ë³„ë¡œ ê²°ê³¼ ì •ë¦¬
        unique_employees = np.unique(employee_ids)
        
        for emp_id in unique_employees:
            emp_mask = employee_ids == emp_id
            emp_attention = attention_weights_np[emp_mask]
            
            if len(emp_attention) > 0:
                # í•´ë‹¹ ì§ì›ì˜ í‰ê·  attention
                mean_attention = np.mean(emp_attention, axis=0)  # (sequence_length,)
                
                # ê° í”¼ì²˜ì— ëŒ€í•œ attention ì¤‘ìš”ë„ (ëª¨ë“  ì‹œê°„ ë‹¨ê³„ì—ì„œ ë™ì¼í•˜ê²Œ ì ìš©)
                feature_attention_importance = np.tile(mean_attention, (len(feature_names), 1)).T
                overall_attention_importance = np.mean(feature_attention_importance, axis=0)
                
                employee_attention_results[emp_id] = {
                    'attention_weights': mean_attention,
                    'feature_importance': overall_attention_importance,
                    'attrition_label': y_test[emp_mask][0] if len(y_test[emp_mask]) > 0 else 0
                }
        
        print(f"âœ… {len(employee_attention_results)}ëª…ì˜ ì§ì›ë³„ Attention Importance ê³„ì‚° ì™„ë£Œ")
        return employee_attention_results
    
    def calculate_gradient_importance_per_employee(self, model, X_test, y_test, employee_ids, feature_names, device, max_samples_per_employee=10):
        """ì§ì›ë³„ Gradient ê¸°ë°˜ Feature Importance ê³„ì‚°"""
        print("ğŸ” ì§ì›ë³„ Gradient Feature Importance ê³„ì‚° ì¤‘...")
        
        employee_gradient_results = {}
        unique_employees = np.unique(employee_ids)
        
        model.eval()
        
        for emp_id in unique_employees:
            emp_mask = employee_ids == emp_id
            emp_X = X_test[emp_mask]
            emp_y = y_test[emp_mask]
            
            if len(emp_X) == 0:
                continue
            
            # ìƒ˜í”Œ ìˆ˜ ì œí•œ
            if len(emp_X) > max_samples_per_employee:
                sample_indices = np.random.choice(len(emp_X), max_samples_per_employee, replace=False)
                emp_X_sample = emp_X[sample_indices]
            else:
                emp_X_sample = emp_X
            
            # PyTorch í…ì„œë¡œ ë³€í™˜ (gradient ê³„ì‚°ì„ ìœ„í•´ requires_grad=True)
            X_tensor = torch.FloatTensor(emp_X_sample).to(device)
            X_tensor.requires_grad_(True)
            
            try:
                # Forward pass
                outputs = model(X_tensor)
                
                # í‡´ì‚¬ í´ë˜ìŠ¤(1)ì— ëŒ€í•œ í™•ë¥ 
                probs = torch.softmax(outputs, dim=1)
                attrition_probs = probs[:, 1]
                
                # ê° ìƒ˜í”Œì— ëŒ€í•œ gradient ê³„ì‚°
                gradients_list = []
                
                for i in range(len(attrition_probs)):
                    if X_tensor.grad is not None:
                        X_tensor.grad.zero_()
                    
                    attrition_probs[i].backward(retain_graph=True)
                    
                    # Gradientì˜ ì ˆëŒ“ê°’ì„ ì¤‘ìš”ë„ë¡œ ì‚¬ìš©
                    sample_gradient = torch.abs(X_tensor.grad[i]).detach().cpu().numpy()
                    gradients_list.append(sample_gradient)
                
                # ëª¨ë“  ìƒ˜í”Œì˜ gradient í‰ê· 
                mean_gradients = np.mean(gradients_list, axis=0)  # (sequence_length, n_features)
                
                # ê° í”¼ì²˜ì˜ ì „ì²´ ì‹œê°„ì— ëŒ€í•œ í‰ê·  gradient ì¤‘ìš”ë„
                feature_gradient_importance = np.mean(mean_gradients, axis=0)
                
                employee_gradient_results[emp_id] = {
                    'gradient_by_timestep': mean_gradients,
                    'feature_importance': feature_gradient_importance,
                    'attrition_label': emp_y[0] if len(emp_y) > 0 else 0
                }
                
            except Exception as e:
                # ë¡œê·¸ ìŠ¤íŒ¸ ë°©ì§€ - ì²« ë²ˆì§¸ ì˜¤ë¥˜ë§Œ ì¶œë ¥
                if emp_id == list(unique_employees)[0]:
                    print(f"âš ï¸ Gradient ê³„ì‚° ì‹¤íŒ¨ (ëŒ€í‘œ ì˜¤ë¥˜): {e}")
                    print("âš ï¸ ëª¨ë“  ì§ì›ì— ëŒ€í•´ ë™ì¼í•œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¡œê·¸ ìŠ¤íŒ¸ ë°©ì§€ë¥¼ ìœ„í•´ ì¶”ê°€ ì˜¤ë¥˜ëŠ” ì¶œë ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                continue
        
        print(f"âœ… {len(employee_gradient_results)}ëª…ì˜ ì§ì›ë³„ Gradient Importance ê³„ì‚° ì™„ë£Œ")
        return employee_gradient_results
    
    def plot_employee_feature_importance_comparison(self, attention_results, gradient_results, feature_names, 
                                                  top_employees=10, top_features=15, save_path=None):
        """ì§ì›ë³„ Feature Importance ë¹„êµ ì‹œê°í™”"""
        print("ğŸ“Š ì§ì›ë³„ Feature Importance ë¹„êµ ì‹œê°í™” ì¤‘...")
        
        # í‡´ì‚¬ í™•ë¥ ì´ ë†’ì€ ìƒìœ„ ì§ì›ë“¤ ì„ íƒ
        employee_scores = {}
        for emp_id in attention_results.keys():
            if emp_id in gradient_results:
                # Attentionê³¼ Gradient ì¤‘ìš”ë„ì˜ í‰ê· ìœ¼ë¡œ ì¢…í•© ì ìˆ˜ ê³„ì‚°
                att_score = np.max(attention_results[emp_id]['feature_importance'])
                grad_score = np.max(gradient_results[emp_id]['feature_importance'])
                employee_scores[emp_id] = (att_score + grad_score) / 2
        
        # ìƒìœ„ ì§ì›ë“¤ ì„ íƒ
        top_employee_ids = sorted(employee_scores.keys(), key=lambda x: employee_scores[x], reverse=True)[:top_employees]
        
        # ì„œë¸Œí”Œë¡¯ ìƒì„±
        fig = make_subplots(
            rows=len(top_employee_ids), cols=2,
            subplot_titles=[f'Employee {emp_id} - Attention' if i % 2 == 0 else f'Employee {emp_id} - Gradient' 
                          for emp_id in top_employee_ids for i in range(2)],
            vertical_spacing=0.02,
            horizontal_spacing=0.1,
            specs=[[{"type": "bar"}, {"type": "bar"}] for _ in range(len(top_employee_ids))]
        )
        
        colors = px.colors.qualitative.Set3
        
        for idx, emp_id in enumerate(top_employee_ids):
            # Attention ì¤‘ìš”ë„
            att_importance = attention_results[emp_id]['feature_importance']
            att_sorted_indices = np.argsort(att_importance)[-top_features:]
            att_top_features = [feature_names[i] for i in att_sorted_indices]
            att_top_values = att_importance[att_sorted_indices]
            
            fig.add_trace(
                go.Bar(
                    x=att_top_values,
                    y=att_top_features,
                    orientation='h',
                    name=f'Emp {emp_id} Attention',
                    marker_color=colors[idx % len(colors)],
                    showlegend=False
                ),
                row=idx+1, col=1
            )
            
            # Gradient ì¤‘ìš”ë„
            grad_importance = gradient_results[emp_id]['feature_importance']
            grad_sorted_indices = np.argsort(grad_importance)[-top_features:]
            grad_top_features = [feature_names[i] for i in grad_sorted_indices]
            grad_top_values = grad_importance[grad_sorted_indices]
            
            fig.add_trace(
                go.Bar(
                    x=grad_top_values,
                    y=grad_top_features,
                    orientation='h',
                    name=f'Emp {emp_id} Gradient',
                    marker_color=colors[idx % len(colors)],
                    showlegend=False
                ),
                row=idx+1, col=2
            )
        
        fig.update_layout(
            title=f"Top {top_employees} Employees - Feature Importance Comparison (Attention vs Gradient)",
            height=200 * len(top_employee_ids),
            template="plotly_white"
        )
        
        if save_path:
            fig.write_html(save_path)
            print(f"ğŸ’¾ ì§ì›ë³„ Feature Importance ë¹„êµ ê·¸ë˜í”„ ì €ì¥: {save_path}")
        
        return fig.to_html(include_plotlyjs='cdn')
    
    def plot_employee_attention_heatmap(self, attention_results, feature_names, employee_ids_to_show=20, save_path=None):
        """ì§ì›ë³„ Attention Weights íˆíŠ¸ë§µ"""
        print("ğŸ“Š ì§ì›ë³„ Attention Heatmap ì‹œê°í™” ì¤‘...")
        
        # ìƒìœ„ ì§ì›ë“¤ ì„ íƒ (attention ì ìˆ˜ ê¸°ì¤€)
        employee_scores = {emp_id: np.max(results['feature_importance']) 
                          for emp_id, results in attention_results.items()}
        top_employees = sorted(employee_scores.keys(), key=lambda x: employee_scores[x], reverse=True)[:employee_ids_to_show]
        
        # íˆíŠ¸ë§µ ë°ì´í„° ì¤€ë¹„
        attention_matrix = []
        employee_labels = []
        
        for emp_id in top_employees:
            attention_weights = attention_results[emp_id]['attention_weights']
            attention_matrix.append(attention_weights)
            attrition_status = "í‡´ì‚¬" if attention_results[emp_id]['attrition_label'] == 1 else "ì¬ì§"
            employee_labels.append(f"ì§ì› {emp_id} ({attrition_status})")
        
        attention_matrix = np.array(attention_matrix)
        
        # ì‹œê°„ ë‹¨ê³„ ë¼ë²¨
        time_labels = [f"Week -{len(attention_weights)-i}" for i in range(len(attention_weights))]
        
        fig = go.Figure(data=go.Heatmap(
            z=attention_matrix,
            x=time_labels,
            y=employee_labels,
            colorscale='YlOrRd',
            colorbar=dict(title="Attention Weight")
        ))
        
        fig.update_layout(
            title=f"Top {employee_ids_to_show} Employees - Attention Weights Heatmap",
            xaxis_title="Time Steps",
            yaxis_title="Employees",
            height=max(600, employee_ids_to_show * 25),
            template="plotly_white"
        )
        
        if save_path:
            fig.write_html(save_path)
            print(f"ğŸ’¾ Attention Heatmap ì €ì¥: {save_path}")
        
        return fig.to_html(include_plotlyjs='cdn')
    
    def plot_feature_importance_distribution(self, attention_results, gradient_results, feature_names, save_path=None):
        """Feature Importance ë¶„í¬ ë¶„ì„"""
        print("ğŸ“Š Feature Importance ë¶„í¬ ë¶„ì„ ì¤‘...")
        
        # ëª¨ë“  ì§ì›ì˜ feature importance ìˆ˜ì§‘
        all_attention_importance = []
        all_gradient_importance = []
        
        for emp_id in attention_results.keys():
            if emp_id in gradient_results:
                all_attention_importance.append(attention_results[emp_id]['feature_importance'])
                all_gradient_importance.append(gradient_results[emp_id]['feature_importance'])
        
        all_attention_importance = np.array(all_attention_importance)
        all_gradient_importance = np.array(all_gradient_importance)
        
        # ê° í”¼ì²˜ë³„ í‰ê·  ì¤‘ìš”ë„ ê³„ì‚°
        mean_attention = np.mean(all_attention_importance, axis=0)
        mean_gradient = np.mean(all_gradient_importance, axis=0)
        std_attention = np.std(all_attention_importance, axis=0)
        std_gradient = np.std(all_gradient_importance, axis=0)
        
        # ìƒìœ„ í”¼ì²˜ë“¤ ì„ íƒ
        top_features_idx = np.argsort(mean_attention + mean_gradient)[-20:]
        top_feature_names = [feature_names[i] for i in top_features_idx]
        
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('Attention Importance Distribution', 'Gradient Importance Distribution',
                          'Feature Importance Correlation', 'Top Features Comparison'),
            specs=[[{"type": "box"}, {"type": "box"}],
                   [{"type": "scatter"}, {"type": "bar"}]]
        )
        
        # 1. Attention Importance Box Plot
        for i, feature_idx in enumerate(top_features_idx[-10:]):  # ìƒìœ„ 10ê°œë§Œ
            fig.add_trace(
                go.Box(y=all_attention_importance[:, feature_idx], 
                      name=feature_names[feature_idx],
                      showlegend=False),
                row=1, col=1
            )
        
        # 2. Gradient Importance Box Plot  
        for i, feature_idx in enumerate(top_features_idx[-10:]):  # ìƒìœ„ 10ê°œë§Œ
            fig.add_trace(
                go.Box(y=all_gradient_importance[:, feature_idx], 
                      name=feature_names[feature_idx],
                      showlegend=False),
                row=1, col=2
            )
        
        # 3. Correlation Scatter Plot
        fig.add_trace(
            go.Scatter(
                x=mean_attention[top_features_idx],
                y=mean_gradient[top_features_idx],
                mode='markers+text',
                text=[f'F{i}' for i in range(len(top_features_idx))],
                textposition="top center",
                marker=dict(size=8, color='blue'),
                name='Features',
                showlegend=False
            ),
            row=2, col=1
        )
        
        # 4. Top Features Comparison
        fig.add_trace(
            go.Bar(
                x=top_feature_names,
                y=mean_attention[top_features_idx],
                name='Attention',
                marker_color='lightblue',
                error_y=dict(type='data', array=std_attention[top_features_idx])
            ),
            row=2, col=2
        )
        
        fig.add_trace(
            go.Bar(
                x=top_feature_names,
                y=mean_gradient[top_features_idx],
                name='Gradient',
                marker_color='lightcoral',
                error_y=dict(type='data', array=std_gradient[top_features_idx])
            ),
            row=2, col=2
        )
        
        fig.update_layout(
            title="Feature Importance Distribution Analysis",
            height=800,
            template="plotly_white"
        )
        
        if save_path:
            fig.write_html(save_path)
            print(f"ğŸ’¾ Feature Importance ë¶„í¬ ë¶„ì„ ì €ì¥: {save_path}")
        
        return fig.to_html(include_plotlyjs='cdn')