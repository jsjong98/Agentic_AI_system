"""
Supervisor Flask ë°±ì—”ë“œ
ìŠˆí¼ë°”ì´ì € ì—ì´ì „íŠ¸ì˜ REST API ì„œë²„
"""

from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
from werkzeug.utils import secure_filename
import asyncio
import logging
import traceback
from datetime import datetime
from typing import Dict, Any, Optional
import os
import json
from pathlib import Path
import pandas as pd
from dotenv import load_dotenv
import requests
import time

from langchain_openai import ChatOpenAI
import openai

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

from .langgraph_workflow import SupervisorWorkflow
from .worker_integrator import DEFAULT_WORKER_CONFIGS
from .agent_state import AgentState
from .hierarchical_result_manager import hierarchical_result_manager

# Flask ì•± ìƒì„±
app = Flask(__name__)

# CORS ì„¤ì • (React ì—°ë™)
CORS(app, resources={
    r"/*": {
        "origins": ["http://localhost:3000", "http://127.0.0.1:3000"],
        "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"],
        "allow_headers": ["Content-Type", "Authorization", "Access-Control-Allow-Origin"],
        "supports_credentials": True
    }
})

# Flask ì„¤ì •
app.config['JSON_AS_ASCII'] = False
app.config['JSONIFY_PRETTYPRINT_REGULAR'] = True
app.config['MAX_CONTENT_LENGTH'] = 300 * 1024 * 1024  # 300MB íŒŒì¼ ì—…ë¡œë“œ ì œí•œ

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜
supervisor_workflow: Optional[SupervisorWorkflow] = None
active_sessions: Dict[str, Dict[str, Any]] = {}
openai_client: Optional[openai.OpenAI] = None


def initialize_supervisor():
    """ìŠˆí¼ë°”ì´ì € ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”"""
    global supervisor_workflow, openai_client
    
    try:
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            logger.warning("OPENAI_API_KEY not found in environment variables")
            logger.warning("Supervisor ì„œë²„ëŠ” OpenAI API í‚¤ ì—†ì´ ì‹œì‘ë©ë‹ˆë‹¤. LLM ê¸°ëŠ¥ì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
            llm = None
            openai_client = None
        else:
            # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            openai_client = openai.OpenAI(api_key=openai_api_key)
            
            # LLM ì´ˆê¸°í™” (gpt-5 ëª¨ë¸ ì‚¬ìš©)
            try:
                from langchain_openai import ChatOpenAI
                llm = ChatOpenAI(
                    model="gpt-5",
                    temperature=0.1,
                    api_key=openai_api_key
                )
                logger.info("âœ… OpenAI LLM (gpt-5) ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as llm_error:
                logger.warning(f"âš ï¸ OpenAI LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {llm_error}")
                logger.warning("LLM ì—†ì´ Supervisor ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
                llm = None
        
        # ì›Œì»¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
        worker_configs = DEFAULT_WORKER_CONFIGS.copy()
        
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì›Œì»¤ URL ì˜¤ë²„ë¼ì´ë“œ
        for worker_name in worker_configs.keys():
            env_key = f"{worker_name.upper()}_URL"
            env_url = os.getenv(env_key)
            if env_url:
                worker_configs[worker_name]["base_url"] = env_url
                logger.info(f"Using {env_key}: {env_url}")
        
        # ìŠˆí¼ë°”ì´ì € ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”
        supervisor_workflow = SupervisorWorkflow(
            worker_configs=worker_configs,
            llm=llm,
            max_retry_count=int(os.getenv("MAX_RETRY_COUNT", "3")),
            timeout_minutes=int(os.getenv("TIMEOUT_MINUTES", "30"))
        )
        
        logger.info("Supervisor workflow initialized successfully")
        
        # ì €ì¥ ê²½ë¡œ í™•ì¸ ë¡œê·¸
        logger.info(f"ğŸ“ Hierarchical Result Manager ì €ì¥ ê²½ë¡œ: {hierarchical_result_manager.base_output_dir}")
        logger.info(f"ğŸ“ ì ˆëŒ€ ê²½ë¡œ: {hierarchical_result_manager.base_output_dir.resolve()}")
        
    except Exception as e:
        logger.error(f"Failed to initialize supervisor workflow: {e}")
        logger.error(traceback.format_exc())
        raise


@app.route('/health', methods=['GET'])
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    try:
        status = {
            'status': 'healthy',
            'service': 'Supervisor',
            'timestamp': datetime.now().isoformat(),
            'version': '1.0.0'
        }
        
        if supervisor_workflow:
            status['workflow_initialized'] = True
            status['available_workers'] = supervisor_workflow.get_available_workers()
        else:
            status['workflow_initialized'] = False
            status['error'] = 'Workflow not initialized'
        
        return jsonify(status)
        
    except Exception as e:
        logger.error(f"Health check error: {e}")
        return jsonify({
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500


@app.route('/analyze_employee', methods=['POST'])
def analyze_employee():
    """ì§ì› ë¶„ì„ ìš”ì²­"""
    try:
        if not supervisor_workflow:
            return jsonify({
                'success': False,
                'error': 'Supervisor workflow not initialized'
            }), 500
        
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'No JSON data provided'
            }), 400
        
        employee_id = data.get('employee_id')
        if not employee_id:
            return jsonify({
                'success': False,
                'error': 'employee_id is required'
            }), 400
        
        session_id = data.get('session_id')
        
        logger.info(f"Starting analysis for employee {employee_id}")
        
        # ë¹„ë™ê¸° ë¶„ì„ ì‹¤í–‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            result = loop.run_until_complete(
                supervisor_workflow.analyze_employee(employee_id, session_id)
            )
            
            # í™œì„± ì„¸ì…˜ì— ê²°ê³¼ ì €ì¥
            if result.get('session_id'):
                active_sessions[result['session_id']] = {
                    'employee_id': employee_id,
                    'result': result,
                    'created_at': datetime.now().isoformat(),
                    'status': 'completed' if result['success'] else 'failed'
                }
            
            logger.info(f"Analysis completed for employee {employee_id}")
            return jsonify(result)
            
        finally:
            loop.close()
        
    except Exception as e:
        logger.error(f"Analysis error for employee {data.get('employee_id', 'unknown')}: {e}")
        logger.error(traceback.format_exc())
        
        return jsonify({
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500


@app.route('/get_workflow_status/<session_id>', methods=['GET'])
def get_workflow_status(session_id: str):
    """ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì¡°íšŒ"""
    try:
        if not supervisor_workflow:
            return jsonify({
                'success': False,
                'error': 'Supervisor workflow not initialized'
            }), 500
        
        # ë¹„ë™ê¸° ìƒíƒœ ì¡°íšŒ
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            status = loop.run_until_complete(
                supervisor_workflow.get_workflow_status(session_id)
            )
            
            return jsonify({
                'success': True,
                'session_id': session_id,
                'status': status
            })
            
        finally:
            loop.close()
        
    except Exception as e:
        logger.error(f"Status query error for session {session_id}: {e}")
        
        return jsonify({
            'success': False,
            'error': str(e),
            'session_id': session_id
        }), 500


@app.route('/list_active_sessions', methods=['GET'])
def list_active_sessions():
    """í™œì„± ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ"""
    try:
        sessions = []
        for session_id, session_data in active_sessions.items():
            sessions.append({
                'session_id': session_id,
                'employee_id': session_data['employee_id'],
                'status': session_data['status'],
                'created_at': session_data['created_at']
            })
        
        return jsonify({
            'success': True,
            'active_sessions': sessions,
            'total_sessions': len(sessions)
        })
        
    except Exception as e:
        logger.error(f"List sessions error: {e}")
        
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/get_session_result/<session_id>', methods=['GET'])
def get_session_result(session_id: str):
    """ì„¸ì…˜ ê²°ê³¼ ì¡°íšŒ"""
    try:
        if session_id not in active_sessions:
            return jsonify({
                'success': False,
                'error': 'Session not found'
            }), 404
        
        session_data = active_sessions[session_id]
        
        return jsonify({
            'success': True,
            'session_id': session_id,
            'employee_id': session_data['employee_id'],
            'status': session_data['status'],
            'created_at': session_data['created_at'],
            'result': session_data['result']
        })
        
    except Exception as e:
        logger.error(f"Get session result error for {session_id}: {e}")
        
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/worker_health_check', methods=['GET'])
def worker_health_check():
    """ì›Œì»¤ ì—ì´ì „íŠ¸ ìƒíƒœ í™•ì¸"""
    try:
        if not supervisor_workflow:
            return jsonify({
                'success': False,
                'error': 'Supervisor workflow not initialized'
            }), 500
        
        # ë¹„ë™ê¸° í—¬ìŠ¤ì²´í¬
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            health_status = loop.run_until_complete(
                supervisor_workflow.worker_integrator.health_check_all()
            )
            
            # ê²°ê³¼ ë³€í™˜
            worker_status = {}
            for worker_type, is_healthy in health_status.items():
                worker_status[worker_type.value] = {
                    'healthy': is_healthy,
                    'status': 'online' if is_healthy else 'offline'
                }
            
            total_workers = len(worker_status)
            healthy_workers = sum(1 for status in worker_status.values() if status['healthy'])
            
            return jsonify({
                'success': True,
                'worker_status': worker_status,
                'summary': {
                    'total_workers': total_workers,
                    'healthy_workers': healthy_workers,
                    'unhealthy_workers': total_workers - healthy_workers,
                    'health_rate': healthy_workers / total_workers if total_workers > 0 else 0
                },
                'timestamp': datetime.now().isoformat()
            })
            
        finally:
            loop.close()
        
    except Exception as e:
        logger.error(f"Worker health check error: {e}")
        
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/system_info', methods=['GET'])
def system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ"""
    try:
        info = {
            'service': 'Supervisor',
            'version': '1.0.0',
            'timestamp': datetime.now().isoformat(),
            'workflow_initialized': supervisor_workflow is not None,
            'active_sessions_count': len(active_sessions),
            'environment': {
                'max_retry_count': os.getenv('MAX_RETRY_COUNT', '3'),
                'timeout_minutes': os.getenv('TIMEOUT_MINUTES', '30'),
                'openai_api_key_configured': bool(os.getenv('OPENAI_API_KEY'))
            }
        }
        
        if supervisor_workflow:
            info['available_workers'] = supervisor_workflow.get_available_workers()
            info['worker_configs'] = supervisor_workflow.worker_integrator.get_worker_status()
        
        return jsonify(info)
        
    except Exception as e:
        logger.error(f"System info error: {e}")
        
        return jsonify({
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500


def analyze_single_agent_sync(agent_name, employee_id, request_data):
    """ê°œë³„ ì—ì´ì „íŠ¸ ë¶„ì„ í•¨ìˆ˜ (ë™ê¸° ë²„ì „)"""
    try:
        analysis_type = request_data.get('analysis_type', 'batch')
        
        if agent_name == 'structura':
            # Structura ë¶„ì„ - ê°œë³„ ì§ì› ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš© (ë°°ì¹˜ ë¶„ì„ ì‹œì—ë„)
            url = f"{os.getenv('STRUCTURA_URL', 'http://localhost:5001')}/api/predict"
            response = requests.post(url, json={'employee_id': employee_id, 'analysis_type': analysis_type, **request_data})
            return response.json() if response.ok else {'error': f'Structura API error: {response.status_code}'}
            
        elif agent_name == 'cognita':
            # Cognita ë¶„ì„ - employee_idë¡œ ê´€ê³„ ë¶„ì„ (post ë°ì´í„° ë¶ˆí•„ìš”)
            url = f"{os.getenv('COGNITA_URL', 'http://localhost:5002')}/api/analyze/employee/{employee_id}"
            response = requests.get(url)
            return response.json() if response.ok else {'error': f'Cognita API error: {response.status_code}'}
            
        elif agent_name == 'chronos':
            # Chronos ë¶„ì„ - ê°œë³„ ì§ì› ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš© (ë°°ì¹˜ ë¶„ì„ ì‹œì—ë„)
            url = f"{os.getenv('CHRONOS_URL', 'http://localhost:5003')}/api/predict"
            response = requests.post(url, json={'employee_id': employee_id, 'analysis_type': analysis_type})
            return response.json() if response.ok else {'error': f'Chronos API error: {response.status_code}'}
            
        elif agent_name == 'sentio':
            # Sentio ë¶„ì„ - employee_idë¡œ ê°ì • ë¶„ì„ (post ë°ì´í„° ë¶ˆí•„ìš”)
            url = f"{os.getenv('SENTIO_URL', 'http://localhost:5004')}/analyze_sentiment"
            response = requests.post(url, json={'employee_id': employee_id, 'analysis_type': 'batch'})
            return response.json() if response.ok else {'error': f'Sentio API error: {response.status_code}'}
            
        elif agent_name == 'agora':
            # Agora ë¶„ì„ - ì‹¤ì œ ì§ì› ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ì—¬ ì •í™•í•œ ì‹œì¥ ë¶„ì„ ìˆ˜í–‰
            url = f"{os.getenv('AGORA_URL', 'http://localhost:5005')}/api/agora/comprehensive-analysis"
            
            employee_data = None
            if 'employees' in request_data:
                for emp in request_data['employees']:
                    emp_id = emp.get('EmployeeNumber') or emp.get('employee_id') or emp.get('id')
                    if str(emp_id) == str(employee_id):
                        employee_data = emp
                        break
            
            if employee_data:
                logger.info(f"ğŸ“Š Agoraì—ê²Œ ì§ì› {employee_id}ì˜ ì‹¤ì œ ë°ì´í„° ì „ë‹¬: {list(employee_data.keys())}")
                response = requests.post(url, json={
                    'employee_id': employee_id, 
                    'analysis_type': analysis_type,
                    **employee_data  # ì‹¤ì œ ì§ì› ë°ì´í„° í¬í•¨
                })
            else:
                logger.warning(f"âš ï¸ ì§ì› {employee_id}ì˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. employee_idë§Œ ì „ë‹¬í•©ë‹ˆë‹¤.")
                response = requests.post(url, json={'employee_id': employee_id, 'analysis_type': analysis_type})
            
            return response.json() if response.ok else {'error': f'Agora API error: {response.status_code}'}
            
        else:
            return {'error': f'Unknown agent: {agent_name}'}
            
    except Exception as e:
        logger.error(f"Error in analyze_single_agent_sync for {agent_name}: {e}")
        return {'error': str(e)}


@app.route('/batch_progress/<batch_id>', methods=['GET'])
def get_batch_progress(batch_id):
    """ë°°ì¹˜ ë¶„ì„ ì§„í–‰ë¥  ì¡°íšŒ"""
    try:
        if not hasattr(app, 'batch_progress') or batch_id not in app.batch_progress:
            return jsonify({
                'success': False,
                'error': 'Batch session not found'
            }), 404
        
        progress_data = app.batch_progress[batch_id]
        
        return jsonify({
            'success': True,
            'batch_id': batch_id,
            'status': progress_data['status'],
            'overall_progress': progress_data.get('overall_progress', 0),
            'current_agent': progress_data.get('current_agent'),
            'agent_progress': progress_data.get('agent_progress', {}),
            'total_employees': progress_data['total_employees'],
            'start_time': progress_data['start_time'],
            'end_time': progress_data.get('end_time'),
            'completed': progress_data['status'] in ['completed', 'failed']
        })
        
    except Exception as e:
        logger.error(f"Batch progress error: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/clear_sessions', methods=['POST'])
def clear_sessions():
    """ì„¸ì…˜ ì •ë¦¬"""
    try:
        data = request.get_json() or {}
        older_than_hours = data.get('older_than_hours', 24)
        
        current_time = datetime.now()
        cleared_count = 0
        
        sessions_to_remove = []
        for session_id, session_data in active_sessions.items():
            created_at = datetime.fromisoformat(session_data['created_at'])
            age_hours = (current_time - created_at).total_seconds() / 3600
            
            if age_hours > older_than_hours:
                sessions_to_remove.append(session_id)
        
        for session_id in sessions_to_remove:
            del active_sessions[session_id]
            cleared_count += 1
        
        return jsonify({
            'success': True,
            'cleared_sessions': cleared_count,
            'remaining_sessions': len(active_sessions),
            'older_than_hours': older_than_hours
        })
        
    except Exception as e:
        logger.error(f"Clear sessions error: {e}")
        
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/batch_analyze', methods=['POST'])
def batch_analyze():
    """ë°°ì¹˜ ë¶„ì„ (ì—¬ëŸ¬ ì§ì› ë™ì‹œ ë¶„ì„)"""
    try:
        if not supervisor_workflow:
            return jsonify({
                'success': False,
                'error': 'Supervisor workflow not initialized'
            }), 500
        
        data = request.get_json()
        if not data:
            logger.error("No JSON data provided in batch_analyze request")
            return jsonify({
                'success': False,
                'error': 'No JSON data provided'
            }), 400
        
        logger.info(f"Received batch_analyze request with keys: {list(data.keys())}")
        
        employee_ids = data.get('employee_ids', [])
        logger.info(f"Employee IDs received: {employee_ids}")
        logger.info(f"Employee IDs type: {type(employee_ids)}")
        
        if not employee_ids or not isinstance(employee_ids, list):
            logger.error(f"Invalid employee_ids: {employee_ids} (type: {type(employee_ids)})")
            return jsonify({
                'success': False,
                'error': 'employee_ids list is required'
            }), 400
        
        # ë¶„ì„ íƒ€ì… ì¶”ì¶œ (ê¸°ë³¸ê°’: batch)
        analysis_type = data.get('analysis_type', 'batch')
        logger.info(f"Analysis type: {analysis_type}")
        
        max_batch_size = int(os.getenv('MAX_BATCH_SIZE', '2000'))  # ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ë¶„ì„ì„ ìœ„í•´ 2000ìœ¼ë¡œ ì¦ê°€
        if len(employee_ids) > max_batch_size:
            logger.warning(f"Large batch size detected: {len(employee_ids)} employees (max: {max_batch_size})")
            return jsonify({
                'success': False,
                'error': f'Batch size exceeds maximum ({max_batch_size}). í˜„ì¬ ìš”ì²­: {len(employee_ids)}ëª…'
            }), 400
        
        logger.info(f"ğŸš€ Starting batch analysis for {len(employee_ids)} employees")
        logger.info(f"ğŸ“Š Analysis configuration: {data.get('integration_config', {})}")
        logger.info(f"ğŸ”§ Agent configuration: {data.get('agentConfig', {})}")
        logger.info(f"ğŸ“ Agent files: {data.get('agent_files', {})}")
        
        # ë°°ì¹˜ ì§„í–‰ë¥  ì¶”ì ì„ ìœ„í•œ ì„¸ì…˜ ìƒì„±
        import uuid
        batch_id = str(uuid.uuid4())
        
        # ì „ì—­ ë°°ì¹˜ ìƒíƒœ ì €ì¥ì†Œ ì´ˆê¸°í™”
        if not hasattr(app, 'batch_progress'):
            app.batch_progress = {}
        
        # ë°°ì¹˜ ì§„í–‰ë¥  ì´ˆê¸°í™” (ì—ì´ì „íŠ¸ë³„ ì§„í–‰ë¥  ì¶”ì )
        app.batch_progress[batch_id] = {
            'total_employees': len(employee_ids),
            'current_agent': 'structura',
            'agent_progress': {
                'structura': 0,
                'cognita': 0,
                'chronos': 0,
                'sentio': 0,
                'agora': 0
            },
            'overall_progress': 0,
            'status': 'processing',
            'start_time': datetime.now().isoformat(),
            'results': {}
        }
        
        # ì—ì´ì „íŠ¸ë³„ ìˆœì°¨ ë°°ì¹˜ ë¶„ì„
        agents = ['structura', 'cognita', 'chronos', 'sentio', 'agora']
        agent_results = {}
        
        for agent_idx, agent_name in enumerate(agents):
            logger.info(f"ğŸš€ Starting {agent_name} analysis for {len(employee_ids)} employees")
            app.batch_progress[batch_id]['current_agent'] = agent_name
            
            # ê° ì—ì´ì „íŠ¸ë³„ë¡œ ëª¨ë“  ì§ì› ì²˜ë¦¬
            agent_results[agent_name] = []
            
            for emp_idx, employee_id in enumerate(employee_ids):
                logger.info(f"ğŸ”® {agent_name}: Processing employee {emp_idx+1}/{len(employee_ids)}: {employee_id}")
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ì²˜ë¦¬ ì‹œì‘ ì‹œì )
                agent_progress = (emp_idx / len(employee_ids)) * 100
                app.batch_progress[batch_id]['agent_progress'][agent_name] = round(agent_progress, 1)
                
                # ì „ì²´ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ì²˜ë¦¬ ì‹œì‘ ì‹œì )
                overall_progress = ((agent_idx * len(employee_ids) + emp_idx) / (len(agents) * len(employee_ids))) * 100
                app.batch_progress[batch_id]['overall_progress'] = round(overall_progress, 1)
                
                logger.info(f"ğŸ“Š ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: {agent_name} {agent_progress:.1f}%, ì „ì²´ {overall_progress:.1f}%")
                
                try:
                    # ê°œë³„ ì—ì´ì „íŠ¸ ë¶„ì„ (employee_idë§Œ ì „ë‹¬)
                    result = analyze_single_agent_sync(agent_name, employee_id, data)
                    agent_results[agent_name].append({
                        'employee_id': employee_id,
                        'success': True,
                        'result': result
                    })
                    logger.info(f"âœ… {agent_name}: Employee {employee_id} ë¶„ì„ ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"âŒ {agent_name} error for employee {employee_id}: {e}")
                    agent_results[agent_name].append({
                        'employee_id': employee_id,
                        'success': False,
                        'error': str(e)
                    })
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ì²˜ë¦¬ ì™„ë£Œ ì‹œì )
                agent_progress = ((emp_idx + 1) / len(employee_ids)) * 100
                app.batch_progress[batch_id]['agent_progress'][agent_name] = round(agent_progress, 1)
                
                # ì „ì²´ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ì²˜ë¦¬ ì™„ë£Œ ì‹œì )
                overall_progress = ((agent_idx * len(employee_ids) + emp_idx + 1) / (len(agents) * len(employee_ids))) * 100
                app.batch_progress[batch_id]['overall_progress'] = round(overall_progress, 1)
                
                logger.info(f"ğŸ“ˆ ì§„í–‰ë¥  ì™„ë£Œ: {agent_name} {agent_progress:.1f}%, ì „ì²´ {overall_progress:.1f}%")
            
            logger.info(f"âœ… {agent_name} analysis completed for all employees")
        
        # ê²°ê³¼ í†µí•© (ì—ì´ì „íŠ¸ë³„ ê²°ê³¼ë¥¼ ì§ì›ë³„ë¡œ ì¬êµ¬ì„±)
        logger.info(f"ğŸ”„ ê²°ê³¼ í†µí•© ì‹œì‘: {len(employee_ids)}ëª…ì˜ ì§ì› ê²°ê³¼ ì²˜ë¦¬")
        batch_results = []
        successful_count = 0
        
        for i, employee_id in enumerate(employee_ids):
            employee_result = {
                'employee_id': employee_id,
                'success': True,
                'agent_results': {}
            }
            
            # ê° ì—ì´ì „íŠ¸ ê²°ê³¼ ìˆ˜ì§‘
            agent_success_count = 0
            for agent_name in agents:
                agent_result = next((r for r in agent_results[agent_name] if r['employee_id'] == employee_id), None)
                if agent_result:
                    employee_result['agent_results'][agent_name] = agent_result
                    if agent_result['success']:
                        agent_success_count += 1
                    else:
                        employee_result['success'] = False
                        logger.warning(f"âš ï¸ ì§ì› {employee_id}: {agent_name} ë¶„ì„ ì‹¤íŒ¨")
            
            batch_results.append(employee_result)
            if employee_result['success']:
                successful_count += 1
            
            # ì§„í–‰ë¥  ë¡œê·¸ (100ëª…ë§ˆë‹¤)
            if (i + 1) % 100 == 0 or (i + 1) == len(employee_ids):
                logger.info(f"ğŸ“Š ê²°ê³¼ í†µí•© ì§„í–‰ë¥ : {i + 1}/{len(employee_ids)} ({((i + 1)/len(employee_ids)*100):.1f}%)")
                logger.info(f"   - ì„±ê³µí•œ ì§ì›: {successful_count}ëª…")
                logger.info(f"   - í‰ê·  ì—ì´ì „íŠ¸ ì„±ê³µë¥ : {agent_success_count}/{len(agents)}")
        
        # ë°°ì¹˜ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
        app.batch_progress[batch_id]['status'] = 'completed'
        app.batch_progress[batch_id]['processed_employees'] = len(employee_ids)
        app.batch_progress[batch_id]['end_time'] = datetime.now().isoformat()
        
        logger.info(f"ğŸ‰ ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ!")
        logger.info(f"   ğŸ“Š ì´ ì§ì› ìˆ˜: {len(employee_ids)}ëª…")
        logger.info(f"   âœ… ì„±ê³µí•œ ë¶„ì„: {successful_count}ëª…")
        logger.info(f"   âŒ ì‹¤íŒ¨í•œ ë¶„ì„: {len(employee_ids) - successful_count}ëª…")
        logger.info(f"   ğŸ“ˆ ì„±ê³µë¥ : {(successful_count/len(employee_ids)*100):.1f}%")
        logger.info(f"   ğŸ†” ë°°ì¹˜ ID: {batch_id}")
        
        # ì—ì´ì „íŠ¸ë³„ ì„±ê³µë¥  ë¡œê·¸
        for agent_name in agents:
            agent_success = sum(1 for r in agent_results[agent_name] if r.get('success', False))
            logger.info(f"   ğŸ¤– {agent_name}: {agent_success}/{len(employee_ids)} ({(agent_success/len(employee_ids)*100):.1f}%)")
            
        return jsonify({
            'success': True,
            'batch_id': batch_id,  # ì§„í–‰ë¥  ì¶”ì ì„ ìœ„í•œ batch_id ë°˜í™˜
            'batch_results': batch_results,
            'summary': {
                'total_employees': len(employee_ids),
                'successful_analyses': successful_count,
                'failed_analyses': len(employee_ids) - successful_count,
                'success_rate': successful_count / len(employee_ids)
            }
        })
            
    except Exception as e:
        logger.error(f"Batch analysis error: {e}")
        logger.error(traceback.format_exc())
        
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/post_analysis', methods=['POST'])
def post_analysis():
    """ì‚¬í›„ ë¶„ì„ ì „ìš© ì—”ë“œí¬ì¸íŠ¸ (ì›Œí¬í”Œë¡œìš° ë¬¸ì œ íšŒí”¼)"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'No JSON data provided'
            }), 400
        
        employee_ids = data.get('employee_ids', [])
        agent_config = data.get('agent_config', {})
        
        if not employee_ids:
            return jsonify({
                'success': False,
                'error': 'employee_ids list is required'
            }), 400
        
        logger.info(f"Starting post-analysis for {len(employee_ids)} employees")
        
        # ê° ì—ì´ì „íŠ¸ë³„ë¡œ ì§ì ‘ API í˜¸ì¶œ (ì›Œí¬í”Œë¡œìš° ìš°íšŒ)
        import requests
        
        results = {}
        successful_analyses = 0
        
        for employee_id in employee_ids:  # ì „ì²´ ì§ì› ë¶„ì„
            employee_result = {
                'employee_id': employee_id,
                'agent_results': {}
            }
            
            # ê° ì—ì´ì „íŠ¸ë³„ ë¶„ì„
            for agent_name, is_enabled in agent_config.items():
                if not is_enabled:
                    continue
                    
                agent_type = agent_name.replace('use_', '')
                
                try:
                    if agent_type == 'structura':
                        # Structura API í˜¸ì¶œ
                        response = requests.get(f'http://localhost:5001/api/predict/{employee_id}', timeout=10)
                        if response.status_code == 200:
                            employee_result['agent_results']['structura'] = response.json()
                    
                    elif agent_type == 'cognita':
                        # Cognita API í˜¸ì¶œ
                        response = requests.get(f'http://localhost:5002/api/analyze/employee/{employee_id}', timeout=10)
                        if response.status_code == 200:
                            employee_result['agent_results']['cognita'] = response.json()
                    
                    elif agent_type == 'chronos':
                        # Chronos API í˜¸ì¶œ
                        response = requests.post(f'http://localhost:5003/api/predict', 
                                               json={'employee_id': employee_id}, timeout=10)
                        if response.status_code == 200:
                            employee_result['agent_results']['chronos'] = response.json()
                    
                    elif agent_type == 'sentio':
                        # Sentio API í˜¸ì¶œ
                        response = requests.post(f'http://localhost:5004/analyze_sentiment', 
                                               json={'employee_id': employee_id}, timeout=10)
                        if response.status_code == 200:
                            employee_result['agent_results']['sentio'] = response.json()
                    
                    elif agent_type == 'agora':
                        # Agora API í˜¸ì¶œ
                        response = requests.post(f'http://localhost:5005/api/agora/comprehensive-analysis', 
                                               json={'employee_id': employee_id}, timeout=10)
                        if response.status_code == 200:
                            employee_result['agent_results']['agora'] = response.json()
                
                except Exception as e:
                    logger.warning(f"Agent {agent_type} failed for employee {employee_id}: {e}")
                    employee_result['agent_results'][agent_type] = {'error': str(e)}
            
            results[employee_id] = employee_result
            if len(employee_result['agent_results']) > 0:
                successful_analyses += 1
        
        return jsonify({
            'success': True,
            'results': results,
            'summary': {
                'total_employees': len(employee_ids),
                'successful_analyses': successful_analyses,
                'processed_employees': len(results)
            }
        })
        
    except Exception as e:
        logger.error(f"Post-analysis error: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/chat', methods=['POST'])
def chat_with_llm():
    """
    LLMê³¼ ì±„íŒ…í•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸ (GPT-5-nano-2025-08-07 ì‚¬ìš©)
    """
    try:
        data = request.get_json()
        
        if not data or 'message' not in data:
            return jsonify({"error": "ë©”ì‹œì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        user_message = data['message']
        context = data.get('context', {})  # ë¶„ì„ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸
        
        if not openai_client:
            return jsonify({"error": "OpenAI APIê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 500
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ ìœ í˜• íŒë‹¨
        is_simple_greeting = is_greeting_or_simple_question(user_message)
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = create_system_prompt(context, is_simple_greeting)
        
        # OpenAI API í˜¸ì¶œ (GPT-5-nano-2025-08-07 ì‚¬ìš©)
        try:
            # GPT-5-nano ëª¨ë¸ ì‚¬ìš© ì‹œë„
            response = openai_client.responses.create(
                model="gpt-5-nano-2025-08-07",
                input=f"{system_prompt}\n\nUser: {user_message}",
                reasoning={"effort": "medium"},
                text={"verbosity": "medium"}
            )
            ai_response = response.output_text
            model_used = "gpt-5-nano-2025-08-07"
            tokens_used = len(ai_response.split())  # ëŒ€ëµì ì¸ í† í° ìˆ˜
            
        except Exception as e:
            logger.warning(f"GPT-5-nano í˜¸ì¶œ ì‹¤íŒ¨, GPT-4o-minië¡œ fallback: {e}")
            # Fallback to GPT-4o-mini
            response = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            ai_response = response.choices[0].message.content
            model_used = "gpt-4o-mini"
            tokens_used = response.usage.total_tokens if response.usage else 0
        
        return jsonify({
            "response": ai_response,
            "timestamp": datetime.now().isoformat(),
            "model": model_used,
            "tokens_used": tokens_used
        })
        
    except Exception as e:
        logger.error(f"ì±„íŒ… API ì˜¤ë¥˜: {str(e)}")
        return jsonify({"error": f"ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}), 500


def is_greeting_or_simple_question(message: str) -> bool:
    """ì‚¬ìš©ì ë©”ì‹œì§€ê°€ ë‹¨ìˆœí•œ ì¸ì‚¬ë§ì´ë‚˜ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì¸ì§€ íŒë‹¨"""
    message_lower = message.lower().strip()
    
    # ì¸ì‚¬ë§ íŒ¨í„´
    greetings = [
        'ì•ˆë…•', 'ì•ˆë…•í•˜ì„¸ìš”', 'ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ', 'ë°˜ê°‘ìŠµë‹ˆë‹¤', 'ì²˜ìŒ ëµ™ê² ìŠµë‹ˆë‹¤',
        'hello', 'hi', 'í•˜ì´', 'í—¬ë¡œ', 'ì¢‹ì€ ì•„ì¹¨', 'ì¢‹ì€ ì˜¤í›„', 'ì¢‹ì€ ì €ë…'
    ]
    
    # ê°„ë‹¨í•œ ì§ˆë¬¸ íŒ¨í„´
    simple_questions = [
        'ë­í•´', 'ë­ í•˜ì„¸ìš”', 'ì–´ë–»ê²Œ ì§€ë‚´', 'ì˜ ì§€ë‚´', 'ì–´ë– ì„¸ìš”', 'ê´œì°®ì•„',
        'ë„ì›€', 'ë„ì™€ì¤˜', 'ë„ì™€ì£¼ì„¸ìš”', 'ë­ê°€ ê°€ëŠ¥í•´', 'ë­˜ í•  ìˆ˜ ìˆì–´',
        'ê¸°ëŠ¥', 'ì‚¬ìš©ë²•', 'ì–´ë–»ê²Œ ì‚¬ìš©'
    ]
    
    # ë©”ì‹œì§€ê°€ ì§§ê³  (20ì ì´í•˜) ì¸ì‚¬ë§ì´ë‚˜ ê°„ë‹¨í•œ ì§ˆë¬¸ íŒ¨í„´ì— í•´ë‹¹í•˜ëŠ”ì§€ í™•ì¸
    if len(message_lower) <= 20:
        for greeting in greetings:
            if greeting in message_lower:
                return True
        for question in simple_questions:
            if question in message_lower:
                return True
    
    return False


def create_system_prompt(context: Dict[str, Any], is_simple_greeting: bool = False) -> str:
    """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    
    if is_simple_greeting:
        # ê°„ë‹¨í•œ ì¸ì‚¬ë§ì´ë‚˜ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸
        base_prompt = """ë‹¹ì‹ ì€ Retain Sentinel 360 AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
HR ë°ì´í„° ë¶„ì„ê³¼ ì´ì§ ì˜ˆì¸¡ ë¶„ì„ì„ ë„ì™€ë“œë¦¬ëŠ” ì¹œê·¼í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ìê°€ ê°„ë‹¨í•œ ì¸ì‚¬ë§ì´ë‚˜ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì„ í–ˆìŠµë‹ˆë‹¤. 
ë‹¤ìŒê³¼ ê°™ì´ ì‘ë‹µí•´ì£¼ì„¸ìš”:
- ê°„ë‹¨í•˜ê³  ì¹œê·¼í•˜ê²Œ ì¸ì‚¬ë¥¼ ë‚˜ëˆ„ì„¸ìš” (2-3ë¬¸ì¥ ì •ë„)
- ì–´ë–¤ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆëŠ”ì§€ ê°„ëµí•˜ê²Œ ì•ˆë‚´í•´ì£¼ì„¸ìš”
- ë„ˆë¬´ ê¸¸ê±°ë‚˜ ì „ë¬¸ì ì¸ ì„¤ëª…ì€ í”¼í•´ì£¼ì„¸ìš”
- ë”°ëœ»í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í†¤ìœ¼ë¡œ ëŒ€í™”í•´ì£¼ì„¸ìš”"""
    else:
        # ì „ë¬¸ì ì¸ ì§ˆë¬¸ì´ë‚˜ ë¶„ì„ ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸
        base_prompt = """ë‹¹ì‹ ì€ Retain Sentinel 360 AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
HR ë°ì´í„° ë¶„ì„ê³¼ ì´ì§ ì˜ˆì¸¡ ë¶„ì„ì„ ë„ì™€ë“œë¦¬ëŠ” ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì‘ë‹µ ê°€ì´ë“œë¼ì¸:
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§ê²Œ ì „ë¬¸ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”
- êµ¬ì²´ì ì¸ ë°ì´í„°ì™€ ìˆ˜ì¹˜ë¥¼ í™œìš©í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”
- ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì¹˜ ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”
- ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ë˜, ì „ë¬¸ì„±ì„ ìœ ì§€í•´ì£¼ì„¸ìš”

ì£¼ìš” ê¸°ëŠ¥:
1. ì´ì§ ìœ„í—˜ë„ ë¶„ì„ ê²°ê³¼ í•´ì„ ë° ì¡°ì–¸
2. HR ê´€ë¦¬ ê´€ë ¨ ì‹¤ìš©ì ì¸ ê°€ì´ë“œ ì œê³µ
3. ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ë° ê¶Œì¥ì‚¬í•­ ì œì‹œ
4. êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ë°©ì•ˆ ì œì•ˆ"""

    # ë¶„ì„ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    if context:
        context_info = "\n\ní˜„ì¬ ë¶„ì„ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸:\n"
        
        if 'totalEmployees' in context:
            context_info += f"- ì „ì²´ ì§ì› ìˆ˜: {context['totalEmployees']}ëª…\n"
        
        if 'highRiskCount' in context:
            context_info += f"- ê³ ìœ„í—˜êµ°: {context['highRiskCount']}ëª…\n"
        
        if 'mediumRiskCount' in context:
            context_info += f"- ì¤‘ìœ„í—˜êµ°: {context['mediumRiskCount']}ëª…\n"
        
        if 'lowRiskCount' in context:
            context_info += f"- ì €ìœ„í—˜êµ°: {context['lowRiskCount']}ëª…\n"
        
        if 'accuracy' in context:
            context_info += f"- ëª¨ë¸ ì •í™•ë„: {context['accuracy']}%\n"
        
        if 'departmentStats' in context:
            context_info += "- ë¶€ì„œë³„ í˜„í™© ë°ì´í„° ë³´ìœ \n"
        
        if 'keyInsights' in context:
            context_info += f"- ì£¼ìš” ì¸ì‚¬ì´íŠ¸: {len(context['keyInsights'])}ê°œ\n"
        
        base_prompt += context_info
    
    return base_prompt


# ì—ëŸ¬ í•¸ë“¤ëŸ¬
# ------------------------------------------------------
# íŒŒì¼ ì—…ë¡œë“œ ë° ê´€ë¦¬ ê¸°ëŠ¥
# ------------------------------------------------------

# ì—ì´ì „íŠ¸ë³„ íŒŒì¼ ì—…ë¡œë“œ ê¸°ëŠ¥ (ìƒˆë¡œìš´ ì²´ê³„ì  ê´€ë¦¬)
@app.route('/api/upload/agent', methods=['POST'])
def upload_agent_file():
    """ì—ì´ì „íŠ¸ë³„ íŒŒì¼ ì—…ë¡œë“œ (batch/post ë¶„ì„ìš©)"""
    try:
        # íŒŒì¼ í™•ì¸
        if 'file' not in request.files:
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        # íŒŒë¼ë¯¸í„° ê²€ì¦
        agent_type = request.form.get('agent_type')  # structura, chronos, sentio, agora
        analysis_type = request.form.get('analysis_type')  # batch, post
        
        if not agent_type or not analysis_type:
            return jsonify({
                "success": False,
                "error": "agent_typeê³¼ analysis_typeì´ í•„ìš”í•©ë‹ˆë‹¤."
            }), 400
        
        if agent_type not in ['structura', 'chronos', 'sentio', 'agora']:
            return jsonify({
                "success": False,
                "error": "ìœ íš¨í•˜ì§€ ì•Šì€ agent_typeì…ë‹ˆë‹¤. (structura, chronos, sentio, agora)"
            }), 400
        
        if analysis_type not in ['batch', 'post']:
            return jsonify({
                "success": False,
                "error": "ìœ íš¨í•˜ì§€ ì•Šì€ analysis_typeì…ë‹ˆë‹¤. (batch, post)"
            }), 400
        
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        allowed_extensions = ['.csv', '.json']
        file_ext = os.path.splitext(file.filename)[1].lower()
        if file_ext not in allowed_extensions:
            return jsonify({
                "success": False,
                "error": "CSV ë˜ëŠ” JSON íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            }), 400
        
        # íŒŒì¼ ì €ì¥ ê²½ë¡œ ìƒì„±
        filename = secure_filename(file.filename)
        upload_dir = os.path.join(os.path.dirname(__file__), '..', 'uploads', agent_type, analysis_type)
        os.makedirs(upload_dir, exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•œ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = os.path.splitext(filename)[0]
        new_filename = f"{base_name}_{timestamp}{file_ext}"
        file_path = os.path.join(upload_dir, new_filename)
        
        # íŒŒì¼ ì €ì¥
        file.save(file_path)
        
        # íŒŒì¼ ì •ë³´ ê²€ì¦ ë° ë©”íƒ€ë°ì´í„° ìƒì„±
        try:
            file_info = {
                "original_filename": filename,
                "saved_filename": new_filename,
                "agent_type": agent_type,
                "analysis_type": analysis_type,
                "file_path": file_path,
                "relative_path": os.path.join('uploads', agent_type, analysis_type, new_filename),
                "upload_time": datetime.now().isoformat(),
                "file_size": os.path.getsize(file_path),
                "file_extension": file_ext
            }
            
            # CSV íŒŒì¼ì¸ ê²½ìš° ì¶”ê°€ ì •ë³´
            if file_ext == '.csv':
                df = pd.read_csv(file_path)
                file_info.update({
                    "rows": len(df),
                    "columns": len(df.columns),
                    "column_names": df.columns.tolist()
                })
                
                # Structuraì˜ ê²½ìš° Attrition ì»¬ëŸ¼ í™•ì¸
                if agent_type == 'structura' and 'Attrition' not in df.columns:
                    logger.warning(f"Structura íŒŒì¼ì— Attrition ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {new_filename}")
                    file_info["warning"] = "Attrition ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì˜ˆì¸¡ ì„±ëŠ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            
            # JSON íŒŒì¼ì¸ ê²½ìš°
            elif file_ext == '.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    json_data = json.load(f)
                file_info.update({
                    "json_keys": list(json_data.keys()) if isinstance(json_data, dict) else None,
                    "json_type": type(json_data).__name__
                })
            
        except Exception as e:
            # íŒŒì¼ ì‚­ì œ
            if os.path.exists(file_path):
                os.remove(file_path)
            return jsonify({
                "success": False,
                "error": f"íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {str(e)}"
            }), 400
        
        # ë©”íƒ€ë°ì´í„° íŒŒì¼ ì €ì¥
        metadata_path = os.path.join(upload_dir, f"{base_name}_{timestamp}.json")
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(file_info, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Agent file uploaded successfully: {agent_type}/{analysis_type}/{new_filename}")
        
        return jsonify({
            "success": True,
            "message": f"{agent_type} {analysis_type} íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "file_info": file_info
        })
        
    except Exception as e:
        logger.error(f"Agent file upload error: {e}")
        return jsonify({
            "success": False,
            "error": f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        }), 500


@app.route('/api/files/<agent_type>/<analysis_type>', methods=['GET'])
def list_agent_files(agent_type, analysis_type):
    """íŠ¹ì • ì—ì´ì „íŠ¸/ë¶„ì„ íƒ€ì…ì˜ íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
    try:
        if agent_type not in ['structura', 'chronos', 'sentio', 'agora']:
            return jsonify({
                "success": False,
                "error": "ìœ íš¨í•˜ì§€ ì•Šì€ agent_typeì…ë‹ˆë‹¤."
            }), 400
        
        if analysis_type not in ['batch', 'post']:
            return jsonify({
                "success": False,
                "error": "ìœ íš¨í•˜ì§€ ì•Šì€ analysis_typeì…ë‹ˆë‹¤."
            }), 400
        
        upload_dir = os.path.join(os.path.dirname(__file__), '..', 'uploads', agent_type, analysis_type)
        
        if not os.path.exists(upload_dir):
            return jsonify({
                "success": True,
                "files": [],
                "message": f"{agent_type}/{analysis_type} ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤."
            })
        
        files = []
        for filename in os.listdir(upload_dir):
            if filename.endswith('.json'):  # ë©”íƒ€ë°ì´í„° íŒŒì¼
                metadata_path = os.path.join(upload_dir, filename)
                try:
                    with open(metadata_path, 'r', encoding='utf-8') as f:
                        file_info = json.load(f)
                    files.append(file_info)
                except Exception as e:
                    logger.warning(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {filename}, {str(e)}")
        
        # ì—…ë¡œë“œ ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        files.sort(key=lambda x: x.get('upload_time', ''), reverse=True)
        
        return jsonify({
            "success": True,
            "files": files,
            "agent_type": agent_type,
            "analysis_type": analysis_type,
            "total_files": len(files)
        })
        
    except Exception as e:
        logger.error(f"List agent files error: {e}")
        return jsonify({
            "success": False,
            "error": f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        }), 500


@app.route('/api/files/<agent_type>/<analysis_type>/<filename>', methods=['DELETE'])
def delete_agent_file(agent_type, analysis_type, filename):
    """ì—ì´ì „íŠ¸ íŒŒì¼ ì‚­ì œ"""
    try:
        if agent_type not in ['structura', 'chronos', 'sentio', 'agora']:
            return jsonify({
                "success": False,
                "error": "ìœ íš¨í•˜ì§€ ì•Šì€ agent_typeì…ë‹ˆë‹¤."
            }), 400
        
        if analysis_type not in ['batch', 'post']:
            return jsonify({
                "success": False,
                "error": "ìœ íš¨í•˜ì§€ ì•Šì€ analysis_typeì…ë‹ˆë‹¤."
            }), 400
        
        upload_dir = os.path.join(os.path.dirname(__file__), '..', 'uploads', agent_type, analysis_type)
        file_path = os.path.join(upload_dir, filename)
        
        # ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        base_name = os.path.splitext(filename)[0]
        metadata_path = os.path.join(upload_dir, f"{base_name}.json")
        
        deleted_files = []
        
        # ì‹¤ì œ íŒŒì¼ ì‚­ì œ
        if os.path.exists(file_path):
            os.remove(file_path)
            deleted_files.append(filename)
        
        # ë©”íƒ€ë°ì´í„° íŒŒì¼ ì‚­ì œ
        if os.path.exists(metadata_path):
            os.remove(metadata_path)
            deleted_files.append(f"{base_name}.json")
        
        if not deleted_files:
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }), 404
        
        logger.info(f"Agent files deleted: {agent_type}/{analysis_type}/{deleted_files}")
        
        return jsonify({
            "success": True,
            "message": f"íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "deleted_files": deleted_files
        })
        
    except Exception as e:
        logger.error(f"Delete agent file error: {e}")
        return jsonify({
            "success": False,
            "error": f"íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        }), 500

@app.route('/upload_file', methods=['POST'])
def upload_file():
    """íŒŒì¼ ì—…ë¡œë“œ"""
    try:
        # íŒŒì¼ í™•ì¸
        if 'file' not in request.files:
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        if not file.filename.lower().endswith('.csv'):
            return jsonify({
                "success": False,
                "error": "CSV íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            }), 400
        
        # íŒŒì¼ ì €ì¥ - ì—ì´ì „íŠ¸ë³„/ë¶„ì„íƒ€ì…ë³„ í´ë” ì§€ì›
        filename = secure_filename(file.filename)
        
        # ì—ì´ì „íŠ¸ íƒ€ì…ê³¼ ë¶„ì„ íƒ€ì… í™•ì¸ (í¼ ë°ì´í„°ì—ì„œ)
        agent_type = request.form.get('agent_type', 'supervisor')
        analysis_type = request.form.get('analysis_type', 'general')
        
        # ì—ì´ì „íŠ¸ë³„ í´ë” êµ¬ì¡° ìƒì„±
        if agent_type in ['structura', 'chronos', 'sentio', 'agora'] and analysis_type in ['batch', 'post']:
            upload_dir = os.path.join(os.path.dirname(__file__), '..', 'uploads', agent_type, analysis_type)
        else:
            upload_dir = os.path.join(os.path.dirname(__file__), '..', 'uploads', 'supervisor')
        
        os.makedirs(upload_dir, exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•œ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = os.path.splitext(filename)[0]
        new_filename = f"{base_name}_{timestamp}.csv"
        file_path = os.path.join(upload_dir, new_filename)
        
        file.save(file_path)
        
        # íŒŒì¼ ì •ë³´ ê²€ì¦
        try:
            df = pd.read_csv(file_path)
            file_info = {
                "original_filename": filename,
                "saved_filename": new_filename,
                "file_path": file_path,
                "relative_path": os.path.join('uploads', agent_type, analysis_type, new_filename) if agent_type in ['structura', 'chronos', 'sentio', 'agora'] else os.path.join('uploads', 'supervisor', new_filename),
                "agent_type": agent_type,
                "analysis_type": analysis_type,
                "upload_time": datetime.now().isoformat(),
                "file_size": os.path.getsize(file_path),
                "rows": len(df),
                "columns": len(df.columns),
                "column_names": df.columns.tolist()
            }
        except Exception as e:
            # íŒŒì¼ ì‚­ì œ
            if os.path.exists(file_path):
                os.remove(file_path)
            return jsonify({
                "success": False,
                "error": f"CSV íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {str(e)}"
            }), 400
        
        logger.info(f"File uploaded successfully: {new_filename}")
        
        return jsonify({
            "success": True,
            "message": "íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "file_info": file_info
        })
        
    except Exception as e:
        logger.error(f"File upload error: {e}")
        return jsonify({
            "success": False,
            "error": f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        }), 500


@app.route('/list_uploaded_files', methods=['GET'])
def list_uploaded_files():
    """ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
    try:
        upload_dir = os.path.join(os.path.dirname(__file__), '..', 'uploads', 'supervisor')
        
        if not os.path.exists(upload_dir):
            return jsonify({
                "success": True,
                "files": [],
                "message": "ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
            })
        
        files = []
        for filename in os.listdir(upload_dir):
            if filename.endswith('.csv'):
                file_path = os.path.join(upload_dir, filename)
                stat = os.stat(file_path)
                
                # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
                try:
                    df = pd.read_csv(file_path)
                    file_info = {
                        "filename": filename,
                        "file_path": file_path,
                        "size": stat.st_size,
                        "upload_time": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                        "rows": len(df),
                        "columns": len(df.columns),
                        "column_names": df.columns.tolist()[:10]  # ì²˜ìŒ 10ê°œ ì»¬ëŸ¼ë§Œ
                    }
                except Exception as e:
                    file_info = {
                        "filename": filename,
                        "file_path": file_path,
                        "size": stat.st_size,
                        "upload_time": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                        "error": f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}"
                    }
                
                files.append(file_info)
        
        # ì—…ë¡œë“œ ì‹œê°„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        files.sort(key=lambda x: x.get('upload_time', ''), reverse=True)
        
        return jsonify({
            "success": True,
            "files": files,
            "total_files": len(files)
        })
        
    except Exception as e:
        logger.error(f"List files error: {e}")
        return jsonify({
            "success": False,
            "error": f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        }), 500


@app.route('/download_file/<filename>', methods=['GET'])
def download_file(filename):
    """íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    try:
        # íŒŒì¼ëª… ë³´ì•ˆ ê²€ì¦
        filename = secure_filename(filename)
        upload_dir = os.path.join(os.path.dirname(__file__), '..', 'uploads', 'supervisor')
        file_path = os.path.join(upload_dir, filename)
        
        if not os.path.exists(file_path):
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }), 404
        
        return send_file(file_path, as_attachment=True, download_name=filename)
        
    except Exception as e:
        logger.error(f"File download error: {e}")
        return jsonify({
            "success": False,
            "error": f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        }), 500


@app.route('/delete_file/<filename>', methods=['DELETE'])
def delete_file(filename):
    """íŒŒì¼ ì‚­ì œ"""
    try:
        # íŒŒì¼ëª… ë³´ì•ˆ ê²€ì¦
        filename = secure_filename(filename)
        upload_dir = os.path.join(os.path.dirname(__file__), '..', 'uploads', 'supervisor')
        file_path = os.path.join(upload_dir, filename)
        
        if not os.path.exists(file_path):
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }), 404
        
        os.remove(file_path)
        logger.info(f"File deleted successfully: {filename}")
        
        return jsonify({
            "success": True,
            "message": f"íŒŒì¼ '{filename}'ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
        })
        
    except Exception as e:
        logger.error(f"File delete error: {e}")
        return jsonify({
            "success": False,
            "error": f"íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        }), 500


@app.route('/get_file_info/<filename>', methods=['GET'])
def get_file_info(filename):
    """íŒŒì¼ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
    try:
        # íŒŒì¼ëª… ë³´ì•ˆ ê²€ì¦
        filename = secure_filename(filename)
        upload_dir = os.path.join(os.path.dirname(__file__), '..', 'uploads', 'supervisor')
        file_path = os.path.join(upload_dir, filename)
        
        if not os.path.exists(file_path):
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }), 404
        
        # íŒŒì¼ ê¸°ë³¸ ì •ë³´
        stat = os.stat(file_path)
        file_info = {
            "filename": filename,
            "file_path": file_path,
            "size": stat.st_size,
            "upload_time": datetime.fromtimestamp(stat.st_mtime).isoformat()
        }
        
        # CSV íŒŒì¼ ìƒì„¸ ì •ë³´
        try:
            df = pd.read_csv(file_path)
            file_info.update({
                "rows": len(df),
                "columns": len(df.columns),
                "column_names": df.columns.tolist(),
                "data_types": df.dtypes.astype(str).to_dict(),
                "sample_data": df.head(5).to_dict('records'),
                "missing_values": df.isnull().sum().to_dict()
            })
        except Exception as e:
            file_info["csv_error"] = f"CSV íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜: {str(e)}"
        
        return jsonify({
            "success": True,
            "file_info": file_info
        })
        
    except Exception as e:
        logger.error(f"Get file info error: {e}")
        return jsonify({
            "success": False,
            "error": f"íŒŒì¼ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        }), 500


# ------------------------------------------------------
# ì›Œì»¤ ì—ì´ì „íŠ¸ API í†µí•© ì—”ë“œí¬ì¸íŠ¸
# ------------------------------------------------------

@app.route('/api/workers/health_check_all', methods=['GET'])
def check_all_workers_health():
    """ëª¨ë“  ì›Œì»¤ ì—ì´ì „íŠ¸ì˜ ìƒíƒœë¥¼ í™•ì¸"""
    try:
        if not supervisor_workflow:
            return jsonify({
                'success': False,
                'error': 'Supervisor workflow not initialized'
            }), 500
        
        # ì›Œì»¤ ìƒíƒœ í™•ì¸
        health_status = asyncio.run(supervisor_workflow.worker_integrator.health_check_all())
        
        return jsonify({
            'success': True,
            'worker_status': {
                worker.value: {
                    'healthy': is_healthy,
                    'status': 'online' if is_healthy else 'offline'
                }
                for worker, is_healthy in health_status.items()
            },
            'summary': {
                'total_workers': len(health_status),
                'healthy_workers': sum(health_status.values()),
                'health_rate': sum(health_status.values()) / len(health_status)
            },
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Worker health check error: {e}")
        return jsonify({
            'success': False,
            'error': f'Health check failed: {str(e)}'
        }), 500


@app.route('/api/workers/structura/predict', methods=['POST'])
def structura_predict():
    """Structura ì˜ˆì¸¡ API í”„ë¡ì‹œ"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'No data provided'
            }), 400
        
        # Structura API í˜¸ì¶œ
        import requests
        response = requests.post(
            'http://localhost:5001/api/predict',
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'structura'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Structura API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Structura predict error: {e}")
        return jsonify({
            'success': False,
            'error': f'Structura prediction failed: {str(e)}'
        }), 500


@app.route('/api/workers/cognita/analyze/<employee_id>', methods=['GET'])
def cognita_analyze_employee(employee_id):
    """Cognita ì§ì› ë¶„ì„ API í”„ë¡ì‹œ"""
    try:
        # Cognita API í˜¸ì¶œ
        import requests
        response = requests.get(
            f'http://localhost:5002/api/analyze/employee/{employee_id}',
            timeout=30
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'cognita',
                'employee_id': employee_id
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Cognita API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Cognita analyze error: {e}")
        return jsonify({
            'success': False,
            'error': f'Cognita analysis failed: {str(e)}'
        }), 500


@app.route('/api/workers/cognita/departments', methods=['GET'])
def cognita_get_departments():
    """Cognita ë¶€ì„œ ëª©ë¡ API í”„ë¡ì‹œ"""
    try:
        # Cognita API í˜¸ì¶œ
        import requests
        response = requests.get(
            'http://localhost:5002/api/departments',
            timeout=30
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'cognita'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Cognita API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Cognita departments error: {e}")
        return jsonify({
            'success': False,
            'error': f'Cognita departments failed: {str(e)}'
        }), 500


@app.route('/api/workers/cognita/employees', methods=['GET'])
def cognita_get_employees():
    """Cognita ì§ì› ëª©ë¡ API í”„ë¡ì‹œ"""
    try:
        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì „ë‹¬
        limit = request.args.get('limit', '10')
        offset = request.args.get('offset', '0')
        
        # Cognita API í˜¸ì¶œ
        import requests
        response = requests.get(
            f'http://localhost:5002/api/employees?limit={limit}&offset={offset}',
            timeout=30
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'cognita'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Cognita API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Cognita employees error: {e}")
        return jsonify({
            'success': False,
            'error': f'Cognita employees failed: {str(e)}'
        }), 500


@app.route('/api/workers/cognita/analyze/department', methods=['POST'])
def cognita_analyze_department():
    """Cognita ë¶€ì„œ ë¶„ì„ API í”„ë¡ì‹œ"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'No data provided'
            }), 400
        
        # Cognita API í˜¸ì¶œ
        import requests
        response = requests.post(
            'http://localhost:5002/api/analyze/department',
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'cognita'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Cognita API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Cognita department analysis error: {e}")
        return jsonify({
            'success': False,
            'error': f'Cognita department analysis failed: {str(e)}'
        }), 500


@app.route('/api/cognita/setup/neo4j', methods=['POST'])
def cognita_setup_neo4j():
    """Cognita Neo4j ì„¤ì • API í”„ë¡ì‹œ"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'No data provided'
            }), 400
        
        # Cognita API í˜¸ì¶œ
        import requests
        response = requests.post(
            'http://localhost:5002/api/setup/neo4j',
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'cognita'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Cognita API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Cognita Neo4j setup error: {e}")
        return jsonify({
            'success': False,
            'error': f'Cognita Neo4j setup failed: {str(e)}'
        }), 500


@app.route('/api/workers/chronos/predict', methods=['POST'])
def chronos_predict():
    """Chronos ì˜ˆì¸¡ API í”„ë¡ì‹œ"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'No data provided'
            }), 400
        
        # Chronos API í˜¸ì¶œ
        import requests
        response = requests.post(
            'http://localhost:5003/api/predict',
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'chronos'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Chronos API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Chronos predict error: {e}")
        return jsonify({
            'success': False,
            'error': f'Chronos prediction failed: {str(e)}'
        }), 500


@app.route('/api/workers/sentio/analyze_sentiment', methods=['POST'])
def sentio_analyze_sentiment():
    """Sentio ê°ì • ë¶„ì„ API í”„ë¡ì‹œ"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'No data provided'
            }), 400
        
        # Sentio API í˜¸ì¶œ
        import requests
        response = requests.post(
            'http://localhost:5004/analyze_sentiment',
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'sentio'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Sentio API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Sentio analyze error: {e}")
        return jsonify({
            'success': False,
            'error': f'Sentio analysis failed: {str(e)}'
        }), 500


@app.route('/api/workers/agora/comprehensive_analysis', methods=['POST'])
def agora_comprehensive_analysis():
    """Agora ì¢…í•© ì‹œì¥ ë¶„ì„ API í”„ë¡ì‹œ"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'No data provided'
            }), 400
        
        # Agora API í˜¸ì¶œ
        import requests
        response = requests.post(
            'http://localhost:5005/api/agora/comprehensive-analysis',
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'agora'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Agora API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Agora analysis error: {e}")
        return jsonify({
            'success': False,
            'error': f'Agora analysis failed: {str(e)}'
        }), 500


@app.route('/api/workers/integration/predict_employee', methods=['POST'])
def integration_predict_employee():
    """Integration ê°œë³„ ì§ì› ì˜ˆì¸¡ API í”„ë¡ì‹œ"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'No data provided'
            }), 400
        
        # Integration API í˜¸ì¶œ
        import requests
        response = requests.post(
            'http://localhost:5007/predict_employee',
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'integration'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Integration API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Integration predict error: {e}")
        return jsonify({
            'success': False,
            'error': f'Integration prediction failed: {str(e)}'
        }), 500


@app.route('/api/workers/integration/generate_report', methods=['POST'])
def integration_generate_report():
    """Integration ë ˆí¬íŠ¸ ìƒì„± API í”„ë¡ì‹œ"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'No data provided'
            }), 400
        
        # Integration API í˜¸ì¶œ
        import requests
        response = requests.post(
            'http://localhost:5007/generate_report',
            json=data,
            timeout=60  # ë ˆí¬íŠ¸ ìƒì„±ì€ ì‹œê°„ì´ ë” ê±¸ë¦´ ìˆ˜ ìˆìŒ
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'integration'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Integration API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Integration report error: {e}")
        return jsonify({
            'success': False,
            'error': f'Integration report generation failed: {str(e)}'
        }), 500


@app.route('/api/workers/integration/generate_batch_analysis_report', methods=['POST'])
def integration_generate_batch_analysis_report():
    """Integration ë°°ì¹˜ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± API í”„ë¡ì‹œ"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'No data provided'
            }), 400
        
        # Integration API í˜¸ì¶œ
        import requests
        response = requests.post(
            'http://localhost:5007/generate_batch_analysis_report',
            json=data,
            timeout=120  # ë°°ì¹˜ ë³´ê³ ì„œ ìƒì„±ì€ ì‹œê°„ì´ ë” ê±¸ë¦´ ìˆ˜ ìˆìŒ
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'integration'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Integration API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Integration batch analysis report error: {e}")
        return jsonify({
            'success': False,
            'error': f'Integration batch analysis report generation failed: {str(e)}'
        }), 500


@app.route('/api/workers/integration/load_data', methods=['POST'])
def integration_load_data():
    """Integration ë°ì´í„° ë¡œë“œ API í”„ë¡ì‹œ"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'No data provided'
            }), 400
        
        # Integration API í˜¸ì¶œ
        import requests
        response = requests.post(
            'http://localhost:5007/load_data',
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'integration'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Integration API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Integration load data error: {e}")
        return jsonify({
            'success': False,
            'error': f'Integration data loading failed: {str(e)}'
        }), 500


@app.route('/api/workers/integration/calculate_thresholds', methods=['POST'])
def integration_calculate_thresholds():
    """Integration ì„ê³„ê°’ ê³„ì‚° API í”„ë¡ì‹œ"""
    try:
        data = request.get_json() or {}
        
        # Integration API í˜¸ì¶œ
        import requests
        response = requests.post(
            'http://localhost:5007/calculate_thresholds',
            json=data,
            timeout=60  # ì„ê³„ê°’ ê³„ì‚°ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŒ
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'integration'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Integration API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Integration calculate thresholds error: {e}")
        return jsonify({
            'success': False,
            'error': f'Integration threshold calculation failed: {str(e)}'
        }), 500


@app.route('/api/workers/integration/optimize_weights', methods=['POST'])
def integration_optimize_weights():
    """Integration ê°€ì¤‘ì¹˜ ìµœì í™” API í”„ë¡ì‹œ"""
    try:
        data = request.get_json() or {}
        
        # Integration API í˜¸ì¶œ
        import requests
        response = requests.post(
            'http://localhost:5007/optimize_weights',
            json=data,
            timeout=120  # ê°€ì¤‘ì¹˜ ìµœì í™”ëŠ” ì‹œê°„ì´ ë§ì´ ê±¸ë¦´ ìˆ˜ ìˆìŒ
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'integration'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Integration API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Integration optimize weights error: {e}")
        return jsonify({
            'success': False,
            'error': f'Integration weight optimization failed: {str(e)}'
        }), 500


@app.route('/api/workers/integration/get_results', methods=['GET'])
def integration_get_results():
    """Integration ê²°ê³¼ ì¡°íšŒ API í”„ë¡ì‹œ"""
    try:
        # Integration API í˜¸ì¶œ
        import requests
        response = requests.get(
            'http://localhost:5007/get_results',
            timeout=30
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'integration'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Integration API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Integration get results error: {e}")
        return jsonify({
            'success': False,
            'error': f'Integration results retrieval failed: {str(e)}'
        }), 500


@app.route('/api/workers/integration/compare_methods', methods=['POST'])
def integration_compare_methods():
    """Integration ìµœì í™” ë°©ë²• ë¹„êµ API í”„ë¡ì‹œ"""
    try:
        data = request.get_json() or {}
        
        # Integration API í˜¸ì¶œ
        import requests
        response = requests.post(
            'http://localhost:5007/compare_methods',
            json=data,
            timeout=180  # ë°©ë²• ë¹„êµëŠ” ì‹œê°„ì´ ë§ì´ ê±¸ë¦´ ìˆ˜ ìˆìŒ
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'integration'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Integration API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Integration compare methods error: {e}")
        return jsonify({
            'success': False,
            'error': f'Integration method comparison failed: {str(e)}'
        }), 500


@app.route('/api/workers/integration/generate_batch_reports', methods=['POST'])
def integration_generate_batch_reports():
    """Integration ì¼ê´„ ë ˆí¬íŠ¸ ìƒì„± API í”„ë¡ì‹œ"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'No data provided'
            }), 400
        
        # Integration API í˜¸ì¶œ
        import requests
        response = requests.post(
            'http://localhost:5007/generate_batch_reports',
            json=data,
            timeout=300  # ì¼ê´„ ë ˆí¬íŠ¸ ìƒì„±ì€ ì‹œê°„ì´ ë§ì´ ê±¸ë¦´ ìˆ˜ ìˆìŒ
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'integration'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Integration API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Integration batch reports error: {e}")
        return jsonify({
            'success': False,
            'error': f'Integration batch report generation failed: {str(e)}'
        }), 500


@app.route('/api/workers/integration/upload_employee_data', methods=['POST'])
def integration_upload_employee_data():
    """Integration ì§ì› ë°ì´í„° ì—…ë¡œë“œ API í”„ë¡ì‹œ"""
    try:
        # íŒŒì¼ í™•ì¸
        if 'file' not in request.files:
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        # Integration APIì— íŒŒì¼ ì „ì†¡
        import requests
        files = {'file': (file.filename, file.read(), 'text/csv')}
        response = requests.post(
            'http://localhost:5007/upload/employee_data',
            files=files,
            timeout=60
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'integration'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Integration API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Integration upload employee data error: {e}")
        return jsonify({
            'success': False,
            'error': f'Integration employee data upload failed: {str(e)}'
        }), 500


@app.route('/api/workers/integration/get_employee_list', methods=['GET'])
def integration_get_employee_list():
    """Integration ì§ì› ëª©ë¡ ì¡°íšŒ API í”„ë¡ì‹œ"""
    try:
        # Integration API í˜¸ì¶œ
        import requests
        response = requests.get(
            'http://localhost:5007/get_employee_list',
            timeout=30
        )
        
        if response.status_code == 200:
            return jsonify({
                'success': True,
                'data': response.json(),
                'source': 'integration'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Integration API error: {response.status_code}',
                'details': response.text
            }), response.status_code
            
    except Exception as e:
        logger.error(f"Integration get employee list error: {e}")
        return jsonify({
            'success': False,
            'error': f'Integration employee list retrieval failed: {str(e)}'
        }), 500


@app.route('/api/workers/upload_data', methods=['POST'])
def upload_data_to_workers():
    """ëª¨ë“  ì›Œì»¤ ì—ì´ì „íŠ¸ì— ë°ì´í„° ì—…ë¡œë“œ"""
    try:
        # íŒŒì¼ í™•ì¸
        if 'file' not in request.files:
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        if not file.filename.lower().endswith('.csv'):
            return jsonify({
                "success": False,
                "error": "CSV íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            }), 400
        
        upload_results = {}
        
        # ê° ì›Œì»¤ì— íŒŒì¼ ì—…ë¡œë“œ ì‹œë„
        worker_endpoints = {
            'structura': 'http://localhost:5001/api/upload/data',
            'chronos': 'http://localhost:5003/api/upload/timeseries',
            'sentio': 'http://localhost:5004/upload/text_data'
        }
        
        import requests
        
        for worker_name, endpoint in worker_endpoints.items():
            try:
                # íŒŒì¼ í¬ì¸í„°ë¥¼ ì²˜ìŒìœ¼ë¡œ ë˜ëŒë¦¬ê¸°
                file.seek(0)
                
                files = {'file': (file.filename, file.read(), 'text/csv')}
                response = requests.post(endpoint, files=files, timeout=30)
                
                upload_results[worker_name] = {
                    'success': response.status_code == 200,
                    'status_code': response.status_code,
                    'response': response.json() if response.status_code == 200 else response.text
                }
                
            except Exception as e:
                upload_results[worker_name] = {
                    'success': False,
                    'error': str(e)
                }
        
        # ì„±ê³µí•œ ì—…ë¡œë“œ ìˆ˜ ê³„ì‚°
        successful_uploads = sum(1 for result in upload_results.values() if result['success'])
        
        return jsonify({
            'success': successful_uploads > 0,
            'message': f'{successful_uploads}/{len(worker_endpoints)} ì›Œì»¤ì— ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'upload_results': upload_results,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Worker upload error: {e}")
        return jsonify({
            'success': False,
            'error': f'Worker upload failed: {str(e)}'
        }), 500


# ------------------------------------------------------
# ë°°ì¹˜ ì²˜ë¦¬ ê¸°ëŠ¥
# ------------------------------------------------------

@app.route('/batch_process', methods=['POST'])
def batch_process():
    """ëŒ€ëŸ‰ ì§ì› ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'No data provided'
            }), 400
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì˜µì…˜
        employees_data = data.get('employees', [])
        process_options = data.get('options', {})
        
        if not employees_data:
            return jsonify({
                'success': False,
                'error': 'No employee data provided'
            }), 400
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì„¸ì…˜ ID ìƒì„±
        import uuid
        batch_id = str(uuid.uuid4())
        
        # ë°°ì¹˜ ì²˜ë¦¬ ìƒíƒœ ì´ˆê¸°í™”
        batch_status = {
            'batch_id': batch_id,
            'total_employees': len(employees_data),
            'processed_employees': 0,
            'status': 'processing',
            'start_time': datetime.now().isoformat(),
            'results': [],
            'errors': []
        }
        
        # ì„¸ì…˜ì— ë°°ì¹˜ ìƒíƒœ ì €ì¥ (ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ì €ì¥)
        if not hasattr(app, 'batch_sessions'):
            app.batch_sessions = {}
        app.batch_sessions[batch_id] = batch_status
        
        # ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘
        import threading
        thread = threading.Thread(
            target=process_batch_async, 
            args=(batch_id, employees_data, process_options)
        )
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'success': True,
            'batch_id': batch_id,
            'message': f'ë°°ì¹˜ ì²˜ë¦¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ {len(employees_data)}ëª…ì˜ ì§ì› ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.',
            'status_url': f'/batch_status/{batch_id}'
        })
        
    except Exception as e:
        logger.error(f"Batch process error: {e}")
        return jsonify({
            'success': False,
            'error': f'Batch processing failed: {str(e)}'
        }), 500


@app.route('/batch_status/<batch_id>', methods=['GET'])
def get_batch_status(batch_id):
    """ë°°ì¹˜ ì²˜ë¦¬ ìƒíƒœ ì¡°íšŒ"""
    try:
        if not hasattr(app, 'batch_sessions') or batch_id not in app.batch_sessions:
            return jsonify({
                'success': False,
                'error': 'Batch session not found'
            }), 404
        
        batch_status = app.batch_sessions[batch_id]
        
        # ì§„í–‰ë¥  ê³„ì‚°
        progress = (batch_status['processed_employees'] / batch_status['total_employees']) * 100
        
        return jsonify({
            'success': True,
            'batch_id': batch_id,
            'status': batch_status['status'],
            'progress': round(progress, 2),
            'total_employees': batch_status['total_employees'],
            'processed_employees': batch_status['processed_employees'],
            'start_time': batch_status['start_time'],
            'results_count': len(batch_status['results']),
            'errors_count': len(batch_status['errors']),
            'completed': batch_status['status'] in ['completed', 'failed']
        })
        
    except Exception as e:
        logger.error(f"Batch status error: {e}")
        return jsonify({
            'success': False,
            'error': f'Failed to get batch status: {str(e)}'
        }), 500


@app.route('/batch_upload_csv', methods=['POST'])
def batch_upload_csv():
    """CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë°°ì¹˜ ì²˜ë¦¬"""
    try:
        # íŒŒì¼ í™•ì¸
        if 'file' not in request.files:
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        if not file.filename.lower().endswith('.csv'):
            return jsonify({
                "success": False,
                "error": "CSV íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            }), 400
        
        # CSV íŒŒì¼ ì½ê¸°
        import pandas as pd
        import io
        
        # íŒŒì¼ ë‚´ìš©ì„ ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ ì½ê¸°
        file_content = file.read().decode('utf-8')
        csv_data = pd.read_csv(io.StringIO(file_content))
        
        # DataFrameì„ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        employees_data = csv_data.to_dict('records')
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['employee_id']
        missing_columns = [col for col in required_columns if col not in csv_data.columns]
        
        if missing_columns:
            return jsonify({
                "success": False,
                "error": f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing_columns)}",
                "required_columns": required_columns,
                "found_columns": list(csv_data.columns)
            }), 400
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì˜µì…˜
        process_options = {
            'source': 'csv_upload',
            'filename': file.filename,
            'include_reports': request.form.get('include_reports', 'true').lower() == 'true',
            'use_llm': request.form.get('use_llm', 'false').lower() == 'true'
        }
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘
        import uuid
        batch_id = str(uuid.uuid4())
        
        batch_status = {
            'batch_id': batch_id,
            'total_employees': len(employees_data),
            'processed_employees': 0,
            'status': 'processing',
            'start_time': datetime.now().isoformat(),
            'source': 'csv_upload',
            'filename': file.filename,
            'results': [],
            'errors': []
        }
        
        if not hasattr(app, 'batch_sessions'):
            app.batch_sessions = {}
        app.batch_sessions[batch_id] = batch_status
        
        # ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘
        import threading
        thread = threading.Thread(
            target=process_batch_async, 
            args=(batch_id, employees_data, process_options)
        )
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'success': True,
            'batch_id': batch_id,
            'message': f'CSV íŒŒì¼ "{file.filename}"ì—ì„œ {len(employees_data)}ëª…ì˜ ì§ì› ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.',
            'total_employees': len(employees_data),
            'columns': list(csv_data.columns),
            'status_url': f'/batch_status/{batch_id}'
        })
        
    except Exception as e:
        logger.error(f"CSV batch upload error: {e}")
        return jsonify({
            'success': False,
            'error': f'CSV ë°°ì¹˜ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}'
        }), 500


@app.route('/batch_results/<batch_id>', methods=['GET'])
def get_batch_results(batch_id):
    """ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ì¡°íšŒ"""
    try:
        if not hasattr(app, 'batch_sessions') or batch_id not in app.batch_sessions:
            return jsonify({
                'success': False,
                'error': 'Batch session not found'
            }), 404
        
        batch_status = app.batch_sessions[batch_id]
        
        return jsonify({
            'success': True,
            'batch_id': batch_id,
            'status': batch_status['status'],
            'total_employees': batch_status['total_employees'],
            'processed_employees': batch_status['processed_employees'],
            'results': batch_status['results'],
            'errors': batch_status['errors'],
            'start_time': batch_status['start_time'],
            'end_time': batch_status.get('end_time'),
            'export_url': f'/batch_export_csv/{batch_id}'
        })
        
    except Exception as e:
        logger.error(f"Batch results error: {e}")
        return jsonify({
            'success': False,
            'error': f'Failed to get batch results: {str(e)}'
        }), 500


@app.route('/api/statistics/group', methods=['GET'])
def get_group_statistics():
    """ë‹¨ì²´ í†µê³„ ì¡°íšŒ API"""
    try:
        group_by = request.args.get('group_by', 'department')
        department_filter = request.args.get('department')
        
        # ëª¨ë“  ë¶€ì„œì˜ ì¸ë±ìŠ¤ íŒŒì¼ì—ì„œ í†µê³„ ìˆ˜ì§‘
        departments = ['Human_Resources', 'Research_&_Development', 'Sales', 'Manufacturing', 'Information_Technology']
        grouped_stats = {}
        
        for dept in departments:
            try:
                dept_stats = hierarchical_result_manager.get_department_statistics(dept)
                if not dept_stats:
                    continue
                
                # ë¶€ì„œë³„ ì¸ë±ìŠ¤ íŒŒì¼ ì½ê¸°
                dept_clean = hierarchical_result_manager._sanitize_folder_name(dept)
                index_file = hierarchical_result_manager.base_output_dir / dept_clean / "department_index.json"
                
                if index_file.exists():
                    with open(index_file, 'r', encoding='utf-8') as f:
                        index_data = json.load(f)
                    
                    # ê·¸ë£¹í™” ê¸°ì¤€ì— ë”°ë¼ í†µê³„ ìƒì„±
                    if group_by == 'department':
                        if department_filter is None or dept == department_filter:
                            grouped_stats[dept] = {
                                'total_employees': dept_stats.get('total_employees', 0),
                                'high_risk': 0,  # ì‹¤ì œ ìœ„í—˜ë„ ê³„ì‚° í•„ìš”
                                'medium_risk': 0,
                                'low_risk': dept_stats.get('total_employees', 0),
                                'avg_risk_score': 0.3,  # ê¸°ë³¸ê°’
                                'common_risk_factors': {}
                            }
                    
                    elif group_by == 'job_role':
                        job_roles = index_data.get('job_roles', {})
                        for role, employees in job_roles.items():
                            if department_filter is None or dept == department_filter:
                                grouped_stats[role] = {
                                    'total_employees': len(employees),
                                    'high_risk': 0,
                                    'medium_risk': 0,
                                    'low_risk': len(employees),
                                    'avg_risk_score': 0.3,
                                    'common_risk_factors': {}
                                }
                    
                    elif group_by == 'job_level':
                        positions = index_data.get('positions', {})
                        for pos, employees in positions.items():
                            if department_filter is None or dept == department_filter:
                                grouped_stats[pos] = {
                                    'total_employees': len(employees),
                                    'high_risk': 0,
                                    'medium_risk': 0,
                                    'low_risk': len(employees),
                                    'avg_risk_score': 0.3,
                                    'common_risk_factors': {}
                                }
                        
            except Exception as e:
                logger.error(f"ë¶€ì„œ {dept} í†µê³„ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        return jsonify({
            'success': True,
            'group_by': group_by,
            'department_filter': department_filter,
            'statistics': grouped_stats,
            'generated_at': datetime.now().isoformat(),
            'data_source': 'department_index'
        })
        
    except Exception as e:
        logger.error(f"Group statistics error: {e}")
        return jsonify({
            'success': False,
            'error': f'Failed to get group statistics: {str(e)}'
        }), 500


@app.route('/batch_export_csv/<batch_id>', methods=['GET'])
def batch_export_csv(batch_id):
    """ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ë¥¼ CSVë¡œ ë‚´ë³´ë‚´ê¸°"""
    try:
        if not hasattr(app, 'batch_sessions') or batch_id not in app.batch_sessions:
            return jsonify({
                'success': False,
                'error': 'Batch session not found'
            }), 404
        
        batch_status = app.batch_sessions[batch_id]
        
        # ê²°ê³¼ë¥¼ CSV í˜•íƒœë¡œ ë³€í™˜
        import pandas as pd
        import io
        
        csv_rows = []
        
        # ì„±ê³µí•œ ê²°ê³¼ë“¤
        for result in batch_status['results']:
            employee_id = result.get('employee_id')
            analysis = result.get('analysis', {})
            
            row = {
                'employee_id': employee_id,
                'processed_at': result.get('processed_at'),
                'status': 'success'
            }
            
            # ì›Œì»¤ ë¶„ì„ ê²°ê³¼
            worker_analyses = analysis.get('worker_analyses', {})
            for worker, worker_result in worker_analyses.items():
                if 'error' in worker_result:
                    row[f'{worker}_status'] = 'error'
                    row[f'{worker}_error'] = worker_result['error']
                else:
                    row[f'{worker}_status'] = 'success'
                    # ì£¼ìš” ì ìˆ˜ ì¶”ì¶œ
                    if worker == 'structura' and 'probability' in worker_result:
                        row[f'{worker}_score'] = worker_result['probability']
                    elif worker == 'cognita' and 'risk_score' in worker_result:
                        row[f'{worker}_score'] = worker_result['risk_score']
                    elif worker == 'chronos' and 'prediction_score' in worker_result:
                        row[f'{worker}_score'] = worker_result['prediction_score']
                    elif worker == 'sentio':
                        # psychological_risk_scoreë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš© (JD-R ëª¨ë¸ ê¸°ë°˜)
                        if 'psychological_risk_score' in worker_result:
                            row[f'{worker}_score'] = worker_result['psychological_risk_score']
                        elif 'risk_score' in worker_result:
                            row[f'{worker}_score'] = worker_result['risk_score']
                        elif 'sentiment_score' in worker_result:
                            row[f'{worker}_score'] = 1.0 - worker_result['sentiment_score']
                    elif worker == 'agora' and 'overall_risk_score' in worker_result:
                        row[f'{worker}_score'] = worker_result['overall_risk_score']
            
            # Integration ê²°ê³¼
            if 'integration_prediction' in analysis:
                integration_pred = analysis['integration_prediction']
                if 'prediction' in integration_pred:
                    row['integration_prediction'] = integration_pred['prediction']
                if 'probability' in integration_pred:
                    row['integration_probability'] = integration_pred['probability']
            
            csv_rows.append(row)
        
        # ì˜¤ë¥˜ ê²°ê³¼ë“¤
        for error in batch_status['errors']:
            row = {
                'employee_id': error.get('employee_id'),
                'processed_at': error.get('processed_at'),
                'status': 'error',
                'error_message': error.get('error')
            }
            csv_rows.append(row)
        
        if not csv_rows:
            return jsonify({
                'success': False,
                'error': 'No results to export'
            }), 400
        
        # DataFrame ìƒì„± ë° CSV ë³€í™˜
        df = pd.DataFrame(csv_rows)
        
        # CSV ë¬¸ìì—´ ìƒì„±
        output = io.StringIO()
        df.to_csv(output, index=False, encoding='utf-8')
        csv_content = output.getvalue()
        
        # íŒŒì¼ëª… ìƒì„±
        filename = f"batch_results_{batch_id[:8]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        # CSV íŒŒì¼ ì‘ë‹µ
        from flask import make_response
        response = make_response(csv_content)
        response.headers['Content-Type'] = 'text/csv; charset=utf-8'
        response.headers['Content-Disposition'] = f'attachment; filename="{filename}"'
        
        return response
        
    except Exception as e:
        logger.error(f"Batch CSV export error: {e}")
        return jsonify({
            'success': False,
            'error': f'CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {str(e)}'
        }), 500


def process_batch_async(batch_id, employees_data, options):
    """ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜"""
    try:
        batch_status = app.batch_sessions[batch_id]
        
        for i, employee_data in enumerate(employees_data):
            try:
                # ê°œë³„ ì§ì› ë¶„ì„ ìˆ˜í–‰
                employee_id = employee_data.get('employee_id', f'employee_{i+1}')
                
                # ëª¨ë“  ì›Œì»¤ ì—ì´ì „íŠ¸ë¥¼ í†µí•œ ë¶„ì„
                analysis_result = analyze_employee_comprehensive(employee_data)
                
                # ê²°ê³¼ ì €ì¥
                batch_status['results'].append({
                    'employee_id': employee_id,
                    'analysis': analysis_result,
                    'processed_at': datetime.now().isoformat()
                })
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                batch_status['processed_employees'] = i + 1
                
                logger.info(f"Batch {batch_id}: Processed employee {i+1}/{len(employees_data)}")
                
            except Exception as e:
                # ê°œë³„ ì§ì› ì²˜ë¦¬ ì˜¤ë¥˜
                error_info = {
                    'employee_id': employee_data.get('employee_id', f'employee_{i+1}'),
                    'error': str(e),
                    'processed_at': datetime.now().isoformat()
                }
                batch_status['errors'].append(error_info)
                logger.error(f"Batch {batch_id}: Error processing employee {i+1}: {e}")
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ
        batch_status['status'] = 'completed'
        batch_status['end_time'] = datetime.now().isoformat()
        
        logger.info(f"Batch {batch_id}: Processing completed. {len(batch_status['results'])} successful, {len(batch_status['errors'])} errors")
        
    except Exception as e:
        # ì „ì²´ ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜
        batch_status['status'] = 'failed'
        batch_status['end_time'] = datetime.now().isoformat()
        batch_status['errors'].append({
            'error': f'Batch processing failed: {str(e)}',
            'processed_at': datetime.now().isoformat()
        })
        logger.error(f"Batch {batch_id}: Critical error: {e}")


def analyze_employee_comprehensive(employee_data):
    """ê°œë³„ ì§ì›ì— ëŒ€í•œ ì¢…í•© ë¶„ì„"""
    try:
        results = {}
        
        # ê° ì›Œì»¤ ì—ì´ì „íŠ¸ë¥¼ í†µí•œ ë¶„ì„
        worker_results = {}
        
        # Structura ë¶„ì„ (ì •í˜• ë°ì´í„°)
        try:
            structura_result = call_worker_api('structura', 'predict', employee_data)
            worker_results['structura'] = structura_result
        except Exception as e:
            worker_results['structura'] = {'error': str(e)}
        
        # Cognita ë¶„ì„ (ê´€ê³„ ë¶„ì„)
        try:
            employee_id = employee_data.get('employee_id')
            if employee_id:
                cognita_result = call_worker_api('cognita', f'analyze/{employee_id}', {})
                worker_results['cognita'] = cognita_result
        except Exception as e:
            worker_results['cognita'] = {'error': str(e)}
        
        # Chronos ë¶„ì„ (ì‹œê³„ì—´)
        try:
            chronos_result = call_worker_api('chronos', 'predict', employee_data)
            worker_results['chronos'] = chronos_result
        except Exception as e:
            worker_results['chronos'] = {'error': str(e)}
        
        # Sentio ë¶„ì„ (ê°ì • ë¶„ì„)
        try:
            sentio_data = {
                'text': employee_data.get('feedback_text', ''),
                'employee_id': employee_data.get('employee_id')
            }
            sentio_result = call_worker_api('sentio', 'analyze_sentiment', sentio_data)
            worker_results['sentio'] = sentio_result
        except Exception as e:
            worker_results['sentio'] = {'error': str(e)}
        
        # Agora ë¶„ì„ (ì‹œì¥ ë¶„ì„)
        try:
            agora_result = call_worker_api('agora', 'comprehensive_analysis', employee_data)
            worker_results['agora'] = agora_result
        except Exception as e:
            worker_results['agora'] = {'error': str(e)}
        
        results['worker_analyses'] = worker_results
        
        # Integrationì„ í†µí•œ ê²°ê³¼ í†µí•©
        try:
            # ì›Œì»¤ ê²°ê³¼ë¥¼ ì ìˆ˜ë¡œ ë³€í™˜
            agent_scores = extract_scores_from_worker_results(worker_results)
            
            # Integration ì˜ˆì¸¡
            integration_data = {'scores': agent_scores}
            integration_result = call_integration_api('predict_employee', integration_data)
            results['integration_prediction'] = integration_result
            
            # Integration ë¦¬í¬íŠ¸ ìƒì„±
            report_data = {
                'employee_id': employee_data.get('employee_id'),
                'agent_scores': agent_scores,
                'format': 'json',
                'use_llm': True
            }
            report_result = call_integration_api('generate_report', report_data)
            results['integration_report'] = report_result
            
        except Exception as e:
            results['integration_error'] = str(e)
        
        # ê³„ì¸µì  ê²°ê³¼ ê´€ë¦¬ìë¥¼ í†µí•œ ê²°ê³¼ ì €ì¥
        try:
            employee_id = employee_data.get('employee_id', 'unknown')
            
            # ì—ëŸ¬ê°€ ìˆëŠ” ì›Œì»¤ ê²°ê³¼ í•„í„°ë§ (ì €ì¥ìš©)
            clean_worker_results = {}
            for worker, result in worker_results.items():
                if isinstance(result, dict) and 'error' not in result:
                    clean_worker_results[worker] = result
            
            # ê²°ê³¼ ì €ì¥ (ì—ëŸ¬ê°€ ì—†ëŠ” ì›Œì»¤ ê²°ê³¼ë§Œ) - ë¶€ì„œ ì •ë³´ ì¶”ì¶œ ë° ì „ë‹¬
            if clean_worker_results:
                # ë¶€ì„œ ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ë¡œì§)
                department = employee_data.get('Department', 'Unknown')
                job_role = employee_data.get('JobRole', 'Unknown')
                job_level = employee_data.get('JobLevel', employee_data.get('Position', 'Unknown'))
                
                # ë‹¤ë¥¸ í•„ë“œëª…ìœ¼ë¡œë„ ì‹œë„
                if department == 'Unknown':
                    department = employee_data.get('department', employee_data.get('dept', 'Unknown'))
                if job_role == 'Unknown':
                    job_role = employee_data.get('job_role', employee_data.get('role', 'Unknown'))
                if job_level == 'Unknown':
                    job_level = employee_data.get('job_level', employee_data.get('level', 'Unknown'))
                
                print(f"ğŸ“‹ Supervisor (ë‹¨ì¼) - ì§ì› {employee_id}: {department}/{job_role}/{job_level}")
                
                saved_path = hierarchical_result_manager.save_employee_result(
                    employee_id=employee_id,
                    employee_data=employee_data,
                    worker_results=clean_worker_results,
                    department=department,
                    job_role=job_role,
                    position=job_level
                )
                results['saved_path'] = saved_path
                logger.info(f"ì§ì› {employee_id} ë¶„ì„ ê²°ê³¼ê°€ {saved_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨ (ì§ì› {employee_data.get('employee_id')}): {e}")
            results['save_error'] = str(e)
        
        return results
        
    except Exception as e:
        return {'error': f'Comprehensive analysis failed: {str(e)}'}


@app.route('/api/batch-analysis/save-results', methods=['POST'])
def save_batch_analysis_results():
    """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥ API"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({
                'success': False,
                'error': 'ìš”ì²­ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        # ë°°ì¹˜ ê²°ê³¼ ë°ì´í„° ì¶”ì¶œ
        batch_results = data.get('batchResults', {})
        analysis_summary = data.get('analysisSummary', {})
        
        if not batch_results:
            return jsonify({
                'success': False,
                'error': 'ë°°ì¹˜ ê²°ê³¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 400
        
        # ê²°ê³¼ ì €ì¥ ë¡œì§
        batch_id = f"batch_{int(time.time())}"
        
        # ë°°ì¹˜ ì„¸ì…˜ì— ê²°ê³¼ ì €ì¥
        if not hasattr(app, 'batch_sessions'):
            app.batch_sessions = {}
        
        app.batch_sessions[batch_id] = {
            'batch_id': batch_id,
            'status': 'completed',
            'results': [],
            'errors': [],
            'start_time': datetime.now().isoformat(),
            'end_time': datetime.now().isoformat(),
            'total_employees': analysis_summary.get('totalEmployees', 0),
            'processed_employees': analysis_summary.get('totalEmployees', 0)
        }
        
        # ê° ì—ì´ì „íŠ¸ë³„ ê²°ê³¼ ì²˜ë¦¬
        for agent_name, agent_results in batch_results.items():
            if isinstance(agent_results, list):
                for result in agent_results:
                    if isinstance(result, dict) and result.get('employee_id'):
                        app.batch_sessions[batch_id]['results'].append({
                            'employee_id': result['employee_id'],
                            'agent': agent_name,
                            'analysis': result,
                            'processed_at': datetime.now().isoformat()
                        })
        
        logger.info(f"ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {batch_id}")
        
        return jsonify({
            'success': True,
            'batch_id': batch_id,
            'message': 'ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.'
        })
        
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': f'ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/api/batch-analysis/save-hierarchical-results', methods=['POST'])
def save_hierarchical_batch_results():
    """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ ê³„ì¸µì  êµ¬ì¡°ë¡œ ì €ì¥ API (Department > JobRole > JobLevel > ì§ì›ë³„)"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({
                'success': False,
                'error': 'ìš”ì²­ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        hierarchical_results = data.get('hierarchical_results', {})
        analysis_summary = data.get('analysis_summary', {})
        analysis_timestamp = data.get('analysis_timestamp', datetime.now().isoformat())
        
        if not hierarchical_results:
            return jsonify({
                'success': False,
                'error': 'ê³„ì¸µì  ê²°ê³¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 400
        
        logger.info(f"ê³„ì¸µì  ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹œì‘: {len(hierarchical_results)}ê°œ ë¶€ì„œ")
        
        saved_paths = []
        total_employees_saved = 0
        
        # ê° ë¶€ì„œë³„ë¡œ ì²˜ë¦¬
        for dept_name, job_roles in hierarchical_results.items():
            logger.info(f"ë¶€ì„œ '{dept_name}' ì²˜ë¦¬ ì¤‘...")
            
            for role_name, job_levels in job_roles.items():
                for level_name, employees in job_levels.items():
                    for employee_id, employee_result in employees.items():
                        try:
                            # employee_resultê°€ Noneì´ê±°ë‚˜ dictê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
                            if not employee_result or not isinstance(employee_result, dict):
                                logger.warning(f"ì§ì› {employee_id}ì˜ ê²°ê³¼ ë°ì´í„°ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {type(employee_result)}")
                                continue
                            
                            # hierarchical_result_managerë¥¼ í†µí•´ ì €ì¥
                            employee_data = employee_result.get('employee_data', {})
                            agent_results = employee_result.get('agent_results', {})
                            
                            # ì—ëŸ¬ê°€ ìˆëŠ” ì—ì´ì „íŠ¸ ê²°ê³¼ í•„í„°ë§
                            clean_agent_results = {}
                            for agent_name, agent_result in agent_results.items():
                                if isinstance(agent_result, dict) and 'error' not in agent_result:
                                    clean_agent_results[agent_name] = agent_result
                            
                            if clean_agent_results:
                                # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì´ë¯¸ ê³„ì¸µ êµ¬ì¡°ë¥¼ ë§Œë“¤ì–´ì„œ ë³´ëƒˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                                # ë³€ìˆ˜ ì„€ë„ì‰ ë°©ì§€ë¥¼ ìœ„í•´ ë‹¤ë¥¸ ì´ë¦„ ì‚¬ìš©
                                department = dept_name
                                job_role = role_name
                                job_level = level_name
                                
                                # 'Unknown' ê°’ì€ ê±´ë„ˆëœ€
                                if department == 'Unknown' or department == 'ë¯¸ë¶„ë¥˜':
                                    logger.warning(f"ì§ì› {employee_id}ì˜ ë¶€ì„œê°€ 'Unknown' ë˜ëŠ” 'ë¯¸ë¶„ë¥˜'ì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                                    continue
                                
                                print(f"ğŸ“‹ Supervisor - ì§ì› {employee_id}: {department}/{job_role}/{job_level}")
                                
                                # ê³„ì¸µì  êµ¬ì¡°ë¡œ ì €ì¥ (Department/JobRole/JobLevel/employee_ID) - ë§¤ê°œë³€ìˆ˜ ì „ë‹¬
                                saved_path = hierarchical_result_manager.save_employee_result(
                                    employee_id=employee_id,
                                    employee_data=employee_data,
                                    worker_results=clean_agent_results,
                                    department=department,
                                    job_role=job_role,
                                    position=job_level
                                )
                                saved_paths.append(saved_path)
                                total_employees_saved += 1
                                
                                logger.debug(f"ì§ì› {employee_id} ì €ì¥ ì™„ë£Œ: {saved_path}")
                            
                        except Exception as e:
                            logger.error(f"ì§ì› {employee_id} ì €ì¥ ì‹¤íŒ¨: {e}")
                            continue
        
        # ì „ì²´ í†µê³„ ì—…ë°ì´íŠ¸
        try:
            # ë¶€ì„œë³„ í†µê³„ ì—…ë°ì´íŠ¸
            for department in hierarchical_results.keys():
                hierarchical_result_manager.update_department_statistics(department)
        except Exception as e:
            logger.warning(f"ë¶€ì„œë³„ í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        
        # batch_analysis í´ë”ì— ì „ì²´ ê²°ê³¼ ìš”ì•½ JSON íŒŒì¼ ì €ì¥
        try:
            batch_analysis_dir = hierarchical_result_manager.base_output_dir / 'batch_analysis'
            batch_analysis_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp_str = datetime.now().strftime('%Y-%m-%d_%H-%M-%S-%f')
            
            # 1. department_summary JSON ìƒì„±
            department_summary = {
                'analysis_timestamp': analysis_timestamp,
                'total_employees': total_employees_saved,
                'total_departments': len(hierarchical_results),
                'department_results': {}
            }
            
            # 2. individual_results JSON ìƒì„± (í”Œë« êµ¬ì¡°)
            individual_results = {
                'analysis_timestamp': analysis_timestamp,
                'total_employees': total_employees_saved,
                'individual_results': []
            }
            
            # ëª¨ë“  ì§ì› ë°ì´í„°ë¥¼ ìˆ˜ì§‘
            for dept_name, job_roles in hierarchical_results.items():
                dept_employees = []
                dept_risk_distribution = {'ì•ˆì „êµ°': 0, 'ì£¼ì˜êµ°': 0, 'ê³ ìœ„í—˜êµ°': 0}
                
                for role_name, job_levels in job_roles.items():
                    for level_name, employees in job_levels.items():
                        for employee_id, employee_result in employees.items():
                            if not employee_result or not isinstance(employee_result, dict):
                                continue
                            
                            employee_data = employee_result.get('employee_data', {})
                            agent_results = employee_result.get('agent_results', {})
                            
                            # ìœ„í—˜ ì ìˆ˜ ê³„ì‚°
                            risk_scores = []
                            if 'structura' in agent_results:
                                risk_scores.append(agent_results['structura'].get('attrition_probability', 0))
                            if 'chronos' in agent_results:
                                risk_scores.append(agent_results['chronos'].get('risk_score', 0))
                            if 'cognita' in agent_results:
                                risk_scores.append(agent_results['cognita'].get('overall_risk_score', 0))
                            if 'sentio' in agent_results:
                                risk_scores.append(agent_results['sentio'].get('risk_score', 0))
                            if 'agora' in agent_results:
                                risk_scores.append(agent_results['agora'].get('agora_score', agent_results['agora'].get('market_risk_score', 0)))
                            
                            avg_risk_score = sum(risk_scores) / len(risk_scores) if risk_scores else 0
                            
                            # ìœ„í—˜ë„ ë ˆë²¨ ê²°ì •
                            if avg_risk_score >= 0.7:
                                risk_level = 'high'
                                risk_level_kr = 'ê³ ìœ„í—˜êµ°'
                            elif avg_risk_score >= 0.3:
                                risk_level = 'medium'
                                risk_level_kr = 'ì£¼ì˜êµ°'
                            else:
                                risk_level = 'low'
                                risk_level_kr = 'ì•ˆì „êµ°'
                            
                            dept_risk_distribution[risk_level_kr] += 1
                            
                            # ì§ì› ê²°ê³¼ ìƒì„±
                            employee_summary = {
                                'employee_id': employee_id,
                                'department': dept_name,
                                'risk_score': avg_risk_score,
                                'risk_level': risk_level,
                                'analysis_timestamp': analysis_timestamp,
                                'agent_results': {
                                    'structura': agent_results.get('structura', {}),
                                    'chronos': agent_results.get('chronos', {}),
                                    'cognita': agent_results.get('cognita', {}),
                                    'sentio': agent_results.get('sentio', {}),
                                    'agora': agent_results.get('agora', {})
                                }
                            }
                            
                            dept_employees.append(employee_summary)
                            individual_results['individual_results'].append(employee_summary)
                
                # ë¶€ì„œë³„ ìš”ì•½ ì¶”ê°€
                department_summary['department_results'][dept_name] = {
                    'department': dept_name,
                    'original_name': dept_name,
                    'total_employees': len(dept_employees),
                    'risk_distribution': dept_risk_distribution,
                    'employees': dept_employees
                }
            
            # íŒŒì¼ ì €ì¥
            dept_summary_file = batch_analysis_dir / f'department_summary_{timestamp_str}.json'
            with open(dept_summary_file, 'w', encoding='utf-8') as f:
                json.dump(department_summary, f, ensure_ascii=False, indent=2)
            
            individual_results_file = batch_analysis_dir / f'individual_results_{timestamp_str}.json'
            with open(individual_results_file, 'w', encoding='utf-8') as f:
                json.dump(individual_results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“Š ë°°ì¹˜ ë¶„ì„ ìš”ì•½ íŒŒì¼ ì €ì¥ ì™„ë£Œ:")
            logger.info(f"  - {dept_summary_file}")
            logger.info(f"  - {individual_results_file}")
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ë¶„ì„ ìš”ì•½ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
        
        logger.info(f"ê³„ì¸µì  ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {total_employees_saved}ëª…")
        
        return jsonify({
            'success': True,
            'message': f'ê³„ì¸µì  êµ¬ì¡°ë¡œ ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.',
            'statistics': {
                'total_departments': len(hierarchical_results),
                'total_employees_saved': total_employees_saved,
                'saved_paths_count': len(saved_paths),
                'structure': 'Department > JobRole > JobLevel > Employee'
            },
            'analysis_timestamp': analysis_timestamp,
            'batch_summary_files': {
                'department_summary': f'department_summary_{timestamp_str}.json',
                'individual_results': f'individual_results_{timestamp_str}.json'
            }
        })
        
    except Exception as e:
        logger.error(f"ê³„ì¸µì  ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': f'ê³„ì¸µì  ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


def call_worker_api(worker_name, endpoint, data):
    """ì›Œì»¤ API í˜¸ì¶œ í—¬í¼ í•¨ìˆ˜"""
    import requests
    
    worker_ports = {
        'structura': 5001,
        'cognita': 5002,
        'chronos': 5003,
        'sentio': 5004,
        'agora': 5005
    }
    
    port = worker_ports.get(worker_name)
    if not port:
        raise ValueError(f"Unknown worker: {worker_name}")
    
    if endpoint.startswith('analyze/'):
        # GET ìš”ì²­
        url = f"http://localhost:{port}/api/{endpoint}"
        response = requests.get(url, timeout=30)
    else:
        # POST ìš”ì²­
        url = f"http://localhost:{port}/api/{endpoint}"
        response = requests.post(url, json=data, timeout=30)
    
    if response.status_code == 200:
        return response.json()
    else:
        raise Exception(f"Worker API error: {response.status_code} - {response.text}")


def call_integration_api(endpoint, data):
    """Integration API í˜¸ì¶œ í—¬í¼ í•¨ìˆ˜"""
    import requests
    
    url = f"http://localhost:5007/{endpoint}"
    response = requests.post(url, json=data, timeout=60)
    
    if response.status_code == 200:
        return response.json()
    else:
        raise Exception(f"Integration API error: {response.status_code} - {response.text}")


def extract_scores_from_worker_results(worker_results):
    """ì›Œì»¤ ê²°ê³¼ì—ì„œ ì ìˆ˜ ì¶”ì¶œ"""
    scores = {}
    
    # Structura ì ìˆ˜ ì¶”ì¶œ
    if 'structura' in worker_results and 'error' not in worker_results['structura']:
        structura_data = worker_results['structura']
        if 'probability' in structura_data:
            scores['structura_score'] = structura_data['probability']
        elif 'prediction_probability' in structura_data:
            scores['structura_score'] = structura_data['prediction_probability']
    
    # Cognita ì ìˆ˜ ì¶”ì¶œ
    if 'cognita' in worker_results and 'error' not in worker_results['cognita']:
        cognita_data = worker_results['cognita']
        if 'risk_score' in cognita_data:
            scores['cognita_score'] = cognita_data['risk_score']
    
    # Chronos ì ìˆ˜ ì¶”ì¶œ
    if 'chronos' in worker_results and 'error' not in worker_results['chronos']:
        chronos_data = worker_results['chronos']
        if 'prediction_score' in chronos_data:
            scores['chronos_score'] = chronos_data['prediction_score']
    
    # Sentio ì ìˆ˜ ì¶”ì¶œ
    if 'sentio' in worker_results and 'error' not in worker_results['sentio']:
        sentio_data = worker_results['sentio']
        # psychological_risk_scoreë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš© (JD-R ëª¨ë¸ ê¸°ë°˜)
        if 'psychological_risk_score' in sentio_data:
            scores['sentio_score'] = sentio_data['psychological_risk_score']
        elif 'risk_score' in sentio_data:
            scores['sentio_score'] = sentio_data['risk_score']
        # sentiment_scoreë„ ëŒ€ì•ˆìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥
        elif 'sentiment_score' in sentio_data:
            scores['sentio_score'] = 1.0 - sentio_data['sentiment_score']  # ê°ì • ì ìˆ˜ë¥¼ ìœ„í—˜ ì ìˆ˜ë¡œ ë³€í™˜
    
    # Agora ì ìˆ˜ ì¶”ì¶œ
    if 'agora' in worker_results and 'error' not in worker_results['agora']:
        agora_data = worker_results['agora']
        if 'overall_risk_score' in agora_data:
            scores['agora_score'] = agora_data['overall_risk_score']
    
    return scores


# ------------------------------------------------------
# ì—ëŸ¬ í•¸ë“¤ëŸ¬
# ------------------------------------------------------

@app.errorhandler(404)
def not_found(error):
    return jsonify({
        'success': False,
        'error': 'Endpoint not found',
        'timestamp': datetime.now().isoformat()
    }), 404


@app.route('/api/results/<path:file_path>')
def serve_results_file(file_path):
    """ê²°ê³¼ íŒŒì¼ ì„œë¹™ (404 ì˜¤ë¥˜ ë°©ì§€ìš©)"""
    try:
        # ì‹¤ì œ íŒŒì¼ ê²½ë¡œ êµ¬ì„±
        # app/results ê²½ë¡œ ì‚¬ìš© (hierarchical_result_managerì™€ ë™ì¼)
        # í˜„ì¬ íŒŒì¼: app/Supervisor/supervisor_flask_backend.py
        app_dir = Path(__file__).parent.parent  # app í´ë”
        results_dir = app_dir / 'results'
        full_path = results_dir / file_path
        
        # ë³´ì•ˆì„ ìœ„í•´ results ë””ë ‰í† ë¦¬ ë‚´ë¶€ì¸ì§€ í™•ì¸
        if not str(full_path.resolve()).startswith(str(results_dir.resolve())):
            return jsonify({
                'success': False,
                'error': 'Invalid file path'
            }), 403
        
        # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if full_path.exists() and full_path.is_file():
            return send_file(str(full_path))
        else:
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ JSON ë°˜í™˜ (404 ëŒ€ì‹ )
            return jsonify({
                'success': False,
                'error': 'File not found',
                'message': f'íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}'
            }), 404
            
    except Exception as e:
        logger.error(f"Error serving results file {file_path}: {e}")
        return jsonify({
            'success': False,
            'error': 'Internal server error'
        }), 500


@app.route('/api/search/employees', methods=['GET'])
def search_employees():
    """ì§ì› ê²€ìƒ‰ API"""
    try:
        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        query = request.args.get('query', '').strip()
        department = request.args.get('department', '').strip()
        job_role = request.args.get('job_role', '').strip()
        job_level = request.args.get('job_level', '').strip()
        risk_level = request.args.get('risk_level', '').strip()
        limit = int(request.args.get('limit', 50))
        
        # app/results ê²½ë¡œ ì‚¬ìš© (hierarchical_result_managerì™€ ë™ì¼)
        # í˜„ì¬ íŒŒì¼: app/Supervisor/supervisor_flask_backend.py
        app_dir = Path(__file__).parent.parent  # app í´ë”
        results_dir = app_dir / 'results'
        employees = []
        
        # ëª¨ë“  ë¶€ì„œ ë””ë ‰í† ë¦¬ íƒìƒ‰
        for dept_dir in results_dir.iterdir():
            if not dept_dir.is_dir() or dept_dir.name in ['global_reports', 'models', 'temp']:
                continue
                
            dept_name = dept_dir.name
            
            # ë¶€ì„œ í•„í„°ë§
            if department and department.lower() not in dept_name.lower():
                continue
            
            # ë¶€ì„œ ì¸ë±ìŠ¤ íŒŒì¼ ì½ê¸°
            dept_index_file = dept_dir / 'department_index.json'
            if not dept_index_file.exists():
                continue
                
            with open(dept_index_file, 'r', encoding='utf-8') as f:
                dept_index = json.load(f)
            
            # ì§ë¬´ë³„ íƒìƒ‰
            for role_name, role_data in dept_index.get('job_roles', {}).items():
                # ì§ë¬´ í•„í„°ë§
                if job_role and job_role.lower() not in role_name.lower():
                    continue
                
                # ì§ê¸‰ë³„ íƒìƒ‰
                for level, employee_ids in role_data.items():
                    # ì§ê¸‰ í•„í„°ë§
                    if job_level and job_level != level:
                        continue
                    
                    # ì§ì›ë³„ íƒìƒ‰
                    for emp_id in employee_ids:
                        # ì¿¼ë¦¬ í•„í„°ë§
                        if query and query.lower() not in emp_id.lower():
                            continue
                        
                        # ì§ì› ì •ë³´ ìˆ˜ì§‘
                        emp_dir = dept_dir / role_name / level / f'employee_{emp_id}'
                        if emp_dir.exists():
                            # ì§ì› ê¸°ë³¸ ì •ë³´ ë¡œë“œ
                            emp_info_file = emp_dir / 'employee_info.json'
                            comprehensive_file = emp_dir / 'comprehensive_report.json'
                            
                            employee_data = {
                                'employee_id': emp_id,
                                'department': dept_name,
                                'job_role': role_name,
                                'job_level': level,
                                'file_path': str(emp_dir.relative_to(results_dir))
                            }
                            
                            # ì¢…í•© ë³´ê³ ì„œì—ì„œ ìœ„í—˜ë„ ì •ë³´ ì¶”ì¶œ
                            if comprehensive_file.exists():
                                try:
                                    with open(comprehensive_file, 'r', encoding='utf-8') as f:
                                        comp_data = json.load(f)
                                    
                                    risk_score = comp_data.get('risk_assessment', {}).get('overall_risk_score', 0)
                                    employee_data['risk_score'] = risk_score
                                    
                                    # ìœ„í—˜ë„ ë ˆë²¨ ê³„ì‚°
                                    if risk_score >= 0.7:
                                        employee_data['risk_level'] = 'HIGH'
                                    elif risk_score >= 0.3:
                                        employee_data['risk_level'] = 'MEDIUM'
                                    else:
                                        employee_data['risk_level'] = 'LOW'
                                        
                                except Exception:
                                    employee_data['risk_score'] = 0
                                    employee_data['risk_level'] = 'UNKNOWN'
                            
                            # ìœ„í—˜ë„ ë ˆë²¨ í•„í„°ë§
                            if risk_level and risk_level.upper() != employee_data.get('risk_level', 'UNKNOWN'):
                                continue
                            
                            employees.append(employee_data)
                            
                            # ì œí•œ í™•ì¸
                            if len(employees) >= limit:
                                break
                    
                    if len(employees) >= limit:
                        break
                if len(employees) >= limit:
                    break
            if len(employees) >= limit:
                break
        
        return jsonify({
            'success': True,
            'employees': employees,
            'total': len(employees),
            'query_params': {
                'query': query,
                'department': department,
                'job_role': job_role,
                'job_level': job_level,
                'risk_level': risk_level,
                'limit': limit
            }
        })
        
    except Exception as e:
        logger.error(f"Employee search error: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/search/departments', methods=['GET'])
def search_departments():
    """ë¶€ì„œ ëª©ë¡ ì¡°íšŒ API"""
    try:
        # app/results ê²½ë¡œ ì‚¬ìš© (hierarchical_result_managerì™€ ë™ì¼)
        # í˜„ì¬ íŒŒì¼: app/Supervisor/supervisor_flask_backend.py
        app_dir = Path(__file__).parent.parent  # app í´ë”
        results_dir = app_dir / 'results'
        departments = []
        
        for dept_dir in results_dir.iterdir():
            if not dept_dir.is_dir() or dept_dir.name in ['global_reports', 'models', 'temp']:
                continue
            
            dept_name = dept_dir.name
            dept_index_file = dept_dir / 'department_index.json'
            
            dept_info = {
                'name': dept_name,
                'display_name': dept_name.replace('_', ' '),
                'total_employees': 0,
                'job_roles': []
            }
            
            if dept_index_file.exists():
                try:
                    with open(dept_index_file, 'r', encoding='utf-8') as f:
                        dept_index = json.load(f)
                    
                    # ì§ë¬´ë³„ ì§ì› ìˆ˜ ê³„ì‚°
                    for role_name, role_data in dept_index.get('job_roles', {}).items():
                        role_employees = sum(len(emp_list) for emp_list in role_data.values())
                        dept_info['total_employees'] += role_employees
                        dept_info['job_roles'].append({
                            'name': role_name,
                            'employee_count': role_employees,
                            'job_levels': list(role_data.keys())
                        })
                        
                except Exception as e:
                    logger.warning(f"Error reading department index {dept_name}: {e}")
            
            departments.append(dept_info)
        
        return jsonify({
            'success': True,
            'departments': departments,
            'total': len(departments)
        })
        
    except Exception as e:
        logger.error(f"Department search error: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/employee/<employee_id>/details', methods=['GET'])
def get_employee_details(employee_id):
    """íŠ¹ì • ì§ì›ì˜ ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
    try:
        include_xai = request.args.get('include_xai', 'false').lower() == 'true'
        
        # app/results ê²½ë¡œ ì‚¬ìš© (hierarchical_result_managerì™€ ë™ì¼)
        # í˜„ì¬ íŒŒì¼: app/Supervisor/supervisor_flask_backend.py
        app_dir = Path(__file__).parent.parent  # app í´ë”
        results_dir = app_dir / 'results'
        
        # ì§ì› ë””ë ‰í† ë¦¬ ì°¾ê¸°
        employee_dir = None
        for dept_dir in results_dir.iterdir():
            if not dept_dir.is_dir() or dept_dir.name in ['global_reports', 'models', 'temp']:
                continue
            
            for role_dir in dept_dir.iterdir():
                if not role_dir.is_dir():
                    continue
                    
                for level_dir in role_dir.iterdir():
                    if not level_dir.is_dir():
                        continue
                    
                    emp_dir = level_dir / f'employee_{employee_id}'
                    if emp_dir.exists():
                        employee_dir = emp_dir
                        break
                if employee_dir:
                    break
            if employee_dir:
                break
        
        if not employee_dir:
            return jsonify({
                'success': False,
                'error': f'ì§ì› {employee_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }), 404
        
        # ê²°ê³¼ íŒŒì¼ë“¤ ë¡œë“œ
        result_data = {
            'employee_id': employee_id,
            'directory_path': str(employee_dir.relative_to(results_dir))
        }
        
        # ê° ê²°ê³¼ íŒŒì¼ ë¡œë“œ
        result_files = [
            'employee_info.json',
            'comprehensive_report.json',
            'structura_result.json',
            'chronos_result.json',
            'cognita_result.json',
            'sentio_result.json'
        ]
        
        for file_name in result_files:
            file_path = employee_dir / file_name
            if file_path.exists():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        result_data[file_name.replace('.json', '')] = json.load(f)
                except Exception as e:
                    logger.warning(f"Error loading {file_name}: {e}")
                    result_data[file_name.replace('.json', '')] = None
        
        # ì‹œê°í™” íŒŒì¼ ëª©ë¡
        viz_dir = employee_dir / 'visualizations'
        if viz_dir.exists():
            viz_files = [f.name for f in viz_dir.iterdir() if f.is_file()]
            result_data['visualizations'] = viz_files
        else:
            result_data['visualizations'] = []
        
        return jsonify({
            'success': True,
            'employee_data': result_data
        })
        
    except Exception as e:
        logger.error(f"Employee details error: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.errorhandler(500)
def internal_error(error):
    logger.error(f"Internal server error: {error}")
    return jsonify({
        'success': False,
        'error': 'Internal server error',
        'timestamp': datetime.now().isoformat()
    }), 500


# ì•± ì´ˆê¸°í™”
def create_app():
    """Flask ì•± ìƒì„± ë° ì´ˆê¸°í™”"""
    try:
        initialize_supervisor()
        logger.info("Supervisor Flask backend initialized successfully")
        return app
    except Exception as e:
        logger.error(f"Failed to create app: {e}")
        raise


if __name__ == '__main__':
    try:
        # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        port = int(os.getenv('SUPERVISOR_PORT', '5006'))
        debug = os.getenv('FLASK_DEBUG', 'False').lower() == 'true'
        
        # ì•± ì´ˆê¸°í™”
        app = create_app()
        
        print(f"ğŸš€ Supervisor Flask ì„œë²„ ì‹œì‘")
        print(f"ğŸ“¡ ì„œë²„ ì£¼ì†Œ: http://localhost:{port}")
        print(f"ğŸ”§ ë””ë²„ê·¸ ëª¨ë“œ: {debug}")
        print("\nì‚¬ìš© ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸:")
        print("  ğŸ“Š ë¶„ì„ ê¸°ëŠ¥:")
        print("    GET  /health - ì„œë²„ ìƒíƒœ í™•ì¸")
        print("    POST /analyze_employee - ì§ì› ë¶„ì„")
        print("    GET  /get_workflow_status/<session_id> - ì›Œí¬í”Œë¡œìš° ìƒíƒœ")
        print("    GET  /list_active_sessions - í™œì„± ì„¸ì…˜ ëª©ë¡")
        print("    GET  /worker_health_check - ì›Œì»¤ ìƒíƒœ í™•ì¸")
        print("    POST /batch_analyze - ë°°ì¹˜ ë¶„ì„")
        print("    GET  /system_info - ì‹œìŠ¤í…œ ì •ë³´")
        print("  ğŸ“ íŒŒì¼ ê´€ë¦¬:")
        print("    POST /upload_file - íŒŒì¼ ì—…ë¡œë“œ")
        print("    GET  /list_uploaded_files - ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡")
        print("    GET  /download_file/<filename> - íŒŒì¼ ë‹¤ìš´ë¡œë“œ")
        print("    DELETE /delete_file/<filename> - íŒŒì¼ ì‚­ì œ")
        print("    GET  /get_file_info/<filename> - íŒŒì¼ ìƒì„¸ ì •ë³´")
        print("  ğŸ”— ì›Œì»¤ ì—ì´ì „íŠ¸ API:")
        print("    GET  /api/workers/health_check_all - ëª¨ë“  ì›Œì»¤ ìƒíƒœ í™•ì¸")
        print("    POST /api/workers/structura/predict - Structura ì˜ˆì¸¡")
        print("    GET  /api/workers/cognita/analyze/<employee_id> - Cognita ë¶„ì„")
        print("    POST /api/workers/chronos/predict - Chronos ì˜ˆì¸¡")
        print("    POST /api/workers/sentio/analyze_sentiment - Sentio ê°ì • ë¶„ì„")
        print("    POST /api/workers/agora/comprehensive_analysis - Agora ì‹œì¥ ë¶„ì„")
        print("    POST /api/workers/upload_data - ëª¨ë“  ì›Œì»¤ì— ë°ì´í„° ì—…ë¡œë“œ")
        print("  ğŸ”§ Integration ì—ì´ì „íŠ¸ API:")
        print("    POST /api/workers/integration/load_data - ë°ì´í„° ë¡œë“œ")
        print("    POST /api/workers/integration/calculate_thresholds - ì„ê³„ê°’ ê³„ì‚°")
        print("    POST /api/workers/integration/optimize_weights - ê°€ì¤‘ì¹˜ ìµœì í™”")
        print("    POST /api/workers/integration/predict_employee - ê°œë³„ ì§ì› ì˜ˆì¸¡")
        print("    GET  /api/workers/integration/get_results - ê²°ê³¼ ì¡°íšŒ")
        print("    POST /api/workers/integration/compare_methods - ìµœì í™” ë°©ë²• ë¹„êµ")
        print("    POST /api/workers/integration/generate_report - ê°œë³„ ë ˆí¬íŠ¸ ìƒì„±")
        print("    POST /api/workers/integration/generate_batch_reports - ì¼ê´„ ë ˆí¬íŠ¸ ìƒì„±")
        print("    POST /api/workers/integration/upload_employee_data - ì§ì› ë°ì´í„° ì—…ë¡œë“œ")
        print("    GET  /api/workers/integration/get_employee_list - ì§ì› ëª©ë¡ ì¡°íšŒ")
        print("  ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ê¸°ëŠ¥:")
        print("    POST /batch_process - ëŒ€ëŸ‰ ì§ì› ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬")
        print("    POST /batch_upload_csv - CSV íŒŒì¼ ì—…ë¡œë“œ ë°°ì¹˜ ì²˜ë¦¬")
        print("    GET  /batch_status/<batch_id> - ë°°ì¹˜ ì²˜ë¦¬ ìƒíƒœ ì¡°íšŒ")
        print("    GET  /batch_results/<batch_id> - ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ì¡°íšŒ")
        print("    GET  /batch_export_csv/<batch_id> - ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ CSV ë‚´ë³´ë‚´ê¸°")
        print("  ğŸ“Š í†µê³„ ë¶„ì„ ê¸°ëŠ¥:")
        print("    GET  /api/statistics/group - ë‹¨ì²´ í†µê³„ ì¡°íšŒ (ë¶€ì„œë³„/ì§ë¬´ë³„/ì§ê¸‰ë³„)")
        
        # ì„œë²„ ì‹¤í–‰
        app.run(host='0.0.0.0', port=port, debug=debug)
        
    except Exception as e:
        logger.error(f"Failed to start server: {e}")
        logger.error(traceback.format_exc())
        exit(1)
