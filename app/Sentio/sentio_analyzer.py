# -*- coding: utf-8 -*-
"""
Sentio í‚¤ì›Œë“œ ë¶„ì„ê¸°
ê°œì„ ëœ ëª…ì‚¬ ì¤‘ì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ë° í‡´ì§ì vs ì¬ì§ì ì°¨ë³„ì  í‚¤ì›Œë“œ ë¶„ì„
"""

import pandas as pd
import re
from collections import Counter
from typing import Dict, List, Optional, Tuple, Any
import logging

logger = logging.getLogger(__name__)

class SentioKeywordAnalyzer:
    """ê°œì„ ëœ HR í‚¤ì›Œë“œ ë¶„ì„ê¸° (ëª…ì‚¬ ì¤‘ì‹¬)"""
    
    def __init__(self, csv_file_path: str):
        """
        í‚¤ì›Œë“œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            csv_file_path (str): CSV íŒŒì¼ ê²½ë¡œ
        """
        self.csv_file_path = csv_file_path
        self.data = None
        self.resigned_data = None
        self.stayed_data = None
        
        # ê°•í™”ëœ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
        self.stopwords = self._initialize_comprehensive_stopwords()
        
        # ëª…ì‚¬ íŒ¨í„´ ì •ì˜
        self.noun_patterns = self._initialize_noun_patterns()
        
    def _initialize_comprehensive_stopwords(self) -> set:
        """í¬ê´„ì ì¸ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”"""
        return set([
            # ì¡°ì‚¬ (ì™„ì „í•œ í˜•íƒœ)
            'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ì˜', 'ë„',
            'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜', 'ì—ê²Œì„œ', 'í•œí…Œì„œ', 'ê»˜ì„œ', 'ë¼', 'ì•¼',
            'ì•„', 'ì–´', 'ì—¬', 'ì´ë‘', 'ë‘', 'í•˜ê³ ', 'ì—ë‹¤', 'ì—ë‹¤ê°€', 'ë¡œì„œ', 'ë¡œì¨', 'ì²˜ëŸ¼',
            
            # ì–´ë¯¸ ë° ìš©ì–¸ í™œìš©í˜•
            'ë‹¤', 'ìŠµë‹ˆë‹¤', 'ì…ë‹ˆë‹¤', 'í–ˆìŠµë‹ˆë‹¤', 'ìˆìŠµë‹ˆë‹¤', 'ì—†ìŠµë‹ˆë‹¤', 'ë©ë‹ˆë‹¤', 
            'ë‹ˆë‹¤', 'ì˜€ìŠµë‹ˆë‹¤', 'ì´ì—ˆìŠµë‹ˆë‹¤', 'í•˜ì˜€ìŠµë‹ˆë‹¤', 'ê²ƒì…ë‹ˆë‹¤', 'ì‹¶ìŠµë‹ˆë‹¤',
            'ë³´ì…ë‹ˆë‹¤', 'ê°™ìŠµë‹ˆë‹¤', 'ê² ìŠµë‹ˆë‹¤', 'ë“œë¦½ë‹ˆë‹¤', 'ë¶€íƒë“œë¦½ë‹ˆë‹¤',
            'í•˜ê² ìŠµë‹ˆë‹¤', 'í–ˆì–´ìš”', 'í•´ìš”', 'ì´ì—ìš”', 'ì˜ˆìš”', 'í–ˆë„¤ìš”', 'í–ˆì£ ',
            
            # ë¶€ì‚¬ (ë¬¸ì œê°€ ë˜ëŠ” ë‹¨ì–´ë“¤)
            'ì•„ì§', 'ì´ë¯¸', 'ë²Œì¨', 'ê³§', 'ë°”ë¡œ', 'ì¦‰ì‹œ', 'í•­ìƒ', 'ëŠ˜', 'ìì£¼', 'ê°€ë”',
            'ë•Œë•Œë¡œ', 'ì¢…ì¢…', 'ë‹¤ì‹œ', 'ë˜', 'ë˜ë‹¤ì‹œ', 'ë‹¤ìŒ', 'ì´ë²ˆ', 'ì €ë²ˆ', 'ì–¸ì œë‚˜',
            'ê·¸ëƒ¥', 'ì¢€', 'ì¡°ê¸ˆ', 'ë§ì´', 'ë„ˆë¬´', 'ë§¤ìš°', 'ì •ë§', 'ì§„ì§œ', 'ì™„ì „',
            'ì—„ì²­', 'ë˜ê²Œ', 'ê½¤', 'ìƒë‹¹íˆ', 'ì•½ê°„', 'ì¡°ê¸ˆì”©', 'ì ì ', 'ê°ˆìˆ˜ë¡',
            
            # ëŒ€ëª…ì‚¬ ë° ì§€ì‹œì–´
            'ê·¸', 'ì´', 'ì €', 'ê²ƒ', 'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ê·¸ëŸ°', 'ì´ëŸ°', 'ì €ëŸ°',
            'ê·¸ëŸ¬í•œ', 'ì´ëŸ¬í•œ', 'ì €ëŸ¬í•œ', 'ê·¸ë ‡', 'ì´ë ‡', 'ì €ë ‡', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°',
            'ì´ê³³', 'ê·¸ê³³', 'ì €ê³³', 'ì–´ë””', 'ì–¸ì œ', 'ëˆ„êµ¬', 'ë¬´ì—‡', 'ë­', 'ì–´ë–¤',
            
            # ê°íƒ„ì‚¬ ë° ì˜ì„±ì–´/ì˜íƒœì–´
            'ì•„', 'ì–´', 'ì˜¤', 'ìš°', 'ì—', 'ì´', 'ì™€', 'ìš°ì™€', 'ì–´ë¨¸', 'ì•„ì´ê³ ',
            'ì–´ë¼', 'ì–´ì–´', 'ìŒìŒ', 'í í ', 'í—ˆí—ˆ', 'í•˜í•˜', 'í—¤í—¤', 'íˆíˆ', 'í˜¸í˜¸',
            
            # ì ‘ì†ì‚¬ ë° ì—°ê²°ì–´
            'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ', 'ë˜í•œ', 'ë˜ëŠ”',
            'í˜¹ì€', 'ë§Œì•½', 'ë§Œì¼', 'ë¹„ë¡', 'ì„¤ë ¹', 'ì‹¬ì§€ì–´', 'íŠ¹íˆ', 'ì˜ˆë¥¼ ë“¤ì–´',
            
            # ì¼ë°˜ì ì´ê³  ì˜ë¯¸ê°€ ì•½í•œ ëª…ì‚¬ë“¤
            'ê²ƒ', 'ê±°', 'ê²Œ', 'ê±¸', 'ê±´', 'ê³³', 'ë°', 'ë•Œ', 'ë¶„', 'ë²ˆ', 'ê°œ', 'ëª…',
            'ì›', 'ë…„', 'ì›”', 'ì¼', 'ì‹œ', 'ë¶„', 'ì´ˆ', 'ì ', 'ê°œìˆ˜', 'ìˆ˜', 'ì–‘', 'ì •ë„',
            
            # ë¬¸ì œê°€ ë˜ëŠ” í˜•ìš©ì‚¬ ì–´ê°„ë“¤
            'ëŠë‚Œ', 'ê¸°ë¶„', 'ìƒê°', 'ë§ˆìŒ', 'ì˜ê²¬', 'ê²¬í•´', 'ì…ì¥', 'ê´€ì ', 'ì‹œê°',
            
            # ìì£¼ ë‚˜ì˜¤ì§€ë§Œ ì˜ë¯¸ê°€ ì•½í•œ ë™ì‚¬ ì–´ê°„ë“¤
            'í•˜', 'ë˜', 'ìˆ', 'ì—†', 'ê°€', 'ì˜¤', 'ë³´', 'ë“£', 'ë§í•˜', 'ìƒê°í•˜', 'ëŠë¼',
            'ì•Œ', 'ëª¨ë¥´', 'ì¢‹', 'ë‚˜ì˜', 'í¬', 'ì‘', 'ë†’', 'ë‚®', 'ë§', 'ì ', 'ë¹ ë¥´', 'ëŠë¦¬',
            
            # ì—…ë¬´ í…ìŠ¤íŠ¸ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ì¼ë°˜ì  í‘œí˜„ë“¤
            'ì €í¬', 'ìš°ë¦¬', 'ì œê°€', 'ì €ë¥¼', 'ì €ëŠ”', 'ì €ì˜', 'ì €ì—ê²Œ', 'íšŒì‚¬', 'ì§ì›',
            'ì‚¬ëŒ', 'ë¶„', 'ë‹˜', 'ì”¨', 'ë¶„ë“¤', 'ë‹˜ë“¤', 'ì”¨ë“¤', 'ì—¬ëŸ¬ë¶„', 'ëª¨ë‘',
            
            # ì‹œê°„ ê´€ë ¨ ì¼ë°˜ì  í‘œí˜„
            'ë¨¼ì €', 'ë‚˜ì¤‘', 'ë‹¤ìŒ', 'ì´ì „', 'ì „ì—', 'í›„ì—', 'ë™ì•ˆ', 'ì‚¬ì´', 'ì¤‘ê°„',
            'ì²˜ìŒ', 'ë§ˆì§€ë§‰', 'ë', 'ì‹œì‘', 'ì¤‘', 'í˜„ì¬', 'ì§€ê¸ˆ', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼',
            
            # ì¶”ìƒì ì´ê³  ì˜ë¯¸ê°€ ì•½í•œ í‘œí˜„ë“¤
            'ë°©í–¥', 'ìª½', 'í¸', 'ë©´', 'ì¸¡', 'ê´€ì ', 'ì…ì¥', 'ìƒí™©', 'ê²½ìš°', 'ìƒíƒœ',
            'ì¡°ê±´', 'í™˜ê²½', 'ë¶„ìœ„ê¸°', 'ëŠë‚Œ', 'ê¸°ë¶„', 'ë°©ë²•', 'ë°©ì‹', 'í˜•íƒœ', 'ëª¨ìŠµ',
            
            # ë¬¸ì œê°€ ë˜ëŠ” íŠ¹ì • ë‹¨ì–´ë“¤ (í„°ë¯¸ë„ì—ì„œ í™•ì¸ëœ ê²ƒë“¤)
            'ëŠë‚Œì´', 'ì•„ì§ë„', 'ì£¼ì–´ì§„', 'ë‚¨ì•„ìˆ', 'ì´ìŠˆê°€', 'íŒŒì•…í•˜', 'ë˜ì–´', 'í•˜ì—¬',
            'ì´ë©°', 'í•˜ê³ ', 'í•˜ë‹ˆ', 'í•˜ë©´', 'ë˜ë©´', 'ì´ë©´', 'ë¼ë©´', 'ë‹¤ë©´', 'í•˜ê¸°',
            'ë˜ê¸°', 'ì´ê¸°', 'í•˜ëŠ”', 'ë˜ëŠ”', 'ì´ëŠ”', 'í•˜ì—¬ì„œ', 'ë˜ì–´ì„œ', 'ì´ì–´ì„œ'
        ])
    
    def _initialize_noun_patterns(self) -> Dict[str, List[str]]:
        """ëª…ì‚¬ íŒ¨í„´ ì •ì˜"""
        return {
            # ì—…ë¬´ ê´€ë ¨ ëª…ì‚¬ íŒ¨í„´
            'work_nouns': [
                r'ì—…ë¬´', r'ì¼', r'ì§ë¬´', r'ê³¼ì—…', r'ì„ë¬´', r'ì—­í• ', r'ì±…ì„', r'ë‹´ë‹¹',
                r'í”„ë¡œì íŠ¸', r'ê³¼ì œ', r'ê³„íš', r'ëª©í‘œ', r'ì„±ê³¼', r'ê²°ê³¼', r'ì‹¤ì '
            ],
            
            # ì¡°ì§ ê´€ë ¨ ëª…ì‚¬ íŒ¨í„´  
            'org_nouns': [
                r'íŒ€', r'ë¶€ì„œ', r'ì¡°ì§', r'íšŒì‚¬', r'ê¸°ì—…', r'ì§ì¥', r'ì‚¬ì—…ë¶€', r'ë³¸ë¶€',
                r'íŒ€ì¥', r'ë¶€ì¥', r'ê³¼ì¥', r'ëŒ€ë¦¬', r'ì‚¬ì›', r'ë™ë£Œ', r'ìƒì‚¬', r'ë¶€í•˜'
            ],
            
            # ê°ì •/ìƒíƒœ ê´€ë ¨ ëª…ì‚¬ íŒ¨í„´
            'emotion_nouns': [
                r'ë§Œì¡±', r'ë¶ˆë§Œ', r'ìŠ¤íŠ¸ë ˆìŠ¤', r'ì••ë°•', r'ë¶€ë‹´', r'í”¼ë¡œ', r'ì†Œì§„', r'ë²ˆì•„ì›ƒ',
                r'ì—´ì •', r'ë™ê¸°', r'ì˜ìš•', r'í¥ë¯¸', r'ê´€ì‹¬', r'ì§‘ì¤‘', r'ëª°ì…', r'ì°¸ì—¬'
            ],
            
            # ì„±ì¥/ë°œì „ ê´€ë ¨ ëª…ì‚¬ íŒ¨í„´
            'growth_nouns': [
                r'ì„±ì¥', r'ë°œì „', r'í–¥ìƒ', r'ê°œì„ ', r'ë°œë‹¬', r'ì§„ë³´', r'ì§„ì „', r'ë„ì•½',
                r'êµìœ¡', r'í•™ìŠµ', r'ì—°ìˆ˜', r'êµìœ¡í›ˆë ¨', r'ì—­ëŸ‰', r'ëŠ¥ë ¥', r'ê¸°ìˆ ', r'ìŠ¤í‚¬'
            ],
            
            # ë³´ìƒ/í‰ê°€ ê´€ë ¨ ëª…ì‚¬ íŒ¨í„´
            'reward_nouns': [
                r'ê¸‰ì—¬', r'ì—°ë´‰', r'ì›”ê¸‰', r'ë³´ìƒ', r'ì„ê¸ˆ', r'ìˆ˜ë‹¹', r'ë³´ë„ˆìŠ¤', r'ì¸ì„¼í‹°ë¸Œ',
                r'í‰ê°€', r'ê³ ê³¼', r'ì‹¬ì‚¬', r'ê²€í† ', r'ìŠ¹ì§„', r'ìŠ¹ê²©', r'ì¸ì‚¬', r'ë°œë ¹'
            ]
        }
    
    def load_data(self) -> bool:
        """CSV ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        try:
            self.data = pd.read_csv(self.csv_file_path, encoding='utf-8-sig')
            logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.data)}ê°œ í–‰")
            
            # Attrition ì»¬ëŸ¼ í™•ì¸
            if 'Attrition' not in self.data.columns:
                logger.warning("âŒ 'Attrition' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # í‡´ì§ì/ì¬ì§ì ë°ì´í„° ë¶„ë¦¬
            self.resigned_data = self.data[self.data['Attrition'] == 'Yes']
            self.stayed_data = self.data[self.data['Attrition'] == 'No']
            
            logger.info(f"ğŸ“Š í‡´ì§ì: {len(self.resigned_data)}ëª…, ì¬ì§ì: {len(self.stayed_data)}ëª…")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    def extract_nouns_only(self, text: str) -> List[str]:
        """ëª…ì‚¬ ì¤‘ì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°œì„ ëœ ë°©ì‹)"""
        if pd.isna(text) or not text:
            return []
        
        # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ì œ
        clean_text = self._clean_text(text)
        
        # 2ë‹¨ê³„: ëª…ì‚¬ í›„ë³´ ì¶”ì¶œ
        noun_candidates = self._extract_noun_candidates(clean_text)
        
        # 3ë‹¨ê³„: ëª…ì‚¬ í•„í„°ë§ ë° ì •ì œ
        filtered_nouns = self._filter_and_clean_nouns(noun_candidates)
        
        return filtered_nouns
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        clean_text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        
        # ì—°ì† ê³µë°± ì •ë¦¬
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        
        return clean_text
    
    def _extract_noun_candidates(self, text: str) -> List[str]:
        """ëª…ì‚¬ í›„ë³´ ì¶”ì¶œ"""
        # í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
        words = re.findall(r'[ê°€-í£]{2,}', text)
        
        # ëª…ì‚¬ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ìš°ì„ ìˆœìœ„ ë¶€ì—¬
        noun_candidates = []
        
        for word in words:
            # ëª…ì‚¬ íŒ¨í„´ì— ë§¤ì¹­ë˜ëŠ” ë‹¨ì–´ë“¤ ìš°ì„  ì„ íƒ
            is_likely_noun = self._is_likely_noun(word)
            if is_likely_noun:
                noun_candidates.append(word)
        
        return noun_candidates
    
    def _is_likely_noun(self, word: str) -> bool:
        """ëª…ì‚¬ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¨ì–´ì¸ì§€ íŒë‹¨"""
        # ëª…ì‚¬ íŒ¨í„´ ë§¤ì¹­
        for category, patterns in self.noun_patterns.items():
            for pattern in patterns:
                if re.search(pattern, word):
                    return True
        
        # ëª…ì‚¬ ì–´ë¯¸ íŒ¨í„´ (í•œêµ­ì–´ ëª…ì‚¬ì˜ ì¼ë°˜ì  íŠ¹ì§•)
        noun_endings = [
            r'.*ì„±$',    # ~ì„± (íŠ¹ì„±, ì¤‘ìš”ì„± ë“±)
            r'.*ë„$',    # ~ë„ (ë§Œì¡±ë„, ì°¸ì—¬ë„ ë“±) 
            r'.*ë ¥$',    # ~ë ¥ (ëŠ¥ë ¥, ì—­ëŸ‰ ë“±)
            r'.*ê°$',    # ~ê° (ë§Œì¡±ê°, ì„±ì·¨ê° ë“±)
            r'.*ìœ¨$',    # ~ìœ¨ (íš¨ìœ¨, ë¹„ìœ¨ ë“±)
            r'.*ì œ$',    # ~ì œ (ì œë„, ë³µë¦¬í›„ìƒì œ ë“±)
            r'.*í™”$',    # ~í™” (ê°œì„ í™”, ì²´ê³„í™” ë“±)
            r'.*ê´€$',    # ~ê´€ (ê°€ì¹˜ê´€, ì§ì—…ê´€ ë“±)
            r'.*ì $',    # ~ì  (ê´€ì , ì‹œì  ë“±)
            r'.*ë©´$',    # ~ë©´ (ì¸¡ë©´, ë°©ë©´ ë“±)
        ]
        
        for pattern in noun_endings:
            if re.match(pattern, word):
                return True
        
        # ë³µí•©ëª…ì‚¬ íŒ¨í„´
        compound_patterns = [
            r'.*ì—…ë¬´.*',   # ì—…ë¬´ ê´€ë ¨
            r'.*ê´€ë¦¬.*',   # ê´€ë¦¬ ê´€ë ¨  
            r'.*ê°œë°œ.*',   # ê°œë°œ ê´€ë ¨
            r'.*í‰ê°€.*',   # í‰ê°€ ê´€ë ¨
            r'.*êµìœ¡.*',   # êµìœ¡ ê´€ë ¨
        ]
        
        for pattern in compound_patterns:
            if re.match(pattern, word):
                return True
        
        return False
    
    def _filter_and_clean_nouns(self, candidates: List[str]) -> List[str]:
        """ëª…ì‚¬ í›„ë³´ í•„í„°ë§ ë° ì •ì œ"""
        filtered = []
        
        for word in candidates:
            # ë¶ˆìš©ì–´ ì œê±°
            if word in self.stopwords:
                continue
            
            # ì–´ë¯¸ ì œê±°
            cleaned_word = self._remove_endings(word)
            if not cleaned_word or len(cleaned_word) < 2:
                continue
            
            # ì˜ë¯¸ì—†ëŠ” ë‹¨ì–´ ì œê±°
            if self._is_meaningless_noun(cleaned_word):
                continue
            
            filtered.append(cleaned_word)
        
        return filtered
    
    def _remove_endings(self, word: str) -> str:
        """í•œêµ­ì–´ ì–´ë¯¸ ì œê±° (ëª…ì‚¬ ì¤‘ì‹¬)"""
        # ëª…ì‚¬ ì–´ë¯¸ ì œê±° íŒ¨í„´
        noun_ending_patterns = [
            r'(.+)ë“¤$',      # ë³µìˆ˜í˜• (~ë“¤)
            r'(.+)ì´$',      # ì£¼ê²©ì¡°ì‚¬ (~ì´)  
            r'(.+)ê°€$',      # ì£¼ê²©ì¡°ì‚¬ (~ê°€)
            r'(.+)ì„$',      # ëª©ì ê²©ì¡°ì‚¬ (~ì„)
            r'(.+)ë¥¼$',      # ëª©ì ê²©ì¡°ì‚¬ (~ë¥¼)
            r'(.+)ì—ì„œ$',    # ì²˜ê²©ì¡°ì‚¬ (~ì—ì„œ)
            r'(.+)ìœ¼ë¡œ$',    # ë„êµ¬ê²©ì¡°ì‚¬ (~ìœ¼ë¡œ)
            r'(.+)ë¡œ$',      # ë„êµ¬ê²©ì¡°ì‚¬ (~ë¡œ)
            r'(.+)ì™€$',      # ì ‘ì†ì¡°ì‚¬ (~ì™€)
            r'(.+)ê³¼$',      # ì ‘ì†ì¡°ì‚¬ (~ê³¼)
            r'(.+)ì˜$',      # ê´€í˜•ê²©ì¡°ì‚¬ (~ì˜)
        ]
        
        for pattern in noun_ending_patterns:
            match = re.match(pattern, word)
            if match:
                return match.group(1)
        
        return word
    
    def _is_meaningless_noun(self, word: str) -> bool:
        """ì˜ë¯¸ì—†ëŠ” ëª…ì‚¬ íŒë³„"""
        meaningless_nouns = {
            # ë„ˆë¬´ ì¼ë°˜ì ì¸ ëª…ì‚¬ë“¤
            'ê²ƒ', 'ê±°', 'ê²Œ', 'ê±¸', 'ê±´', 'ê³³', 'ë°', 'ë•Œ', 'ë¶„', 'ë²ˆ',
            'ê°œ', 'ëª…', 'ì›', 'ë…„', 'ì›”', 'ì¼', 'ì‹œ', 'ë¶„', 'ì´ˆ',
            
            # ì¶”ìƒì ì´ê³  ì˜ë¯¸ê°€ ì•½í•œ ëª…ì‚¬ë“¤
            'ë°©í–¥', 'ìª½', 'í¸', 'ë©´', 'ì¸¡', 'ë¶€ë¶„', 'ì „ì²´', 'ì¼ë¶€',
            'ìƒí™©', 'ê²½ìš°', 'ìƒíƒœ', 'ì¡°ê±´', 'í™˜ê²½', 'ë¶„ìœ„ê¸°',
            
            # ë„ˆë¬´ ì¼ë°˜ì ì¸ ì‚¬ëŒ ì§€ì¹­ì–´
            'ì‚¬ëŒ', 'ë¶„', 'ë‹˜', 'ì”¨', 'ì´', 'ì', 'è€…',
            
            # ì˜ë¯¸ê°€ ì•½í•œ ì‹œê°„ í‘œí˜„
            'ì‹œê°„', 'ê¸°ê°„', 'ë™ì•ˆ', 'ì‚¬ì´', 'ì¤‘', 'ë•Œë¬¸',
        }
        
        return word in meaningless_nouns
    
    def analyze_text_columns(self, text_columns: Optional[List[str]] = None) -> Optional[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ë¶„ì„"""
        if self.data is None:
            logger.error("âŒ ë°ì´í„°ë¥¼ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return None
        
        # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì°¾ê¸°
        if text_columns is None:
            text_columns = []
            for col in self.data.columns:
                if col.endswith('_text') or 'text' in col.lower():
                    text_columns.append(col)
        
        if not text_columns:
            logger.error("âŒ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        logger.info(f"ğŸ“ ë°œê²¬ëœ í…ìŠ¤íŠ¸ ì»¬ëŸ¼: {text_columns}")
        
        results = {}
        
        for col in text_columns:
            logger.info(f"ğŸ” '{col}' ì»¬ëŸ¼ ë¶„ì„ ì¤‘...")
            
            # í‡´ì§ì í‚¤ì›Œë“œ ì¶”ì¶œ
            resigned_keywords = []
            for text in self.resigned_data[col].dropna():
                resigned_keywords.extend(self.extract_nouns_only(text))
            
            # ì¬ì§ì í‚¤ì›Œë“œ ì¶”ì¶œ  
            stayed_keywords = []
            for text in self.stayed_data[col].dropna():
                stayed_keywords.extend(self.extract_nouns_only(text))
            
            # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
            resigned_counter = Counter(resigned_keywords)
            stayed_counter = Counter(stayed_keywords)
            
            # ê²°ê³¼ ì €ì¥
            results[col] = {
                'resigned_keywords': resigned_counter,
                'stayed_keywords': stayed_counter,
                'resigned_total': len(resigned_keywords),
                'stayed_total': len(stayed_keywords)
            }
            
            logger.info(f"  í‡´ì§ì í‚¤ì›Œë“œ: {len(resigned_keywords)}ê°œ (ìœ ë‹ˆí¬: {len(resigned_counter)}ê°œ)")
            logger.info(f"  ì¬ì§ì í‚¤ì›Œë“œ: {len(stayed_keywords)}ê°œ (ìœ ë‹ˆí¬: {len(stayed_counter)}ê°œ)")
        
        return results
    
    def find_distinctive_keywords(self, results: Dict[str, Any], min_frequency: int = 5) -> Optional[Dict[str, Any]]:
        """ì°¨ë³„ì  í‚¤ì›Œë“œ ì°¾ê¸° (ê°œì„ ëœ ë°©ì‹)"""
        if not results:
            return None
        
        logger.info(f"ğŸ¯ ì°¨ë³„ì  í‚¤ì›Œë“œ ë¶„ì„ (ìµœì†Œ ë¹ˆë„: {min_frequency}íšŒ)")
        
        all_distinctive = {}
        
        for col, data in results.items():
            logger.info(f"ğŸ“Š [{col}] ì»¬ëŸ¼ ë¶„ì„ ê²°ê³¼:")
            
            resigned_counter = data['resigned_keywords']
            stayed_counter = data['stayed_keywords']
            
            # í‡´ì§ì ê³ ìœ  í‚¤ì›Œë“œ (ì¬ì§ìì—ì„œëŠ” ê±°ì˜ ë‚˜íƒ€ë‚˜ì§€ ì•ŠìŒ)
            resigned_unique = []
            for word, freq in resigned_counter.most_common():
                if freq >= min_frequency:
                    stayed_freq = stayed_counter.get(word, 0)
                    # í‡´ì§ìì—ì„œ ë§ì´ ë‚˜ì˜¤ê³  ì¬ì§ìì—ì„œëŠ” ì ê²Œ ë‚˜ì˜¤ëŠ” ê²½ìš°
                    if stayed_freq <= freq * 0.3:  # ì¬ì§ì ë¹ˆë„ê°€ í‡´ì§ìì˜ 30% ì´í•˜
                        ratio = freq / max(stayed_freq, 1)
                        resigned_unique.append((word, freq, stayed_freq, ratio))
            
            # ì¬ì§ì ê³ ìœ  í‚¤ì›Œë“œ
            stayed_unique = []
            for word, freq in stayed_counter.most_common():
                if freq >= min_frequency:
                    resigned_freq = resigned_counter.get(word, 0)
                    if resigned_freq <= freq * 0.3:
                        ratio = freq / max(resigned_freq, 1)
                        stayed_unique.append((word, freq, resigned_freq, ratio))
            
            # ê²°ê³¼ ì €ì¥
            all_distinctive[col] = {
                'resigned_unique': resigned_unique,
                'stayed_unique': stayed_unique
            }
            
            logger.info(f"ğŸ”´ í‡´ì§ì íŠ¹ì§•ì  í‚¤ì›Œë“œ: {len(resigned_unique)}ê°œ")
            logger.info(f"ğŸ”µ ì¬ì§ì íŠ¹ì§•ì  í‚¤ì›Œë“œ: {len(stayed_unique)}ê°œ")
        
        return all_distinctive
