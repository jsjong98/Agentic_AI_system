# -*- coding: utf-8 -*-
"""
Sentio í‚¤ì›Œë“œ ë¶„ì„ê¸°
ê°œì„ ëœ ëª…ì‚¬ ì¤‘ì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ë° í‡´ì§ì vs ì¬ì§ì ì°¨ë³„ì  í‚¤ì›Œë“œ ë¶„ì„
"""

import pandas as pd
import re
from collections import Counter
from typing import Dict, List, Optional, Tuple, Any
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

class SentioKeywordAnalyzer:
    """ê°œì„ ëœ HR í‚¤ì›Œë“œ ë¶„ì„ê¸° (ëª…ì‚¬ ì¤‘ì‹¬)"""
    
    def __init__(self, csv_file_path: str):
        """
        í‚¤ì›Œë“œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            csv_file_path (str): CSV íŒŒì¼ ê²½ë¡œ
        """
        self.csv_file_path = csv_file_path
        self.data = None
        self.resigned_data = None
        self.stayed_data = None
        
        # ê°•í™”ëœ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
        self.stopwords = self._initialize_comprehensive_stopwords()
        
        # ëª…ì‚¬ íŒ¨í„´ ì •ì˜
        self.noun_patterns = self._initialize_noun_patterns()
        
    def _initialize_comprehensive_stopwords(self) -> set:
        """í¬ê´„ì ì¸ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”"""
        return set([
            # ì¡°ì‚¬ (ì™„ì „í•œ í˜•íƒœ)
            'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ì˜', 'ë„',
            'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜', 'ì—ê²Œì„œ', 'í•œí…Œì„œ', 'ê»˜ì„œ', 'ë¼', 'ì•¼',
            'ì•„', 'ì–´', 'ì—¬', 'ì´ë‘', 'ë‘', 'í•˜ê³ ', 'ì—ë‹¤', 'ì—ë‹¤ê°€', 'ë¡œì„œ', 'ë¡œì¨', 'ì²˜ëŸ¼',
            
            # ì–´ë¯¸ ë° ìš©ì–¸ í™œìš©í˜•
            'ë‹¤', 'ìŠµë‹ˆë‹¤', 'ì…ë‹ˆë‹¤', 'í–ˆìŠµë‹ˆë‹¤', 'ìˆìŠµë‹ˆë‹¤', 'ì—†ìŠµë‹ˆë‹¤', 'ë©ë‹ˆë‹¤', 
            'ë‹ˆë‹¤', 'ì˜€ìŠµë‹ˆë‹¤', 'ì´ì—ˆìŠµë‹ˆë‹¤', 'í•˜ì˜€ìŠµë‹ˆë‹¤', 'ê²ƒì…ë‹ˆë‹¤', 'ì‹¶ìŠµë‹ˆë‹¤',
            'ë³´ì…ë‹ˆë‹¤', 'ê°™ìŠµë‹ˆë‹¤', 'ê² ìŠµë‹ˆë‹¤', 'ë“œë¦½ë‹ˆë‹¤', 'ë¶€íƒë“œë¦½ë‹ˆë‹¤',
            'í•˜ê² ìŠµë‹ˆë‹¤', 'í–ˆì–´ìš”', 'í•´ìš”', 'ì´ì—ìš”', 'ì˜ˆìš”', 'í–ˆë„¤ìš”', 'í–ˆì£ ',
            
            # ë¶€ì‚¬ (ë¬¸ì œê°€ ë˜ëŠ” ë‹¨ì–´ë“¤)
            'ì•„ì§', 'ì´ë¯¸', 'ë²Œì¨', 'ê³§', 'ë°”ë¡œ', 'ì¦‰ì‹œ', 'í•­ìƒ', 'ëŠ˜', 'ìì£¼', 'ê°€ë”',
            'ë•Œë•Œë¡œ', 'ì¢…ì¢…', 'ë‹¤ì‹œ', 'ë˜', 'ë˜ë‹¤ì‹œ', 'ë‹¤ìŒ', 'ì´ë²ˆ', 'ì €ë²ˆ', 'ì–¸ì œë‚˜',
            'ê·¸ëƒ¥', 'ì¢€', 'ì¡°ê¸ˆ', 'ë§ì´', 'ë„ˆë¬´', 'ë§¤ìš°', 'ì •ë§', 'ì§„ì§œ', 'ì™„ì „',
            'ì—„ì²­', 'ë˜ê²Œ', 'ê½¤', 'ìƒë‹¹íˆ', 'ì•½ê°„', 'ì¡°ê¸ˆì”©', 'ì ì ', 'ê°ˆìˆ˜ë¡',
            
            # ëŒ€ëª…ì‚¬ ë° ì§€ì‹œì–´
            'ê·¸', 'ì´', 'ì €', 'ê²ƒ', 'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ê·¸ëŸ°', 'ì´ëŸ°', 'ì €ëŸ°',
            'ê·¸ëŸ¬í•œ', 'ì´ëŸ¬í•œ', 'ì €ëŸ¬í•œ', 'ê·¸ë ‡', 'ì´ë ‡', 'ì €ë ‡', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°',
            'ì´ê³³', 'ê·¸ê³³', 'ì €ê³³', 'ì–´ë””', 'ì–¸ì œ', 'ëˆ„êµ¬', 'ë¬´ì—‡', 'ë­', 'ì–´ë–¤',
            
            # ê°íƒ„ì‚¬ ë° ì˜ì„±ì–´/ì˜íƒœì–´
            'ì•„', 'ì–´', 'ì˜¤', 'ìš°', 'ì—', 'ì´', 'ì™€', 'ìš°ì™€', 'ì–´ë¨¸', 'ì•„ì´ê³ ',
            'ì–´ë¼', 'ì–´ì–´', 'ìŒìŒ', 'í í ', 'í—ˆí—ˆ', 'í•˜í•˜', 'í—¤í—¤', 'íˆíˆ', 'í˜¸í˜¸',
            
            # ì ‘ì†ì‚¬ ë° ì—°ê²°ì–´
            'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ', 'ë˜í•œ', 'ë˜ëŠ”',
            'í˜¹ì€', 'ë§Œì•½', 'ë§Œì¼', 'ë¹„ë¡', 'ì„¤ë ¹', 'ì‹¬ì§€ì–´', 'íŠ¹íˆ', 'ì˜ˆë¥¼ ë“¤ì–´',
            
            # ì¼ë°˜ì ì´ê³  ì˜ë¯¸ê°€ ì•½í•œ ëª…ì‚¬ë“¤
            'ê²ƒ', 'ê±°', 'ê²Œ', 'ê±¸', 'ê±´', 'ê³³', 'ë°', 'ë•Œ', 'ë¶„', 'ë²ˆ', 'ê°œ', 'ëª…',
            'ì›', 'ë…„', 'ì›”', 'ì¼', 'ì‹œ', 'ë¶„', 'ì´ˆ', 'ì ', 'ê°œìˆ˜', 'ìˆ˜', 'ì–‘', 'ì •ë„',
            
            # ë¬¸ì œê°€ ë˜ëŠ” í˜•ìš©ì‚¬ ì–´ê°„ë“¤
            'ëŠë‚Œ', 'ê¸°ë¶„', 'ìƒê°', 'ë§ˆìŒ', 'ì˜ê²¬', 'ê²¬í•´', 'ì…ì¥', 'ê´€ì ', 'ì‹œê°',
            
            # ìì£¼ ë‚˜ì˜¤ì§€ë§Œ ì˜ë¯¸ê°€ ì•½í•œ ë™ì‚¬ ì–´ê°„ë“¤
            'í•˜', 'ë˜', 'ìˆ', 'ì—†', 'ê°€', 'ì˜¤', 'ë³´', 'ë“£', 'ë§í•˜', 'ìƒê°í•˜', 'ëŠë¼',
            'ì•Œ', 'ëª¨ë¥´', 'ì¢‹', 'ë‚˜ì˜', 'í¬', 'ì‘', 'ë†’', 'ë‚®', 'ë§', 'ì ', 'ë¹ ë¥´', 'ëŠë¦¬',
            
            # ì—…ë¬´ í…ìŠ¤íŠ¸ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ì¼ë°˜ì  í‘œí˜„ë“¤
            'ì €í¬', 'ìš°ë¦¬', 'ì œê°€', 'ì €ë¥¼', 'ì €ëŠ”', 'ì €ì˜', 'ì €ì—ê²Œ', 'íšŒì‚¬', 'ì§ì›',
            'ì‚¬ëŒ', 'ë¶„', 'ë‹˜', 'ì”¨', 'ë¶„ë“¤', 'ë‹˜ë“¤', 'ì”¨ë“¤', 'ì—¬ëŸ¬ë¶„', 'ëª¨ë‘',
            
            # ì‹œê°„ ê´€ë ¨ ì¼ë°˜ì  í‘œí˜„
            'ë¨¼ì €', 'ë‚˜ì¤‘', 'ë‹¤ìŒ', 'ì´ì „', 'ì „ì—', 'í›„ì—', 'ë™ì•ˆ', 'ì‚¬ì´', 'ì¤‘ê°„',
            'ì²˜ìŒ', 'ë§ˆì§€ë§‰', 'ë', 'ì‹œì‘', 'ì¤‘', 'í˜„ì¬', 'ì§€ê¸ˆ', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼',
            
            # ì¶”ìƒì ì´ê³  ì˜ë¯¸ê°€ ì•½í•œ í‘œí˜„ë“¤
            'ë°©í–¥', 'ìª½', 'í¸', 'ë©´', 'ì¸¡', 'ê´€ì ', 'ì…ì¥', 'ìƒí™©', 'ê²½ìš°', 'ìƒíƒœ',
            'ì¡°ê±´', 'í™˜ê²½', 'ë¶„ìœ„ê¸°', 'ëŠë‚Œ', 'ê¸°ë¶„', 'ë°©ë²•', 'ë°©ì‹', 'í˜•íƒœ', 'ëª¨ìŠµ',
            
            # ë¬¸ì œê°€ ë˜ëŠ” íŠ¹ì • ë‹¨ì–´ë“¤ (í„°ë¯¸ë„ì—ì„œ í™•ì¸ëœ ê²ƒë“¤)
            'ëŠë‚Œì´', 'ì•„ì§ë„', 'ì£¼ì–´ì§„', 'ë‚¨ì•„ìˆ', 'ì´ìŠˆê°€', 'íŒŒì•…í•˜', 'ë˜ì–´', 'í•˜ì—¬',
            'ì´ë©°', 'í•˜ê³ ', 'í•˜ë‹ˆ', 'í•˜ë©´', 'ë˜ë©´', 'ì´ë©´', 'ë¼ë©´', 'ë‹¤ë©´', 'í•˜ê¸°',
            'ë˜ê¸°', 'ì´ê¸°', 'í•˜ëŠ”', 'ë˜ëŠ”', 'ì´ëŠ”', 'í•˜ì—¬ì„œ', 'ë˜ì–´ì„œ', 'ì´ì–´ì„œ'
        ])
    
    def _initialize_noun_patterns(self) -> Dict[str, List[str]]:
        """ëª…ì‚¬ íŒ¨í„´ ì •ì˜"""
        return {
            # ì—…ë¬´ ê´€ë ¨ ëª…ì‚¬ íŒ¨í„´
            'work_nouns': [
                r'ì—…ë¬´', r'ì¼', r'ì§ë¬´', r'ê³¼ì—…', r'ì„ë¬´', r'ì—­í• ', r'ì±…ì„', r'ë‹´ë‹¹',
                r'í”„ë¡œì íŠ¸', r'ê³¼ì œ', r'ê³„íš', r'ëª©í‘œ', r'ì„±ê³¼', r'ê²°ê³¼', r'ì‹¤ì '
            ],
            
            # ì¡°ì§ ê´€ë ¨ ëª…ì‚¬ íŒ¨í„´  
            'org_nouns': [
                r'íŒ€', r'ë¶€ì„œ', r'ì¡°ì§', r'íšŒì‚¬', r'ê¸°ì—…', r'ì§ì¥', r'ì‚¬ì—…ë¶€', r'ë³¸ë¶€',
                r'íŒ€ì¥', r'ë¶€ì¥', r'ê³¼ì¥', r'ëŒ€ë¦¬', r'ì‚¬ì›', r'ë™ë£Œ', r'ìƒì‚¬', r'ë¶€í•˜'
            ],
            
            # ê°ì •/ìƒíƒœ ê´€ë ¨ ëª…ì‚¬ íŒ¨í„´
            'emotion_nouns': [
                r'ë§Œì¡±', r'ë¶ˆë§Œ', r'ìŠ¤íŠ¸ë ˆìŠ¤', r'ì••ë°•', r'ë¶€ë‹´', r'í”¼ë¡œ', r'ì†Œì§„', r'ë²ˆì•„ì›ƒ',
                r'ì—´ì •', r'ë™ê¸°', r'ì˜ìš•', r'í¥ë¯¸', r'ê´€ì‹¬', r'ì§‘ì¤‘', r'ëª°ì…', r'ì°¸ì—¬'
            ],
            
            # ì„±ì¥/ë°œì „ ê´€ë ¨ ëª…ì‚¬ íŒ¨í„´
            'growth_nouns': [
                r'ì„±ì¥', r'ë°œì „', r'í–¥ìƒ', r'ê°œì„ ', r'ë°œë‹¬', r'ì§„ë³´', r'ì§„ì „', r'ë„ì•½',
                r'êµìœ¡', r'í•™ìŠµ', r'ì—°ìˆ˜', r'êµìœ¡í›ˆë ¨', r'ì—­ëŸ‰', r'ëŠ¥ë ¥', r'ê¸°ìˆ ', r'ìŠ¤í‚¬'
            ],
            
            # ë³´ìƒ/í‰ê°€ ê´€ë ¨ ëª…ì‚¬ íŒ¨í„´
            'reward_nouns': [
                r'ê¸‰ì—¬', r'ì—°ë´‰', r'ì›”ê¸‰', r'ë³´ìƒ', r'ì„ê¸ˆ', r'ìˆ˜ë‹¹', r'ë³´ë„ˆìŠ¤', r'ì¸ì„¼í‹°ë¸Œ',
                r'í‰ê°€', r'ê³ ê³¼', r'ì‹¬ì‚¬', r'ê²€í† ', r'ìŠ¹ì§„', r'ìŠ¹ê²©', r'ì¸ì‚¬', r'ë°œë ¹'
            ]
        }
    
    def load_data(self) -> bool:
        """CSV ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        try:
            self.data = pd.read_csv(self.csv_file_path, encoding='utf-8-sig')
            logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.data)}ê°œ í–‰")
            
            # Attrition ì»¬ëŸ¼ í™•ì¸
            if 'Attrition' not in self.data.columns:
                logger.warning("âŒ 'Attrition' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # í‡´ì§ì/ì¬ì§ì ë°ì´í„° ë¶„ë¦¬
            self.resigned_data = self.data[self.data['Attrition'] == 'Yes']
            self.stayed_data = self.data[self.data['Attrition'] == 'No']
            
            logger.info(f"ğŸ“Š í‡´ì§ì: {len(self.resigned_data)}ëª…, ì¬ì§ì: {len(self.stayed_data)}ëª…")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    def extract_nouns_only(self, text: str) -> List[str]:
        """ëª…ì‚¬ ì¤‘ì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°œì„ ëœ ë°©ì‹, persona í…ìŠ¤íŠ¸ ì œì™¸)"""
        if pd.isna(text) or not text:
            return []
        
        # 0ë‹¨ê³„: persona í…ìŠ¤íŠ¸ í™•ì¸ ë° ì œì™¸
        if self._is_persona_text(text):
            return []
        
        # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ì œ
        clean_text = self._clean_text(text)
        
        # 2ë‹¨ê³„: ëª…ì‚¬ í›„ë³´ ì¶”ì¶œ
        noun_candidates = self._extract_noun_candidates(clean_text)
        
        # 3ë‹¨ê³„: ëª…ì‚¬ í•„í„°ë§ ë° ì •ì œ
        filtered_nouns = self._filter_and_clean_nouns(noun_candidates)
        
        return filtered_nouns
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        clean_text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        
        # ì—°ì† ê³µë°± ì •ë¦¬
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        
        return clean_text
    
    def _extract_noun_candidates(self, text: str) -> List[str]:
        """ëª…ì‚¬ í›„ë³´ ì¶”ì¶œ"""
        # í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
        words = re.findall(r'[ê°€-í£]{2,}', text)
        
        # ëª…ì‚¬ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ìš°ì„ ìˆœìœ„ ë¶€ì—¬
        noun_candidates = []
        
        for word in words:
            # ëª…ì‚¬ íŒ¨í„´ì— ë§¤ì¹­ë˜ëŠ” ë‹¨ì–´ë“¤ ìš°ì„  ì„ íƒ
            is_likely_noun = self._is_likely_noun(word)
            if is_likely_noun:
                noun_candidates.append(word)
        
        return noun_candidates
    
    def _is_likely_noun(self, word: str) -> bool:
        """ëª…ì‚¬ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¨ì–´ì¸ì§€ íŒë‹¨"""
        # ëª…ì‚¬ íŒ¨í„´ ë§¤ì¹­
        for category, patterns in self.noun_patterns.items():
            for pattern in patterns:
                if re.search(pattern, word):
                    return True
        
        # ëª…ì‚¬ ì–´ë¯¸ íŒ¨í„´ (í•œêµ­ì–´ ëª…ì‚¬ì˜ ì¼ë°˜ì  íŠ¹ì§•)
        noun_endings = [
            r'.*ì„±$',    # ~ì„± (íŠ¹ì„±, ì¤‘ìš”ì„± ë“±)
            r'.*ë„$',    # ~ë„ (ë§Œì¡±ë„, ì°¸ì—¬ë„ ë“±) 
            r'.*ë ¥$',    # ~ë ¥ (ëŠ¥ë ¥, ì—­ëŸ‰ ë“±)
            r'.*ê°$',    # ~ê° (ë§Œì¡±ê°, ì„±ì·¨ê° ë“±)
            r'.*ìœ¨$',    # ~ìœ¨ (íš¨ìœ¨, ë¹„ìœ¨ ë“±)
            r'.*ì œ$',    # ~ì œ (ì œë„, ë³µë¦¬í›„ìƒì œ ë“±)
            r'.*í™”$',    # ~í™” (ê°œì„ í™”, ì²´ê³„í™” ë“±)
            r'.*ê´€$',    # ~ê´€ (ê°€ì¹˜ê´€, ì§ì—…ê´€ ë“±)
            r'.*ì $',    # ~ì  (ê´€ì , ì‹œì  ë“±)
            r'.*ë©´$',    # ~ë©´ (ì¸¡ë©´, ë°©ë©´ ë“±)
        ]
        
        for pattern in noun_endings:
            if re.match(pattern, word):
                return True
        
        # ë³µí•©ëª…ì‚¬ íŒ¨í„´
        compound_patterns = [
            r'.*ì—…ë¬´.*',   # ì—…ë¬´ ê´€ë ¨
            r'.*ê´€ë¦¬.*',   # ê´€ë¦¬ ê´€ë ¨  
            r'.*ê°œë°œ.*',   # ê°œë°œ ê´€ë ¨
            r'.*í‰ê°€.*',   # í‰ê°€ ê´€ë ¨
            r'.*êµìœ¡.*',   # êµìœ¡ ê´€ë ¨
        ]
        
        for pattern in compound_patterns:
            if re.match(pattern, word):
                return True
        
        return False
    
    def _filter_and_clean_nouns(self, candidates: List[str]) -> List[str]:
        """ëª…ì‚¬ í›„ë³´ í•„í„°ë§ ë° ì •ì œ"""
        filtered = []
        
        for word in candidates:
            # ë¶ˆìš©ì–´ ì œê±°
            if word in self.stopwords:
                continue
            
            # ì–´ë¯¸ ì œê±°
            cleaned_word = self._remove_endings(word)
            if not cleaned_word or len(cleaned_word) < 2:
                continue
            
            # ì˜ë¯¸ì—†ëŠ” ë‹¨ì–´ ì œê±°
            if self._is_meaningless_noun(cleaned_word):
                continue
            
            filtered.append(cleaned_word)
        
        return filtered
    
    def _remove_endings(self, word: str) -> str:
        """í•œêµ­ì–´ ì–´ë¯¸ ì œê±° (ëª…ì‚¬ ì¤‘ì‹¬)"""
        # ëª…ì‚¬ ì–´ë¯¸ ì œê±° íŒ¨í„´
        noun_ending_patterns = [
            r'(.+)ë“¤$',      # ë³µìˆ˜í˜• (~ë“¤)
            r'(.+)ì´$',      # ì£¼ê²©ì¡°ì‚¬ (~ì´)  
            r'(.+)ê°€$',      # ì£¼ê²©ì¡°ì‚¬ (~ê°€)
            r'(.+)ì„$',      # ëª©ì ê²©ì¡°ì‚¬ (~ì„)
            r'(.+)ë¥¼$',      # ëª©ì ê²©ì¡°ì‚¬ (~ë¥¼)
            r'(.+)ì—ì„œ$',    # ì²˜ê²©ì¡°ì‚¬ (~ì—ì„œ)
            r'(.+)ìœ¼ë¡œ$',    # ë„êµ¬ê²©ì¡°ì‚¬ (~ìœ¼ë¡œ)
            r'(.+)ë¡œ$',      # ë„êµ¬ê²©ì¡°ì‚¬ (~ë¡œ)
            r'(.+)ì™€$',      # ì ‘ì†ì¡°ì‚¬ (~ì™€)
            r'(.+)ê³¼$',      # ì ‘ì†ì¡°ì‚¬ (~ê³¼)
            r'(.+)ì˜$',      # ê´€í˜•ê²©ì¡°ì‚¬ (~ì˜)
        ]
        
        for pattern in noun_ending_patterns:
            match = re.match(pattern, word)
            if match:
                return match.group(1)
        
        return word
    
    def _is_meaningless_noun(self, word: str) -> bool:
        """ì˜ë¯¸ì—†ëŠ” ëª…ì‚¬ íŒë³„"""
        meaningless_nouns = {
            # ë„ˆë¬´ ì¼ë°˜ì ì¸ ëª…ì‚¬ë“¤
            'ê²ƒ', 'ê±°', 'ê²Œ', 'ê±¸', 'ê±´', 'ê³³', 'ë°', 'ë•Œ', 'ë¶„', 'ë²ˆ',
            'ê°œ', 'ëª…', 'ì›', 'ë…„', 'ì›”', 'ì¼', 'ì‹œ', 'ë¶„', 'ì´ˆ',
            
            # ì¶”ìƒì ì´ê³  ì˜ë¯¸ê°€ ì•½í•œ ëª…ì‚¬ë“¤
            'ë°©í–¥', 'ìª½', 'í¸', 'ë©´', 'ì¸¡', 'ë¶€ë¶„', 'ì „ì²´', 'ì¼ë¶€',
            'ìƒí™©', 'ê²½ìš°', 'ìƒíƒœ', 'ì¡°ê±´', 'í™˜ê²½', 'ë¶„ìœ„ê¸°',
            
            # ë„ˆë¬´ ì¼ë°˜ì ì¸ ì‚¬ëŒ ì§€ì¹­ì–´
            'ì‚¬ëŒ', 'ë¶„', 'ë‹˜', 'ì”¨', 'ì´', 'ì', 'è€…',
            
            # ì˜ë¯¸ê°€ ì•½í•œ ì‹œê°„ í‘œí˜„
            'ì‹œê°„', 'ê¸°ê°„', 'ë™ì•ˆ', 'ì‚¬ì´', 'ì¤‘', 'ë•Œë¬¸',
        }
        
        return word in meaningless_nouns
    
    def _is_persona_text(self, text_content: str) -> bool:
        """persona ê´€ë ¨ í…ìŠ¤íŠ¸ì¸ì§€ íŒë³„"""
        if not text_content or pd.isna(text_content):
            return False
        
        text_lower = str(text_content).lower()
        
        # persona ê´€ë ¨ í‚¤ì›Œë“œ íŒ¨í„´ (ê°•í™”ëœ ë²„ì „)
        persona_indicators = [
            'persona', 'í˜ë¥´ì†Œë‚˜', 'p01', 'p02', 'p03', 'p04', 'p05', 'n01', 's02',
            'ë²ˆì•„ì›ƒ ìœ„í—˜êµ°', 'ì„±ì¥ ì¶”êµ¬í˜•', 'ì•ˆì • ì§€í–¥í˜•', 'ë„ì „ ì¶”êµ¬í˜•', 'ê· í˜• ì¶”êµ¬í˜•', 'í˜„ìƒ ìœ ì§€ì', 'ë¼ì´ì§• ìŠ¤íƒ€', 'ì €í‰ê°€ëœ ì „ë¬¸ê°€',
            'burnout risk', 'growth seeker', 'stability oriented', 'challenge seeker', 'balance seeker',
            'softmax_persona', 'persona_code', 'persona_type', 'persona_name',
            # ì¶”ê°€ persona ê´€ë ¨ íŒ¨í„´
            'ìœ„í—˜êµ°', 'ì¶”êµ¬í˜•', 'ì§€í–¥í˜•', 'ìœ ì§€ì', 'ì „ë¬¸ê°€', 'ìŠ¤íƒ€'
        ]
        
        # persona í…ìŠ¤íŠ¸ íŒ¨í„´ í™•ì¸
        for indicator in persona_indicators:
            if indicator in text_lower:
                return True
        
        # persona ì„¤ëª… íŒ¨í„´ (ì¼ë°˜ì ì¸ persona ì„¤ëª… í˜•íƒœ)
        persona_patterns = [
            r'ë†’ì€\s+ì—…ë¬´\s+ë¶€ë‹´',
            r'ìŠ¤íŠ¸ë ˆìŠ¤\s+ìˆ˜ì¤€ì´\s+ë†’',
            r'ë²ˆì•„ì›ƒ\s+ìœ„í—˜',
            r'ì„±ì¥\s+ê¸°íšŒë¥¼\s+ì¶”êµ¬',
            r'ì•ˆì •ì ì¸\s+í™˜ê²½ì„\s+ì„ í˜¸',
            r'ìƒˆë¡œìš´\s+ë„ì „ì„\s+ì¶”êµ¬',
            r'ì¼ê³¼\s+ì‚¶ì˜\s+ê· í˜•'
        ]
        
        for pattern in persona_patterns:
            if re.search(pattern, text_content):
                return True
        
        return False
    
    def calculate_jdr_scores(self, text: str) -> Dict[str, Any]:
        """JD-R (Job Demands-Resources) ëª¨ë¸ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°"""
        
        # ì§ë¬´ ìš”êµ¬ ê´€ë ¨ í‚¤ì›Œë“œ (ë¶€ì •ì  ìš”ì¸)
        job_demands_keywords = {
            'ì—…ë¬´ëŸ‰', 'ì•¼ê·¼', 'ìŠ¤íŠ¸ë ˆìŠ¤', 'ì••ë°•', 'ë§ˆê°', 'ê³¼ë¡œ', 'ë²ˆì•„ì›ƒ', 'í”¼ë¡œ',
            'ë°”ì˜ë‹¤', 'í˜ë“¤ë‹¤', 'ì–´ë µë‹¤', 'ë³µì¡í•˜ë‹¤', 'ë§ë‹¤', 'ë¶€ì¡±í•˜ë‹¤', 'ê¸‰í•˜ë‹¤',
            'ê±±ì •', 'ë¶ˆì•ˆ', 'ê¸´ì¥', 'ë¶€ë‹´', 'ì±…ì„', 'ë¬¸ì œ', 'ì–´ë ¤ì›€', 'ê³¤ë€',
            'ì‹œê°„ë¶€ì¡±', 'ì¸ë ¥ë¶€ì¡±', 'ìì›ë¶€ì¡±', 'ì˜ˆì‚°ë¶€ì¡±', 'ì§€ì›ë¶€ì¡±'
        }
        
        # ì§ë¬´ ìì› ê²°í• ê´€ë ¨ í‚¤ì›Œë“œ (ìì› ë¶€ì¡±)
        job_resources_deficiency_keywords = {
            'ì§€ì›ë¶€ì¡±', 'ë„ì›€ì—†ìŒ', 'í˜¼ì', 'ì™¸ë¡­ë‹¤', 'ì†Œí†µë¶€ì¡±', 'í”¼ë“œë°±ë¶€ì¡±',
            'êµìœ¡ë¶€ì¡±', 'í›ˆë ¨ë¶€ì¡±', 'ì •ë³´ë¶€ì¡±', 'ìë£Œë¶€ì¡±', 'ì¥ë¹„ë¶€ì¡±', 'ì‹œì„¤ë¶€ì¡±',
            'ì¸ì •ë°›ì§€ëª»í•¨', 'ì„±ì¥ê¸°íšŒì—†ìŒ', 'ë°œì „ì—†ìŒ', 'ìŠ¹ì§„ì–´ë ¤ì›€', 'ë³´ìƒë¶€ì¡±',
            'ë¶ˆê³µì •', 'ì°¨ë³„', 'ë¬´ì‹œ', 'ë°°ì œ', 'ì†Œì™¸', 'ê´€ì‹¬ì—†ìŒ', 'ë°©ì¹˜',
            'ììœ¨ì„±ë¶€ì¡±', 'ê¶Œí•œì—†ìŒ', 'ê²°ì •ê¶Œì—†ìŒ', 'ì°¸ì—¬ê¸°íšŒì—†ìŒ'
        }
        
        # í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        detected_keywords = self.extract_nouns_only(text)
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¹­ í‚¤ì›Œë“œ ì°¾ê¸°
        job_demands_matches = []
        job_resources_deficiency_matches = []
        
        for keyword in detected_keywords:
            if keyword in job_demands_keywords:
                job_demands_matches.append(keyword)
            if keyword in job_resources_deficiency_keywords:
                job_resources_deficiency_matches.append(keyword)
        
        # ì ìˆ˜ ê³„ì‚° (í‚¤ì›Œë“œ ë¹ˆë„ ê¸°ë°˜, 0-1 ì •ê·œí™”)
        total_keywords = len(detected_keywords) if detected_keywords else 1
        
        job_demands_score = min(len(job_demands_matches) / total_keywords * 2.0, 1.0)
        job_resources_deficiency_score = min(len(job_resources_deficiency_matches) / total_keywords * 2.0, 1.0)
        
        return {
            'job_demands_score': job_demands_score,
            'job_resources_deficiency_score': job_resources_deficiency_score,
            'detected_keywords': detected_keywords,
            'job_demands_matches': job_demands_matches,
            'job_resources_deficiency_matches': job_resources_deficiency_matches
        }
    
    def analyze_employee_text(self, employee_id, self_review, peer_feedback, weekly_survey):
        """ì§ì› í…ìŠ¤íŠ¸ ì¢…í•© ë¶„ì„ (ê°œì„ ëœ JD-R ëª¨ë¸ ê¸°ë°˜)"""
        
        # í…ìŠ¤íŠ¸ ê²°í•©
        texts = [self_review, peer_feedback, weekly_survey]
        combined_text = ' '.join([str(text) for text in texts if pd.notna(text)])
        
        if not combined_text.strip():
            return {
                'employee_id': employee_id,
                'psychological_risk_score': 0.5,
                'jd_r_indicators': {
                    'job_demands_score': 0.0,
                    'job_resources_deficiency_score': 0.5
                },
                'detected_keywords': [],
                'sentiment_score': 0.5,  # í•˜ìœ„ í˜¸í™˜ì„±
                'risk_keywords': [],     # í•˜ìœ„ í˜¸í™˜ì„±
                'risk_level': 'MEDIUM',
                'analysis_details': "í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ì •í™•í•œ ë¶„ì„ì´ ì–´ë µìŠµë‹ˆë‹¤. ì¶”ê°€ì ì¸ í”¼ë“œë°± ìˆ˜ì§‘ì„ ê¶Œì¥í•©ë‹ˆë‹¤.",
                'attrition_prediction': 0
            }
        
        # JD-R ì§€í‘œ ê³„ì‚°
        jdr_result = self.calculate_jdr_scores(combined_text)
        
        # ì‹¬ë¦¬ì  ìœ„í—˜ ì ìˆ˜ ê³„ì‚° (ê°œì„ ëœ ê°€ì¤‘ì¹˜)
        psychological_risk_score = (
            jdr_result['job_demands_score'] * 0.75 +
            jdr_result['job_resources_deficiency_score'] * 0.25
        )
        psychological_risk_score = min(max(psychological_risk_score, 0.0), 1.0)
        
        # ìµœì  ì„ê³„ê°’ (ê¸°ë³¸ê°’, ì‹¤ì œë¡œëŠ” í•™ìŠµì„ í†µí•´ ê²°ì •)
        optimal_threshold = getattr(self, 'optimal_threshold', 0.45)
        
        # ì´ì§„ ì˜ˆì¸¡ (ìµœì  ì„ê³„ê°’ ì‚¬ìš©)
        attrition_prediction = 1 if psychological_risk_score > optimal_threshold else 0
        
        # ìœ„í—˜ ìˆ˜ì¤€ ê²°ì • (ê°œì„ ëœ ê¸°ì¤€)
        if psychological_risk_score >= 0.7:
            risk_level = 'HIGH'
        elif psychological_risk_score >= 0.3:
            risk_level = 'MEDIUM'
        else:
            risk_level = 'LOW'
        
        # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ í•„ë“œë“¤
        sentiment_score = 1.0 - psychological_risk_score  # ê°ì • ì ìˆ˜ëŠ” ìœ„í—˜ ì ìˆ˜ì˜ ì—­
        risk_keywords = jdr_result['detected_keywords'][:10]  # ìƒìœ„ 10ê°œë§Œ
        
        # CSV ì €ì¥ìš© í•µì‹¬ ê²°ê³¼ (LLM ì—†ì´, ë¹ ë¥¸ ì²˜ë¦¬)
        return {
            'employee_id': employee_id,
            'psychological_risk_score': psychological_risk_score,
            'jd_r_indicators': {
                'job_demands_score': jdr_result['job_demands_score'],
                'job_resources_deficiency_score': jdr_result['job_resources_deficiency_score']
            },
            'detected_keywords': jdr_result['detected_keywords'],
            'job_demands_matches': jdr_result['job_demands_matches'],
            'job_resources_deficiency_matches': jdr_result['job_resources_deficiency_matches'],
            'sentiment_score': sentiment_score,  # í•˜ìœ„ í˜¸í™˜ì„±
            'risk_keywords': risk_keywords,     # í•˜ìœ„ í˜¸í™˜ì„±
            'risk_level': risk_level,
            'attrition_prediction': attrition_prediction,
            'analysis_timestamp': datetime.now().isoformat(),
            # CSV ì €ì¥ì„ ìœ„í•œ ê°„ë‹¨í•œ ìš”ì•½ (LLM ì—†ì´)
            'analysis_summary': f"ìœ„í—˜ë„: {risk_level}, ì§ë¬´ìš”êµ¬: {jdr_result['job_demands_score']:.3f}, ìì›ê²°í•: {jdr_result['job_resources_deficiency_score']:.3f}"
        }
    
    def generate_csv_batch_analysis(self, text_data_list: List[Dict]) -> pd.DataFrame:
        """ëŒ€ëŸ‰ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ CSV ì €ì¥ìš©ìœ¼ë¡œ ë¹ ë¥´ê²Œ ë¶„ì„ (LLM ì—†ì´)"""
        
        results = []
        
        for i, text_data in enumerate(text_data_list):
            employee_id = text_data.get('employee_id', f'emp_{i+1:04d}')
            self_review = text_data.get('self_review', '')
            peer_feedback = text_data.get('peer_feedback', '')
            weekly_survey = text_data.get('weekly_survey', '')
            
            # JD-R ê¸°ë°˜ ë¹ ë¥¸ ë¶„ì„ (LLM ì—†ì´)
            analysis_result = self.analyze_employee_text(
                employee_id=employee_id,
                self_review=self_review,
                peer_feedback=peer_feedback,
                weekly_survey=weekly_survey
            )
            
            # CSVìš© í”Œë« êµ¬ì¡°ë¡œ ë³€í™˜
            csv_row = {
                'employee_id': employee_id,
                'psychological_risk_score': analysis_result['psychological_risk_score'],
                'job_demands_score': analysis_result['jd_r_indicators']['job_demands_score'],
                'job_resources_deficiency_score': analysis_result['jd_r_indicators']['job_resources_deficiency_score'],
                'risk_level': analysis_result['risk_level'],
                'attrition_prediction': analysis_result['attrition_prediction'],
                'sentiment_score': analysis_result['sentiment_score'],
                'detected_keywords_count': len(analysis_result['detected_keywords']),
                'job_demands_keywords': ', '.join(analysis_result['job_demands_matches'][:5]),  # ìƒìœ„ 5ê°œ
                'job_resources_deficiency_keywords': ', '.join(analysis_result['job_resources_deficiency_matches'][:5]),  # ìƒìœ„ 5ê°œ
                'top_risk_keywords': ', '.join(analysis_result['risk_keywords'][:5]),  # ìƒìœ„ 5ê°œ
                'analysis_timestamp': analysis_result['analysis_timestamp'],
                'analysis_summary': analysis_result['analysis_summary']
            }
            
            results.append(csv_row)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥ (100ê°œë§ˆë‹¤)
            if (i + 1) % 100 == 0:
                logger.info(f"CSV ë¶„ì„ ì§„í–‰: {i + 1}/{len(text_data_list)} ì™„ë£Œ")
        
        # DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(results)
        logger.info(f"CSV ë¶„ì„ ì™„ë£Œ: ì´ {len(results)}ëª… ì²˜ë¦¬")
        
        return df
    
    def save_analysis_to_csv(self, df: pd.DataFrame, output_path: str = "sentio_analysis_results.csv") -> str:
        """ë¶„ì„ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        
        try:
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            logger.info(f"CSV ì €ì¥ ì™„ë£Œ: {output_path} ({len(df)}í–‰)")
            return output_path
        except Exception as e:
            logger.error(f"CSV ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def generate_individual_comprehensive_report(self, employee_id: str, all_worker_results: Dict, use_llm: bool = False) -> Dict:
        """ê°œë³„ ì§ì›ì˜ ëª¨ë“  ì›Œì»¤ ì—ì´ì „íŠ¸ ê²°ê³¼ë¥¼ ì¢…í•©í•œ ìµœì¢… ìš”ì•½ ë ˆí¬íŠ¸ ìƒì„±"""
        
        if not all_worker_results:
            return {"error": f"ì§ì› {employee_id}ì˜ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        # ê° ì›Œì»¤ë³„ ê²°ê³¼ ì¶”ì¶œ
        structura_result = all_worker_results.get('structura', {})
        cognita_result = all_worker_results.get('cognita', {})
        chronos_result = all_worker_results.get('chronos', {})
        sentio_result = all_worker_results.get('sentio', {})
        
        # ì¢…í•© ìœ„í—˜ë„ ê³„ì‚° (ê° ì›Œì»¤ì˜ ì ìˆ˜ë¥¼ ê°€ì¤‘í‰ê· )
        risk_scores = []
        
        # Structura: í‡´ì§ í™•ë¥ 
        if structura_result.get('attrition_probability'):
            risk_scores.append(('structura', structura_result['attrition_probability'], 0.3))
        
        # Cognita: ì „ì²´ ìœ„í—˜ë„
        if cognita_result.get('overall_risk_score'):
            risk_scores.append(('cognita', cognita_result['overall_risk_score'], 0.25))
        
        # Chronos: ì˜ˆì¸¡ í™•ë¥ 
        if chronos_result.get('probability'):
            risk_scores.append(('chronos', chronos_result['probability'], 0.2))
        
        # Sentio: ì‹¬ë¦¬ì  ìœ„í—˜ ì ìˆ˜
        if sentio_result.get('psychological_risk_score'):
            risk_scores.append(('sentio', sentio_result['psychological_risk_score'], 0.25))
        
        # ê°€ì¤‘í‰ê·  ê³„ì‚°
        if risk_scores:
            weighted_sum = sum(score * weight for _, score, weight in risk_scores)
            total_weight = sum(weight for _, _, weight in risk_scores)
            comprehensive_risk_score = weighted_sum / total_weight
        else:
            comprehensive_risk_score = 0.5
        
        # ì¢…í•© ìœ„í—˜ ìˆ˜ì¤€ ê²°ì •
        if comprehensive_risk_score >= 0.7:
            overall_risk_level = "HIGH"
            risk_color = "ğŸ”´"
        elif comprehensive_risk_score >= 0.4:
            overall_risk_level = "MEDIUM" 
            risk_color = "ğŸŸ¡"
        else:
            overall_risk_level = "LOW"
            risk_color = "ğŸŸ¢"
        
        # ì£¼ìš” ìœ„í—˜ ìš”ì¸ ì§‘ê³„
        primary_concerns = []
        
        if structura_result.get('top_risk_factors'):
            primary_concerns.extend([f"êµ¬ì¡°ì : {factor}" for factor in structura_result['top_risk_factors'][:2]])
        
        if cognita_result.get('risk_factors'):
            primary_concerns.extend([f"ê´€ê³„ì : {factor}" for factor in cognita_result['risk_factors'][:2]])
        
        if chronos_result.get('risk_indicators'):
            primary_concerns.extend([f"ì‹œê³„ì—´: {factor}" for factor in chronos_result['risk_indicators'][:2]])
        
        if sentio_result.get('job_demands_matches'):
            primary_concerns.extend([f"ì‹¬ë¦¬ì : {factor}" for factor in sentio_result['job_demands_matches'][:2]])
        
        # ê¸°ë³¸ ë ˆí¬íŠ¸ êµ¬ì¡° (LLM ì—†ì´)
        comprehensive_report = {
            'employee_id': employee_id,
            'analysis_timestamp': datetime.now().isoformat(),
            'comprehensive_assessment': {
                'overall_risk_score': round(comprehensive_risk_score, 3),
                'overall_risk_level': overall_risk_level,
                'risk_indicator': risk_color,
                'confidence_level': 'HIGH' if len(risk_scores) >= 3 else 'MEDIUM'
            },
            'worker_scores': {
                'structura': {
                    'attrition_probability': structura_result.get('attrition_probability', 0),
                    'prediction': structura_result.get('prediction', 'Unknown'),
                    'confidence': structura_result.get('confidence', 0)
                },
                'cognita': {
                    'overall_risk_score': cognita_result.get('overall_risk_score', 0),
                    'risk_category': cognita_result.get('risk_category', 'Unknown'),
                    'network_centrality': cognita_result.get('network_centrality', 0)
                },
                'chronos': {
                    'prediction': chronos_result.get('prediction', 'Unknown'),
                    'probability': chronos_result.get('probability', 0),
                    'trend': chronos_result.get('trend', 'Stable')
                },
                'sentio': {
                    'psychological_risk_score': sentio_result.get('psychological_risk_score', 0),
                    'risk_level': sentio_result.get('risk_level', 'MEDIUM'),
                    'job_demands_score': sentio_result.get('jd_r_indicators', {}).get('job_demands_score', 0),
                    'resources_deficiency_score': sentio_result.get('jd_r_indicators', {}).get('job_resources_deficiency_score', 0)
                }
            },
            'primary_concerns': primary_concerns[:6],  # ìƒìœ„ 6ê°œ
            'llm_interpretation': None  # LLM í•´ì„ì€ ì„ íƒì ìœ¼ë¡œ ì¶”ê°€
        }
        
        return comprehensive_report
    
    def generate_comprehensive_llm_interpretation(self, comprehensive_report: Dict, use_llm: bool = False) -> str:
        """ê°œë³„ ì§ì›ì˜ ì¢…í•© ë ˆí¬íŠ¸ì— ëŒ€í•œ LLM í•´ì„ ìƒì„± (ì„ íƒì )"""
        
        if not use_llm:
            # ê·œì¹™ ê¸°ë°˜ í•´ì„ (LLM ì—†ì´)
            employee_id = comprehensive_report['employee_id']
            assessment = comprehensive_report['comprehensive_assessment']
            worker_scores = comprehensive_report['worker_scores']
            concerns = comprehensive_report['primary_concerns']
            
            interpretation = f"""
=== ì§ì› {employee_id} ì¢…í•© ë¶„ì„ ê²°ê³¼ ===

{assessment['risk_indicator']} ì „ì²´ ìœ„í—˜ë„: {assessment['overall_risk_level']} ({assessment['overall_risk_score']:.3f}/1.0)
ğŸ“Š ì‹ ë¢°ë„: {assessment['confidence_level']}

ğŸ” ì›Œì»¤ë³„ ìƒì„¸ ë¶„ì„:
"""
            
            # Structura ë¶„ì„
            structura = worker_scores['structura']
            if structura['attrition_probability'] > 0:
                interpretation += f"ğŸ“ˆ êµ¬ì¡°ì  ë¶„ì„ (Structura): í‡´ì§ í™•ë¥  {structura['attrition_probability']:.1%}, ì˜ˆì¸¡ '{structura['prediction']}'\n"
            
            # Cognita ë¶„ì„
            cognita = worker_scores['cognita']
            if cognita['overall_risk_score'] > 0:
                interpretation += f"ğŸŒ ê´€ê³„ì  ë¶„ì„ (Cognita): ìœ„í—˜ë„ {cognita['overall_risk_score']:.3f}, ì¹´í…Œê³ ë¦¬ '{cognita['risk_category']}'\n"
            
            # Chronos ë¶„ì„
            chronos = worker_scores['chronos']
            if chronos['probability'] > 0:
                interpretation += f"â° ì‹œê³„ì—´ ë¶„ì„ (Chronos): í™•ë¥  {chronos['probability']:.1%}, íŠ¸ë Œë“œ '{chronos['trend']}'\n"
            
            # Sentio ë¶„ì„
            sentio = worker_scores['sentio']
            if sentio['psychological_risk_score'] > 0:
                interpretation += f"ğŸ§  ì‹¬ë¦¬ì  ë¶„ì„ (Sentio): ìœ„í—˜ë„ {sentio['psychological_risk_score']:.3f}, ìˆ˜ì¤€ '{sentio['risk_level']}'\n"
                interpretation += f"   - ì§ë¬´ ìš”êµ¬: {sentio['job_demands_score']:.3f}, ìì› ê²°í•: {sentio['resources_deficiency_score']:.3f}\n"
            
            interpretation += f"\nâš ï¸ ì£¼ìš” ìš°ë ¤ì‚¬í•­:\n"
            for i, concern in enumerate(concerns[:5], 1):
                interpretation += f"{i}. {concern}\n"
            
            interpretation += f"\nğŸ’¡ ê¶Œì¥ ì¡°ì¹˜:\n"
            
            # ìœ„í—˜ ìˆ˜ì¤€ë³„ ê¶Œì¥ì‚¬í•­
            if assessment['overall_risk_level'] == 'HIGH':
                interpretation += "ğŸš¨ ì¦‰ì‹œ ê°œì… í•„ìš”:\n"
                interpretation += "- ìƒê¸‰ìì™€ì˜ ê¸´ê¸‰ ë©´ë‹´ ì‹¤ì‹œ\n"
                interpretation += "- ì—…ë¬´ ì¡°ì • ë° ì§€ì› ë°©ì•ˆ ê²€í† \n"
                interpretation += "- ì •ê¸°ì  ëª¨ë‹ˆí„°ë§ ì²´ê³„ êµ¬ì¶•\n"
            elif assessment['overall_risk_level'] == 'MEDIUM':
                interpretation += "âš ï¸ ì˜ˆë°©ì  ê´€ë¦¬ í•„ìš”:\n"
                interpretation += "- ì •ê¸°ì  ìƒë‹´ ë° í”¼ë“œë°± ì œê³µ\n"
                interpretation += "- ì—…ë¬´ í™˜ê²½ ê°œì„  ê²€í† \n"
                interpretation += "- ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ í”„ë¡œê·¸ë¨ ì°¸ì—¬ ê¶Œì¥\n"
            else:
                interpretation += "âœ… í˜„ì¬ ìƒíƒœ ìœ ì§€:\n"
                interpretation += "- ì •ê¸°ì  ëª¨ë‹ˆí„°ë§ ì§€ì†\n"
                interpretation += "- ê¸ì •ì  ìš”ì†Œ ê°•í™”\n"
                interpretation += "- ì„±ì¥ ê¸°íšŒ ì œê³µ ê²€í† \n"
            
            return interpretation.strip()
        
        else:
            # LLM ê¸°ë°˜ í•´ì„ (ì„ íƒì  ì‚¬ìš©)
            # TODO: OpenAI API í˜¸ì¶œë¡œ ë” ìƒì„¸í•œ í•´ì„ ìƒì„±
            return "LLM ê¸°ë°˜ ê°œë³„ ì§ì› ìƒì„¸ í•´ì„ (êµ¬í˜„ ì˜ˆì •)"
    
    def analyze_text_columns(self, text_columns: Optional[List[str]] = None) -> Optional[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ë¶„ì„"""
        if self.data is None:
            logger.error("âŒ ë°ì´í„°ë¥¼ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return None
        
        # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì°¾ê¸° (persona ê´€ë ¨ ì»¬ëŸ¼ ì œì™¸)
        if text_columns is None:
            text_columns = []
            # persona ê´€ë ¨ ì»¬ëŸ¼ ì œì™¸ ëª©ë¡
            persona_columns_to_exclude = [
                'Persona_Code', 'Persona_Name', 'persona_code', 'persona_name', 
                'persona_type', 'Persona_Type', 'softmax_persona'
            ]
            
            for col in self.data.columns:
                # persona ê´€ë ¨ ì»¬ëŸ¼ì€ ì œì™¸
                if col in persona_columns_to_exclude:
                    continue
                    
                # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ë§Œ í¬í•¨
                if col.endswith('_text') or 'text' in col.lower():
                    text_columns.append(col)
        
        if not text_columns:
            logger.error("âŒ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        logger.info(f"ğŸ“ ë°œê²¬ëœ í…ìŠ¤íŠ¸ ì»¬ëŸ¼: {text_columns}")
        
        results = {}
        
        for col in text_columns:
            logger.info(f"ğŸ” '{col}' ì»¬ëŸ¼ ë¶„ì„ ì¤‘...")
            
            # í‡´ì§ì í‚¤ì›Œë“œ ì¶”ì¶œ (persona í…ìŠ¤íŠ¸ ì œì™¸)
            resigned_keywords = []
            for text in self.resigned_data[col].dropna():
                if not self._is_persona_text(text):
                    resigned_keywords.extend(self.extract_nouns_only(text))
            
            # ì¬ì§ì í‚¤ì›Œë“œ ì¶”ì¶œ (persona í…ìŠ¤íŠ¸ ì œì™¸)
            stayed_keywords = []
            for text in self.stayed_data[col].dropna():
                if not self._is_persona_text(text):
                    stayed_keywords.extend(self.extract_nouns_only(text))
            
            # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
            resigned_counter = Counter(resigned_keywords)
            stayed_counter = Counter(stayed_keywords)
            
            # ê²°ê³¼ ì €ì¥
            results[col] = {
                'resigned_keywords': resigned_counter,
                'stayed_keywords': stayed_counter,
                'resigned_total': len(resigned_keywords),
                'stayed_total': len(stayed_keywords)
            }
            
            logger.info(f"  í‡´ì§ì í‚¤ì›Œë“œ: {len(resigned_keywords)}ê°œ (ìœ ë‹ˆí¬: {len(resigned_counter)}ê°œ)")
            logger.info(f"  ì¬ì§ì í‚¤ì›Œë“œ: {len(stayed_keywords)}ê°œ (ìœ ë‹ˆí¬: {len(stayed_counter)}ê°œ)")
        
        return results
    
    def find_distinctive_keywords(self, results: Dict[str, Any], min_frequency: int = 5) -> Optional[Dict[str, Any]]:
        """ì°¨ë³„ì  í‚¤ì›Œë“œ ì°¾ê¸° (ê°œì„ ëœ ë°©ì‹)"""
        if not results:
            return None
        
        logger.info(f"ğŸ¯ ì°¨ë³„ì  í‚¤ì›Œë“œ ë¶„ì„ (ìµœì†Œ ë¹ˆë„: {min_frequency}íšŒ)")
        
        all_distinctive = {}
        
        for col, data in results.items():
            logger.info(f"ğŸ“Š [{col}] ì»¬ëŸ¼ ë¶„ì„ ê²°ê³¼:")
            
            resigned_counter = data['resigned_keywords']
            stayed_counter = data['stayed_keywords']
            
            # í‡´ì§ì ê³ ìœ  í‚¤ì›Œë“œ (ì¬ì§ìì—ì„œëŠ” ê±°ì˜ ë‚˜íƒ€ë‚˜ì§€ ì•ŠìŒ)
            resigned_unique = []
            for word, freq in resigned_counter.most_common():
                if freq >= min_frequency:
                    stayed_freq = stayed_counter.get(word, 0)
                    # í‡´ì§ìì—ì„œ ë§ì´ ë‚˜ì˜¤ê³  ì¬ì§ìì—ì„œëŠ” ì ê²Œ ë‚˜ì˜¤ëŠ” ê²½ìš°
                    if stayed_freq <= freq * 0.3:  # ì¬ì§ì ë¹ˆë„ê°€ í‡´ì§ìì˜ 30% ì´í•˜
                        ratio = freq / max(stayed_freq, 1)
                        resigned_unique.append((word, freq, stayed_freq, ratio))
            
            # ì¬ì§ì ê³ ìœ  í‚¤ì›Œë“œ
            stayed_unique = []
            for word, freq in stayed_counter.most_common():
                if freq >= min_frequency:
                    resigned_freq = resigned_counter.get(word, 0)
                    if resigned_freq <= freq * 0.3:
                        ratio = freq / max(resigned_freq, 1)
                        stayed_unique.append((word, freq, resigned_freq, ratio))
            
            # ê²°ê³¼ ì €ì¥
            all_distinctive[col] = {
                'resigned_unique': resigned_unique,
                'stayed_unique': stayed_unique
            }
            
            logger.info(f"ğŸ”´ í‡´ì§ì íŠ¹ì§•ì  í‚¤ì›Œë“œ: {len(resigned_unique)}ê°œ")
            logger.info(f"ğŸ”µ ì¬ì§ì íŠ¹ì§•ì  í‚¤ì›Œë“œ: {len(stayed_unique)}ê°œ")
        
        return all_distinctive
