# -*- coding: utf-8 -*-
"""
Sentio HR í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ Flask ë°±ì—”ë“œ ì„œë¹„ìŠ¤
í‚¤ì›Œë“œ ë¶„ì„ + í‡´ì§ ìœ„í—˜ ì‹ í˜¸ íƒì§€ + í…ìŠ¤íŠ¸ ìƒì„± ì‹œìŠ¤í…œ
React ì—°ë™ì— ìµœì í™”ëœ REST API ì„œë²„
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
from werkzeug.exceptions import HTTPException
from werkzeug.utils import secure_filename
import logging
import os
import json
import pandas as pd
import numpy as np
from datetime import datetime
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Tuple, Any
from collections import Counter, defaultdict
import re
import traceback
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ (OpenAI API í‚¤ ë“±)
load_dotenv()

# ë¡œì»¬ ëª¨ë“ˆ import
from sentio_processor import SentioTextProcessor
from sentio_analyzer import SentioKeywordAnalyzer
from sentio_generator import SentioTextGenerator

# Flask ì•± ì´ˆê¸°í™”
app = Flask(__name__)
CORS(app)

# ë¡œê¹… ì„¤ì • - íŒŒì¼ê³¼ ì½˜ì†” ëª¨ë‘ ì¶œë ¥
log_dir = "../../logs"
os.makedirs(log_dir, exist_ok=True)
log_file = os.path.join(log_dir, "sentio_server.log")

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# íŒŒì¼ í•¸ë“¤ëŸ¬
file_handler = logging.FileHandler(log_file, encoding='utf-8')
file_handler.setLevel(logging.INFO)

# ì½˜ì†” í•¸ë“¤ëŸ¬  
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)

# í¬ë§·í„°
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)
console_handler.setFormatter(formatter)

# í•¸ë“¤ëŸ¬ ì¶”ê°€
logger.addHandler(file_handler)
logger.addHandler(console_handler)

# ì „ì—­ ë³€ìˆ˜
text_processor = None
keyword_analyzer = None
text_generator = None

# ë°ì´í„° ê²½ë¡œ ì„¤ì • - uploads ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
def get_sentio_data_paths(analysis_type='batch'):
    """uploads ë””ë ‰í† ë¦¬ì—ì„œ Sentio ë°ì´í„° íŒŒì¼ ì°¾ê¸°"""
    uploads_dir = f"app/uploads/Sentio/{analysis_type}"
    data_paths = {
        'hr_data': None,
        'text_data': None,
        'sample_texts': None
    }
    
    print(f"ğŸ” Sentio ë°ì´í„° ê²½ë¡œ í™•ì¸: {uploads_dir}")
    
    if os.path.exists(uploads_dir):
        files = [f for f in os.listdir(uploads_dir) if f.endswith('.csv')]
        if files:
            # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì‚¬ìš© (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€)
            files.sort(reverse=True)
            print(f"ğŸ“ ë°œê²¬ëœ íŒŒì¼ë“¤: {files}")
            
            # ê°€ì¥ ìµœì‹  íŒŒì¼ì„ ëª¨ë“  ìš©ë„ë¡œ ì‚¬ìš© (Sentio ë°ì´í„°ëŠ” í†µí•© íŒŒì¼)
            latest_file = files[0]
            latest_file_path = os.path.join(uploads_dir, latest_file)
            
            data_paths['hr_data'] = latest_file_path
            data_paths['text_data'] = latest_file_path  
            data_paths['sample_texts'] = latest_file_path
            
            print(f"âœ… ìµœì‹  íŒŒì¼ ì‚¬ìš©: {latest_file}")
    
    # batchì— íŒŒì¼ì´ ì—†ìœ¼ë©´ post ë””ë ‰í† ë¦¬ í™•ì¸
    if analysis_type == 'batch' and not any(data_paths.values()):
        print("ğŸ”„ batch ë””ë ‰í† ë¦¬ì— íŒŒì¼ì´ ì—†ì–´ì„œ post ë””ë ‰í† ë¦¬ í™•ì¸ ì¤‘...")
        post_paths = get_sentio_data_paths('post')
        for key, value in post_paths.items():
            if data_paths[key] is None:
                data_paths[key] = value
    
    # ê¸°ë³¸ê°’ìœ¼ë¡œ fallback (íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ)
    if not any(data_paths.values()):
        print("âš ï¸ uploads ë””ë ‰í† ë¦¬ì— íŒŒì¼ì´ ì—†ì–´ì„œ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©")
        if data_paths['hr_data'] is None:
            data_paths['hr_data'] = 'data/IBM_HR.csv'
        if data_paths['text_data'] is None:
            data_paths['text_data'] = 'data/IBM_HR_text.csv'
        if data_paths['sample_texts'] is None:
            data_paths['sample_texts'] = 'sample_hr_texts.csv'
    
    print(f"ğŸ“Š ìµœì¢… ë°ì´í„° ê²½ë¡œ: {data_paths}")
    return data_paths

# ì´ˆê¸°í™” ì‹œ ë°ì´í„°ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì•„ì„œ ì‚¬ìš©
def find_available_sentio_data():
    """ì‚¬ìš© ê°€ëŠ¥í•œ Sentio ë°ì´í„° ê²½ë¡œ ì°¾ê¸°"""
    # post ë””ë ‰í† ë¦¬ ë¨¼ì € í™•ì¸ (ì‚¬í›„ ë¶„ì„ ë°ì´í„°ê°€ ë” ìµœì‹ )
    for analysis_type in ['post', 'batch']:
        paths = get_sentio_data_paths(analysis_type)
        if any(paths.values()) and any(os.path.exists(path) for path in paths.values() if path):
            print(f"âœ… {analysis_type} ë””ë ‰í† ë¦¬ì—ì„œ ë°ì´í„° ë°œê²¬")
            return paths
    
    # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
    print("âš ï¸ uploads ë””ë ‰í† ë¦¬ì— ë°ì´í„°ê°€ ì—†ì–´ì„œ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©")
    return get_sentio_data_paths('batch')

DATA_PATH = find_available_sentio_data()

MODEL_PATH = 'app/Sentio/models'
os.makedirs(MODEL_PATH, exist_ok=True)

@dataclass
class SentioAnalysisResult:
    """Sentio ë¶„ì„ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    employee_id: str
    text_type: str
    original_text: str
    keywords: List[str]
    sentiment_score: float
    attrition_risk_score: float
    risk_factors: List[str]
    analysis_timestamp: str

@dataclass
class SentioGenerationResult:
    """Sentio í…ìŠ¤íŠ¸ ìƒì„± ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    employee_id: str
    text_type: str
    generated_text: str
    keywords_used: List[str]
    generation_timestamp: str

def initialize_system():
    """
    Sentio ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    """
    global text_processor, keyword_analyzer, text_generator
    
    try:
        logger.info("Sentio ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
        
        # í‚¤ì›Œë“œ ë¶„ì„ê¸° ì´ˆê¸°í™” (í•„ìˆ˜ - ì ìˆ˜ ê³„ì‚°ì„ ìœ„í•´ ë°˜ë“œì‹œ í•„ìš”)
        keyword_analyzer = None
        try:
            sample_texts_path = DATA_PATH['sample_texts']
            print(f"ğŸ” í‚¤ì›Œë“œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹œë„: {sample_texts_path}")
            
            if sample_texts_path and os.path.exists(sample_texts_path):
                print(f"ğŸ“ íŒŒì¼ ì¡´ì¬ í™•ì¸ë¨: {sample_texts_path}")
                keyword_analyzer = SentioKeywordAnalyzer(sample_texts_path)
                
                # ë°ì´í„° ë¡œë“œ ì‹œë„
                load_success = keyword_analyzer.load_data()
                print(f"ğŸ“Š ë°ì´í„° ë¡œë“œ ê²°ê³¼: {load_success}")
                
                if load_success:
                    logger.info("âœ… í‚¤ì›Œë“œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
                    print(f"ğŸ“Š í‚¤ì›Œë“œ ë¶„ì„ê¸° ë°ì´í„° ë¡œë“œ ì„±ê³µ: {sample_texts_path}")
                    print(f"ğŸ“ˆ í‡´ì§ì: {len(keyword_analyzer.resigned_data)}ëª…, ì¬ì§ì: {len(keyword_analyzer.stayed_data)}ëª…")
                else:
                    logger.error("âŒ í‚¤ì›Œë“œ ë¶„ì„ê¸° ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ - ì ìˆ˜ ê³„ì‚° ë¶ˆê°€")
                    keyword_analyzer = None
            else:
                logger.error(f"âŒ í…ìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {sample_texts_path}")
                keyword_analyzer = None
                
        except Exception as e:
            logger.error(f"âŒ í‚¤ì›Œë“œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print(f"âŒ í‚¤ì›Œë“œ ë¶„ì„ê¸° ì˜¤ë¥˜: {str(e)}")
            import traceback
            print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            keyword_analyzer = None
        
        # í…ìŠ¤íŠ¸ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” (analyzer ì—°ê²°)
        text_processor = SentioTextProcessor(analyzer=keyword_analyzer)
        logger.info("âœ… í…ìŠ¤íŠ¸ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì™„ë£Œ (JD-R ëª¨ë¸ ì—°ê²°)")
        
        # í…ìŠ¤íŠ¸ ìƒì„±ê¸° ì´ˆê¸°í™” (ì„ íƒì )
        api_key = os.environ.get('OPENAI_API_KEY')
        if api_key:
            try:
                # í˜ë¥´ì†Œë‚˜ íŒŒì¼ ì—†ì´ í‚¤ì›Œë“œ ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„±ê¸° ì´ˆê¸°í™”
                text_generator = SentioTextGenerator(api_key, None)
                logger.info("âœ… í…ìŠ¤íŠ¸ ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ (í‚¤ì›Œë“œ ê¸°ë°˜)")
            except Exception as e:
                logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ ìƒì„±ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                text_generator = None
        else:
            logger.info("âš ï¸ OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ ìƒì„± ê¸°ëŠ¥ì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
        
        logger.info("ğŸ‰ Sentio ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())
        return False

# ============================================================================
# API ì—”ë“œí¬ì¸íŠ¸
# ============================================================================

@app.route('/')
def home():
    """í™ˆí˜ì´ì§€"""
    return jsonify({
        "service": "Sentio HR Text Analysis API",
        "version": "1.0.0",
        "status": "running",
        "description": "HR í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ ë° í‡´ì§ ìœ„í—˜ ì‹ í˜¸ íƒì§€ ì„œë¹„ìŠ¤",
        "endpoints": {
            "/analyze/text": "í…ìŠ¤íŠ¸ ë¶„ì„ (í‚¤ì›Œë“œ ì¶”ì¶œ + ê°ì • ë¶„ì„)",
            "/analyze/keywords": "í‚¤ì›Œë“œ ë¶„ì„ (í‡´ì§ì vs ì¬ì§ì)",
            "/analyze/risk": "í‡´ì§ ìœ„í—˜ ì‹ í˜¸ ë¶„ì„",
            "/generate/text": "í‚¤ì›Œë“œ ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„±",
            "/health": "ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"
        }
    })

@app.route('/health')
def health_check():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    status = {
        "service": "Sentio",
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "components": {
            "text_processor": text_processor is not None,
            "keyword_analyzer": keyword_analyzer is not None,
            "text_generator": text_generator is not None
        }
    }
    
    # ì „ì²´ ìƒíƒœ í™•ì¸
    all_healthy = all(status["components"].values())
    status["status"] = "healthy" if all_healthy else "degraded"
    
    return jsonify(status)

@app.route('/upload/text_data', methods=['POST'])
def upload_text_data():
    """HR í…ìŠ¤íŠ¸ ë°ì´í„° CSV íŒŒì¼ ì—…ë¡œë“œ"""
    try:
        # íŒŒì¼ í™•ì¸
        if 'file' not in request.files:
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        if not file.filename.lower().endswith('.csv'):
            return jsonify({
                "success": False,
                "error": "CSV íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            }), 400
        
        # íŒŒì¼ ì €ì¥
        filename = secure_filename(file.filename)
        upload_dir = os.path.join(os.path.dirname(__file__), '..', 'uploads', 'Sentio')
        os.makedirs(upload_dir, exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•œ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = os.path.splitext(filename)[0]
        new_filename = f"{base_name}_{timestamp}.csv"
        file_path = os.path.join(upload_dir, new_filename)
        
        # ìµœì‹  íŒŒì¼ ë§í¬ë„ ìƒì„±
        latest_link = os.path.join(upload_dir, 'latest_text_data.csv')
        file.save(file_path)
        
        # ìµœì‹  íŒŒì¼ ë§í¬ ìƒì„±
        try:
            if os.path.exists(latest_link):
                os.remove(latest_link)
            import shutil
            shutil.copy2(file_path, latest_link)
        except Exception as e:
            logger.warning(f"ìµœì‹  íŒŒì¼ ë§í¬ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # ë°ì´í„° ê²€ì¦
        try:
            df = pd.read_csv(file_path)
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_columns = ['employee_id', 'text']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                return jsonify({
                    "success": False,
                    "error": f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing_columns)}",
                    "required_columns": required_columns,
                    "found_columns": list(df.columns)
                }), 400
            
            # í…ìŠ¤íŠ¸ ë°ì´í„° ê¸°ë³¸ í†µê³„
            text_stats = {
                "total_texts": len(df),
                "unique_employees": df['employee_id'].nunique(),
                "avg_text_length": df['text'].str.len().mean(),
                "text_types": df['text_type'].value_counts().to_dict() if 'text_type' in df.columns else {}
            }
            
            # ê¸°ì¡´ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ìƒˆ ë°ì´í„°ë¡œ ì¬ì²˜ë¦¬ í•„ìš”)
            global text_processor, keyword_analyzer
            text_processor = None
            keyword_analyzer = None
            
            return jsonify({
                "success": True,
                "message": "í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "file_info": {
                    "original_filename": filename,
                    "saved_filename": new_filename,
                    "upload_path": upload_dir,
                    "file_path": file_path,
                    "latest_link": latest_link,
                    "rows": len(df),
                    "columns": len(df.columns),
                    "column_names": list(df.columns)
                },
                "text_stats": text_stats,
                "note": "ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì‹œìŠ¤í…œì„ ì¬ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”."
            })
            
        except Exception as e:
            return jsonify({
                "success": False,
                "error": f"ë°ì´í„° íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}"
            }), 400
            
    except Exception as e:
        return jsonify({
            "success": False,
            "error": f"íŒŒì¼ ì—…ë¡œë“œ ì˜¤ë¥˜: {str(e)}"
        }), 500

@app.route('/analyze/text', methods=['POST'])
def analyze_text():
    """
    í…ìŠ¤íŠ¸ ë¶„ì„ API
    ì…ë ¥: í…ìŠ¤íŠ¸, ì§ì›ID (ì„ íƒ)
    ì¶œë ¥: í‚¤ì›Œë“œ, ê°ì •ì ìˆ˜, í‡´ì§ìœ„í—˜ì ìˆ˜
    """
    try:
        data = request.get_json()
        
        if not data or 'text' not in data:
            return jsonify({"error": "í…ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        text = data['text']
        employee_id = data.get('employee_id', 'unknown')
        text_type = data.get('text_type', 'general')
        
        if not text_processor:
            return jsonify({"error": "í…ìŠ¤íŠ¸ í”„ë¡œì„¸ì„œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 500
        
        # í…ìŠ¤íŠ¸ ë¶„ì„ ìˆ˜í–‰
        analysis_result = text_processor.analyze_text(
            text=text,
            employee_id=employee_id,
            text_type=text_type
        )
        
        # ê²°ê³¼ ë°˜í™˜
        result = SentioAnalysisResult(
            employee_id=employee_id,
            text_type=text_type,
            original_text=text,
            keywords=analysis_result['keywords'],
            sentiment_score=analysis_result['sentiment_score'],
            attrition_risk_score=analysis_result['attrition_risk_score'],
            risk_factors=analysis_result['risk_factors'],
            analysis_timestamp=datetime.now().isoformat()
        )
        
        return jsonify(asdict(result))
        
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return jsonify({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}), 500

@app.route('/analyze/keywords', methods=['POST'])
def analyze_keywords():
    """
    í‚¤ì›Œë“œ ë¶„ì„ API
    ì…ë ¥: ë¶„ì„ ì˜µì…˜
    ì¶œë ¥: í‡´ì§ì vs ì¬ì§ì ì°¨ë³„ì  í‚¤ì›Œë“œ
    """
    try:
        data = request.get_json() or {}
        min_frequency = data.get('min_frequency', 5)
        text_columns = data.get('text_columns', None)
        
        if not keyword_analyzer:
            return jsonify({"error": "í‚¤ì›Œë“œ ë¶„ì„ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 500
        
        # í‚¤ì›Œë“œ ë¶„ì„ ìˆ˜í–‰
        results = keyword_analyzer.analyze_text_columns(text_columns)
        if not results:
            return jsonify({"error": "ë¶„ì„í•  í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 400
        
        # ì°¨ë³„ì  í‚¤ì›Œë“œ ì°¾ê¸°
        distinctive_keywords = keyword_analyzer.find_distinctive_keywords(
            results, min_frequency=min_frequency
        )
        
        # ê²°ê³¼ ì •ë¦¬
        analysis_summary = {
            "analysis_timestamp": datetime.now().isoformat(),
            "min_frequency": min_frequency,
            "columns_analyzed": list(results.keys()),
            "distinctive_keywords": distinctive_keywords,
            "summary": {
                col: {
                    "resigned_total_keywords": data['resigned_total'],
                    "stayed_total_keywords": data['stayed_total'],
                    "resigned_unique_count": len(distinctive_keywords[col]['resigned_unique']),
                    "stayed_unique_count": len(distinctive_keywords[col]['stayed_unique'])
                }
                for col, data in results.items()
            }
        }
        
        return jsonify(analysis_summary)
        
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return jsonify({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}), 500

@app.route('/analyze/risk', methods=['POST'])
def analyze_attrition_risk():
    """
    í‡´ì§ ìœ„í—˜ ì‹ í˜¸ ë¶„ì„ API
    ì…ë ¥: ì§ì› í…ìŠ¤íŠ¸ ë°ì´í„°
    ì¶œë ¥: ìœ„í—˜ ì ìˆ˜ ë° ì£¼ìš” ì‹ í˜¸
    """
    try:
        data = request.get_json()
        
        if not data or 'texts' not in data:
            return jsonify({"error": "ë¶„ì„í•  í…ìŠ¤íŠ¸ ëª©ë¡ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        texts = data['texts']  # List of {employee_id, text, text_type}
        
        if not text_processor:
            return jsonify({"error": "í…ìŠ¤íŠ¸ í”„ë¡œì„¸ì„œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 500
        
        risk_analysis_results = []
        
        for text_data in texts:
            employee_id = text_data.get('employee_id', 'unknown')
            text = text_data.get('text', '')
            text_type = text_data.get('text_type', 'general')
            
            if not text:
                continue
            
            # ìœ„í—˜ ì‹ í˜¸ ë¶„ì„
            risk_result = text_processor.analyze_attrition_risk(
                text=text,
                employee_id=employee_id
            )
            
            risk_analysis_results.append({
                "employee_id": employee_id,
                "text_type": text_type,
                "risk_score": risk_result['risk_score'],
                "risk_level": risk_result['risk_level'],
                "risk_factors": risk_result['risk_factors'],
                "keywords_detected": risk_result['keywords_detected']
            })
        
        # ì „ì²´ ìš”ì•½ í†µê³„
        risk_scores = [r['risk_score'] for r in risk_analysis_results]
        summary_stats = {
            "total_analyzed": len(risk_analysis_results),
            "average_risk_score": np.mean(risk_scores) if risk_scores else 0,
            "high_risk_count": len([r for r in risk_analysis_results if r['risk_level'] == 'high']),
            "medium_risk_count": len([r for r in risk_analysis_results if r['risk_level'] == 'medium']),
            "low_risk_count": len([r for r in risk_analysis_results if r['risk_level'] == 'low'])
        }
        
        return jsonify({
            "analysis_timestamp": datetime.now().isoformat(),
            "summary": summary_stats,
            "individual_results": risk_analysis_results
        })
        
    except Exception as e:
        logger.error(f"ìœ„í—˜ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return jsonify({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}), 500

@app.route('/generate/text', methods=['POST'])
def generate_text():
    """
    í‚¤ì›Œë“œ ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„± API
    ì…ë ¥: í…ìŠ¤íŠ¸ íƒ€ì…, í‚¤ì›Œë“œ ëª©ë¡
    ì¶œë ¥: ìƒì„±ëœ í…ìŠ¤íŠ¸
    """
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({"error": "ìš”ì²­ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        if not text_generator:
            return jsonify({"error": "í…ìŠ¤íŠ¸ ìƒì„±ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. OpenAI API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."}), 500
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„±
        if 'keywords' in data:
            keywords = data['keywords']
            text_type = data.get('text_type', 'SELF_REVIEW')
            employee_id = data.get('employee_id', 'unknown')
            analysis_type = data.get('analysis_type', 'batch')
            
            # ë°°ì¹˜/ì‚¬í›„ ë¶„ì„ì—ì„œëŠ” LLM ì‚¬ìš© ì•ˆí•¨ (API ë¹„ìš© ì ˆì•½)
            use_llm = analysis_type not in ['batch', 'post']
            
            if use_llm:
                generated_text = text_generator.generate_text_from_keywords(
                    keywords=keywords,
                    text_type=text_type
                )
            else:
                # ë°°ì¹˜/ì‚¬í›„ ë¶„ì„ì—ì„œëŠ” ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ í…ìŠ¤íŠ¸ ë°˜í™˜
                generated_text = f"í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ (ë¶„ì„ íƒ€ì…: {analysis_type}, í‚¤ì›Œë“œ: {', '.join(keywords[:5])})"
            
            result = SentioGenerationResult(
                employee_id=employee_id,
                text_type=text_type,
                generated_text=generated_text,
                keywords_used=keywords[:5],  # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œë§Œ
                generation_timestamp=datetime.now().isoformat()
            )
            
            return jsonify(asdict(result))
        
        else:
            return jsonify({"error": "keywordsê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return jsonify({"error": f"ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}), 500

@app.route('/data/keywords', methods=['GET'])
def get_available_keywords():
    """ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ì›Œë“œ ëª©ë¡ ì¡°íšŒ API"""
    try:
        if not text_generator:
            return jsonify({"error": "í…ìŠ¤íŠ¸ ìƒì„±ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 500
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ë°˜í™˜
        available_keywords = text_generator.get_all_available_keywords()
        
        return jsonify({
            "keyword_categories": available_keywords,
            "total_categories": len(available_keywords)
        })
        
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return jsonify({"error": f"ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}), 500

@app.route('/analyze/comprehensive_report', methods=['POST'])
def generate_comprehensive_report():
    """
    ê°œë³„ ì§ì›ì˜ ëª¨ë“  ì›Œì»¤ ì—ì´ì „íŠ¸ ê²°ê³¼ë¥¼ ì¢…í•©í•œ ìµœì¢… ë ˆí¬íŠ¸ ìƒì„± API (LLM ì„ íƒì  ì‚¬ìš©)
    ì…ë ¥: í•œ ì§ì›ì˜ ëª¨ë“  ì›Œì»¤ ë¶„ì„ ê²°ê³¼
    ì¶œë ¥: ê°œë³„ ì§ì› ì¢…í•© ë¶„ì„ ë ˆí¬íŠ¸
    """
    try:
        data = request.get_json()
        
        if not data or 'employee_id' not in data or 'worker_results' not in data:
            return jsonify({"error": "ì§ì› IDì™€ ëª¨ë“  ì›Œì»¤ ë¶„ì„ ê²°ê³¼ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        employee_id = data['employee_id']
        worker_results = data['worker_results']  # {structura: {...}, cognita: {...}, chronos: {...}, sentio: {...}}
        analysis_type = data.get('analysis_type', 'batch')
        
        # ë°°ì¹˜/ì‚¬í›„ ë¶„ì„ì—ì„œëŠ” LLM ì‚¬ìš© ì•ˆí•¨ (API ë¹„ìš© ì ˆì•½)
        use_llm = data.get('use_llm', analysis_type not in ['batch', 'post'])
        
        if not keyword_analyzer:
            return jsonify({"error": "í‚¤ì›Œë“œ ë¶„ì„ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. JD-R ëª¨ë¸ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ê±°ë‚˜ ë°ì´í„° íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."}), 500
        
        # ì¢…í•© ë ˆí¬íŠ¸ ìƒì„±
        comprehensive_report = keyword_analyzer.generate_individual_comprehensive_report(
            employee_id=employee_id,
            all_worker_results=worker_results,
            use_llm=use_llm
        )
        
        # LLM í•´ì„ ì¶”ê°€ (ì„ íƒì )
        if use_llm:
            llm_interpretation = keyword_analyzer.generate_comprehensive_llm_interpretation(
                comprehensive_report=comprehensive_report,
                use_llm=True
            )
            comprehensive_report['llm_interpretation'] = llm_interpretation
        else:
            # ê·œì¹™ ê¸°ë°˜ í•´ì„
            rule_based_interpretation = keyword_analyzer.generate_comprehensive_llm_interpretation(
                comprehensive_report=comprehensive_report,
                use_llm=False
            )
            comprehensive_report['rule_based_interpretation'] = rule_based_interpretation
        
        return jsonify(comprehensive_report)
        
    except Exception as e:
        logger.error(f"ì¢…í•© ë ˆí¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return jsonify({"error": f"ë ˆí¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}), 500

@app.route('/analyze_sentiment', methods=['POST'])
def analyze_sentiment():
    """
    Supervisorì—ì„œ í˜¸ì¶œí•˜ëŠ” ê°ì • ë¶„ì„ API
    ì…ë ¥: employee_id, ì¶”ê°€ ë°ì´í„°
    ì¶œë ¥: ê°ì • ë¶„ì„ ê²°ê³¼
    """
    global DATA_PATH, keyword_analyzer, text_processor
    
    try:
        data = request.get_json()
        
        # ë‹¨ì¼ ì§ì› ë¶„ì„ê³¼ ë°°ì¹˜ ë¶„ì„ ëª¨ë‘ ì§€ì›
        if 'employee_id' in data:
            # ë‹¨ì¼ ì§ì› ë¶„ì„
            employee_id = data['employee_id']
            analysis_type = data.get('analysis_type', 'batch')
            employees_data = [{'employee_id': employee_id, 'text_data': data.get('text_data', {})}]
        elif 'employees' in data:
            # ë°°ì¹˜ ë¶„ì„ - CSV íŒŒì¼ì—ì„œ ì§ì› ë°ì´í„° ì½ì–´ì˜¤ê¸°
            employee_ids = data['employees']
            analysis_type = data.get('analysis_type', 'batch')
            if not employee_ids:
                return jsonify({"error": "employees ë°°ì—´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}), 400
            
            # CSV íŒŒì¼ì—ì„œ í•´ë‹¹ ì§ì›ë“¤ì˜ ë°ì´í„° ì½ì–´ì˜¤ê¸°
            try:
                csv_path = get_sentio_data_paths(analysis_type)['hr_data']
                if os.path.exists(csv_path):
                    df = pd.read_csv(csv_path)
                    employees_data = []
                    for emp_id in employee_ids:
                        # EmployeeNumber ì»¬ëŸ¼ìœ¼ë¡œ ê²€ìƒ‰ (CSV íŒŒì¼ì˜ ì‹¤ì œ ì»¬ëŸ¼ëª…)
                        emp_row = df[df['EmployeeNumber'] == int(emp_id)] if 'EmployeeNumber' in df.columns else df[df['employee_id'] == int(emp_id)] if df['employee_id'].dtype != 'object' else df[df['employee_id'] == emp_id]
                        if not emp_row.empty:
                            row_data = emp_row.iloc[0].to_dict()
                            # ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ í•©ì³ì„œ ì‚¬ìš©
                            text_parts = []
                            for col in ['SELF_REVIEW_text', 'PEER_FEEDBACK_text', 'WEEKLY_SURVEY_text', 'text']:
                                if col in row_data and pd.notna(row_data[col]):
                                    text_parts.append(str(row_data[col]))
                            combined_text = ' '.join(text_parts) if text_parts else f"ì§ì› {emp_id}ì˜ ê¸°ë³¸ í…ìŠ¤íŠ¸ ë°ì´í„°"
                            
                            employees_data.append({
                                'employee_id': emp_id,
                                'text_data': combined_text
                            })
                        else:
                            employees_data.append({
                                'employee_id': emp_id,
                                'text_data': f"ì§ì› {emp_id}ì˜ ê¸°ë³¸ í…ìŠ¤íŠ¸ ë°ì´í„°"
                            })
                else:
                    # CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë°ì´í„°ë¡œ êµ¬ì„±
                    employees_data = [{'employee_id': emp_id, 'text_data': f"ì§ì› {emp_id}ì˜ ê¸°ë³¸ í…ìŠ¤íŠ¸ ë°ì´í„°"} for emp_id in employee_ids]
            except Exception as e:
                logger.warning(f"CSV ë°ì´í„° ì½ê¸° ì‹¤íŒ¨: {e}, ê¸°ë³¸ ë°ì´í„° ì‚¬ìš©")
                employees_data = [{'employee_id': emp_id, 'text_data': f"ì§ì› {emp_id}ì˜ ê¸°ë³¸ í…ìŠ¤íŠ¸ ë°ì´í„°"} for emp_id in employee_ids]
        else:
            return jsonify({"error": "employee_id ë˜ëŠ” employees ë°°ì—´ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        # ë°°ì¹˜/ì‚¬í›„ ë¶„ì„ì—ì„œëŠ” LLM ì‚¬ìš© ì•ˆí•¨ (API ë¹„ìš© ì ˆì•½)
        use_llm = analysis_type not in ['batch', 'post']
        
        # ë¶„ì„ íƒ€ì…ì— ë”°ë¥¸ ë°ì´í„° ê²½ë¡œ í™•ì¸ ë° ì¬ë¡œë“œ
        new_data_paths = get_sentio_data_paths(analysis_type)
        current_data_paths = get_sentio_data_paths()
        
        if new_data_paths != current_data_paths:
            print(f"ğŸ”„ Sentio: {analysis_type} ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¬ë¡œë“œ")
            DATA_PATH = new_data_paths
            
            # í‚¤ì›Œë“œ ë¶„ì„ê¸° ì¬ì´ˆê¸°í™” (í•„ìˆ˜ - ì ìˆ˜ ê³„ì‚°ì„ ìœ„í•´ ë°˜ë“œì‹œ í•„ìš”)
            try:
                sample_texts_path = new_data_paths['sample_texts']
                logger.info(f"ğŸ” {analysis_type} ë¶„ì„ìš© í‚¤ì›Œë“œ ë¶„ì„ê¸° ì¬ì´ˆê¸°í™”: {sample_texts_path}")
                
                if sample_texts_path and os.path.exists(sample_texts_path):
                    logger.info(f"ğŸ“ {analysis_type} íŒŒì¼ ì¡´ì¬ í™•ì¸ë¨: {sample_texts_path}")
                    keyword_analyzer = SentioKeywordAnalyzer(sample_texts_path)
                    
                    load_success = keyword_analyzer.load_data()
                    logger.info(f"ğŸ“Š {analysis_type} ë°ì´í„° ë¡œë“œ ê²°ê³¼: {load_success}")
                    
                    if load_success:
                        text_processor = SentioTextProcessor(analyzer=keyword_analyzer)
                        logger.info(f"âœ… Sentio {analysis_type} í‚¤ì›Œë“œ ë¶„ì„ê¸° ì¬ë¡œë“œ ì™„ë£Œ")
                        logger.info(f"ğŸ“ˆ {analysis_type} í‡´ì§ì: {len(keyword_analyzer.resigned_data)}ëª…, ì¬ì§ì: {len(keyword_analyzer.stayed_data)}ëª…")
                    else:
                        logger.error(f"âŒ Sentio {analysis_type} í‚¤ì›Œë“œ ë¶„ì„ê¸° ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ - ì ìˆ˜ ê³„ì‚° ë¶ˆê°€")
                        keyword_analyzer = None
                        text_processor = SentioTextProcessor(analyzer=None)
                else:
                    logger.warning(f"âŒ {analysis_type} ë¶„ì„ìš© í…ìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ ì—†ìŒ: {sample_texts_path}")
                    keyword_analyzer = None
                    text_processor = SentioTextProcessor(analyzer=None)
                    
            except Exception as e:
                logger.error(f"âŒ Sentio {analysis_type} ë°ì´í„° ì¬ë¡œë“œ ì‹¤íŒ¨: {e}")
                import traceback
                logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                keyword_analyzer = None
                text_processor = SentioTextProcessor(analyzer=None)
        
        logger.info(f"ğŸ“Š Sentio {analysis_type} ë¶„ì„ ì‹œì‘ - {len(employees_data)}ëª…")
        
        if not text_processor:
            return jsonify({"error": "í…ìŠ¤íŠ¸ í”„ë¡œì„¸ì„œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 500
        
        # ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥
        analysis_results = []
        
        for emp_data in employees_data:
            employee_id = emp_data.get('employee_id')
            text_data = emp_data.get('text_data', {})
            
            # í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ
            if isinstance(text_data, str):
                # ë‹¨ìˆœ ë¬¸ìì—´ì¸ ê²½ìš°
                combined_text = text_data
            else:
                # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
                self_review = text_data.get('self_review', '')
                peer_feedback = text_data.get('peer_feedback', '')
                weekly_survey = text_data.get('weekly_survey', '')
                combined_text = ' '.join([str(text) for text in [self_review, peer_feedback, weekly_survey] if text])
            
            # í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            if not combined_text or combined_text.strip() == '':
                combined_text = f"ì§ì› {employee_id}ì˜ ê¸°ë³¸ í…ìŠ¤íŠ¸ ë°ì´í„°"
            
            try:
                logger.info(f"ğŸ” ì§ì› {employee_id} í…ìŠ¤íŠ¸ ë¶„ì„ ì‹œì‘ (ê¸¸ì´: {len(combined_text)}ì)")
                # ì‹¤ì œ í…ìŠ¤íŠ¸ ë¶„ì„ ìˆ˜í–‰
                analysis_result = text_processor.analyze_text(
                    text=combined_text,
                    employee_id=employee_id,
                    text_type="comprehensive"
                )
                logger.info(f"ğŸ” ë¶„ì„ ê²°ê³¼ íƒ€ì…: {type(analysis_result)}, ê°’: {analysis_result}")
                
                # analysis_resultê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
                if not isinstance(analysis_result, dict):
                    logger.error(f"âŒ ë¶„ì„ ê²°ê³¼ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤: {type(analysis_result)}")
                    raise ValueError(f"ë¶„ì„ ê²°ê³¼ íƒ€ì… ì˜¤ë¥˜: {type(analysis_result)}")
                
                # ì•ˆì „í•œ ë¡œê¹…
                if isinstance(analysis_result, dict):
                    keywords_count = len(analysis_result.get('keywords', []))
                    risk_level = analysis_result.get('risk_level', 'N/A')
                else:
                    keywords_count = 0
                    risk_level = 'N/A'
                logger.info(f"âœ… ì§ì› {employee_id} ë¶„ì„ ì™„ë£Œ - í‚¤ì›Œë“œ: {keywords_count}ê°œ, ìœ„í—˜ë„: {risk_level}")
                
                # ì•ˆì „í•œ ê°’ ì¶”ì¶œ
                sentiment_score = analysis_result.get('sentiment_score', 0.5) if isinstance(analysis_result, dict) else 0.5
                risk_factors = analysis_result.get('risk_factors', []) if isinstance(analysis_result, dict) else []
                keywords = analysis_result.get('keywords', []) if isinstance(analysis_result, dict) else []
                risk_level = analysis_result.get('risk_level', 'MEDIUM') if isinstance(analysis_result, dict) else 'MEDIUM'
                attrition_risk_score = analysis_result.get('attrition_risk_score', 0.5) if isinstance(analysis_result, dict) else 0.5
                jd_r_indicators = analysis_result.get('jd_r_indicators', {}) if isinstance(analysis_result, dict) else {}
                
                # ê°œë³„ ê²°ê³¼ ìƒì„±
                individual_result = {
                    "employee_id": employee_id,
                    "sentiment_score": sentiment_score,
                    "risk_keywords": risk_factors[:10],
                    "emotional_state": determine_emotional_state(sentiment_score),
                    "confidence_score": min(0.9, max(0.1, len(keywords) / 20)),
                    "text_analysis_summary": f"JD-R ëª¨ë¸ ê¸°ë°˜ ë¶„ì„ - ìœ„í—˜ë„: {risk_level}, í‚¤ì›Œë“œ: {len(keywords)}ê°œ{' (ë¶„ì„ íƒ€ì…: ' + analysis_type + ')' if not use_llm else ''}",
                    "analysis_timestamp": datetime.now().isoformat(),
                    "detailed_analysis": {
                        "attrition_risk_score": attrition_risk_score,
                        "risk_level": risk_level,
                        "keywords_count": len(keywords),
                        "jd_r_indicators": jd_r_indicators
                    },
                    # PostAnalysis.jsì—ì„œ ê¸°ëŒ€í•˜ëŠ” í•„ë“œ ì¶”ê°€
                    "psychological_risk_score": attrition_risk_score
                }
                
                analysis_results.append(individual_result)
                
            except Exception as e:
                logger.warning(f"ì§ì› {employee_id} ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                # ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
                analysis_results.append({
                    "employee_id": employee_id,
                    "sentiment_score": 0.5,
                    "risk_keywords": ["analysis_error"],
                    "emotional_state": "neutral",
                    "confidence_score": 0.1,
                    "text_analysis_summary": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                    "analysis_timestamp": datetime.now().isoformat(),
                    "psychological_risk_score": 0.5
                })
        
        logger.info(f"ğŸ‰ Sentio {analysis_type} ë¶„ì„ ì™„ë£Œ - ì´ {len(analysis_results)}ëª… ì²˜ë¦¬")
        
        # ë‹¨ì¼ ì§ì›ì¸ ê²½ìš° ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
        if len(employees_data) == 1:
            return jsonify(analysis_results[0])
        
        # ë°°ì¹˜ ë¶„ì„ì¸ ê²½ìš° ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
        return jsonify({
            "success": True,
            "analysis_results": analysis_results,
            "total_analyzed": len(analysis_results),
            "analysis_type": analysis_type
        })
        
    except Exception as e:
        # employee_idê°€ ì •ì˜ë˜ì§€ ì•Šì€ ê²½ìš°ë¥¼ ëŒ€ë¹„
        emp_id = locals().get('employee_id', 'unknown')
        logger.error(f"ê°ì • ë¶„ì„ ì˜¤ë¥˜ (ì§ì› {emp_id}): {str(e)}")
        return jsonify({
            "sentiment_score": 0.5,
            "risk_keywords": ["analysis_error"],
            "emotional_state": "neutral",
            "confidence_score": 0.1,
            "text_analysis_summary": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            "analysis_timestamp": datetime.now().isoformat()
        }), 200  # ì˜¤ë¥˜ê°€ ìˆì–´ë„ 200ìœ¼ë¡œ ë°˜í™˜í•˜ì—¬ ì›Œí¬í”Œë¡œìš° ì¤‘ë‹¨ ë°©ì§€

def determine_emotional_state(sentiment_score):
    """ê°ì • ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°ì • ìƒíƒœ ê²°ì •"""
    if sentiment_score >= 0.7:
        return "positive"
    elif sentiment_score >= 0.4:
        return "neutral_positive"
    elif sentiment_score >= 0.3:
        return "neutral"
    else:
        return "negative"

@app.route('/analyze/batch_csv', methods=['POST'])
def generate_batch_csv():
    """
    ëŒ€ëŸ‰ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ CSVë¡œ ë¹ ë¥´ê²Œ ë¶„ì„ (LLM ì—†ì´)
    ì…ë ¥: í…ìŠ¤íŠ¸ ë°ì´í„° ëª©ë¡
    ì¶œë ¥: CSV íŒŒì¼ ê²½ë¡œ ë° ë¶„ì„ í†µê³„
    """
    try:
        data = request.get_json()
        
        if not data or 'text_data_list' not in data:
            return jsonify({"error": "í…ìŠ¤íŠ¸ ë°ì´í„° ëª©ë¡ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        text_data_list = data['text_data_list']
        output_filename = data.get('output_filename', 'sentio_batch_analysis.csv')
        
        if not keyword_analyzer:
            return jsonify({"error": "í‚¤ì›Œë“œ ë¶„ì„ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CSV ë°°ì¹˜ ë¶„ì„ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 500
        
        logger.info(f"ë°°ì¹˜ CSV ë¶„ì„ ì‹œì‘: {len(text_data_list)}ê°œ ë°ì´í„°")
        
        # ëŒ€ëŸ‰ ë¶„ì„ ìˆ˜í–‰ (LLM ì—†ì´)
        start_time = datetime.now()
        df = keyword_analyzer.generate_csv_batch_analysis(text_data_list)
        
        # CSV ì €ì¥
        output_path = keyword_analyzer.save_analysis_to_csv(df, output_filename)
        end_time = datetime.now()
        
        # í†µê³„ ê³„ì‚°
        processing_time = (end_time - start_time).total_seconds()
        
        # ìœ„í—˜ë„ë³„ ë¶„í¬
        risk_distribution = df['risk_level'].value_counts().to_dict()
        
        # í‰ê·  ì ìˆ˜
        avg_scores = {
            'psychological_risk_score': float(df['psychological_risk_score'].mean()),
            'job_demands_score': float(df['job_demands_score'].mean()),
            'job_resources_deficiency_score': float(df['job_resources_deficiency_score'].mean()),
            'sentiment_score': float(df['sentiment_score'].mean())
        }
        
        # ì˜ˆì¸¡ ë¶„í¬
        prediction_distribution = df['attrition_prediction'].value_counts().to_dict()
        
        result = {
            "status": "success",
            "output_file": output_path,
            "processing_stats": {
                "total_processed": len(df),
                "processing_time_seconds": round(processing_time, 2),
                "records_per_second": round(len(df) / processing_time, 2),
                "analysis_timestamp": end_time.isoformat()
            },
            "analysis_summary": {
                "risk_distribution": risk_distribution,
                "average_scores": avg_scores,
                "prediction_distribution": {
                    "will_leave": prediction_distribution.get(1, 0),
                    "will_stay": prediction_distribution.get(0, 0)
                }
            },
            "message": f"âœ… {len(df)}ëª…ì˜ í…ìŠ¤íŠ¸ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (LLM ë¯¸ì‚¬ìš©ìœ¼ë¡œ ë¹ ë¥¸ ì²˜ë¦¬)"
        }
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"ë°°ì¹˜ CSV ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return jsonify({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}), 500

# ============================================================================
# ì˜¤ë¥˜ ì²˜ë¦¬
# ============================================================================

@app.errorhandler(HTTPException)
def handle_http_exception(e):
    """HTTP ì˜ˆì™¸ ì²˜ë¦¬"""
    return jsonify({
        "error": e.description,
        "status_code": e.code
    }), e.code

@app.errorhandler(Exception)
def handle_general_exception(e):
    """ì¼ë°˜ ì˜ˆì™¸ ì²˜ë¦¬"""
    logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
    logger.error(traceback.format_exc())
    
    return jsonify({
        "error": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "message": str(e)
    }), 500

# ============================================================================
# ì•± ì‹¤í–‰
# ============================================================================

if __name__ == '__main__':
    print("ğŸš€ Sentio HR Text Analysis API ì„œë²„ ì‹œì‘...")
    print("=" * 60)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    if initialize_system():
        print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        print("ğŸŒ ì„œë²„ ì£¼ì†Œ: http://localhost:5004")
        print("ğŸ“š API ë¬¸ì„œ: http://localhost:5004/")
        print("=" * 60)
        
        app.run(
            host='0.0.0.0',
            port=5004,
            debug=True,
            threaded=True
        )
    else:
        print("âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        print("ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
