# -*- coding: utf-8 -*-
"""
Sentio HR í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ Flask ë°±ì—”ë“œ ì„œë¹„ìŠ¤
í‚¤ì›Œë“œ ë¶„ì„ + í‡´ì§ ìœ„í—˜ ì‹ í˜¸ íƒì§€ + í…ìŠ¤íŠ¸ ìƒì„± ì‹œìŠ¤í…œ
React ì—°ë™ì— ìµœì í™”ëœ REST API ì„œë²„
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
from werkzeug.exceptions import HTTPException
import logging
import os
import json
import pandas as pd
import numpy as np
from datetime import datetime
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Tuple, Any
from collections import Counter, defaultdict
import re
import traceback

# ë¡œì»¬ ëª¨ë“ˆ import
from sentio_processor import SentioTextProcessor
from sentio_analyzer import SentioKeywordAnalyzer
from sentio_generator import SentioTextGenerator

# Flask ì•± ì´ˆê¸°í™”
app = Flask(__name__)
CORS(app)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜
text_processor = None
keyword_analyzer = None
text_generator = None

# ë°ì´í„° ê²½ë¡œ ì„¤ì •
DATA_PATH = {
    'hr_data': '../../data/IBM_HR.csv',
    'text_data': '../../data/IBM_HR_text.csv',
    'sample_texts': '../../sample_hr_texts.csv'
}

MODEL_PATH = 'app/Sentio/models'
os.makedirs(MODEL_PATH, exist_ok=True)

@dataclass
class SentioAnalysisResult:
    """Sentio ë¶„ì„ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    employee_id: str
    text_type: str
    original_text: str
    keywords: List[str]
    sentiment_score: float
    attrition_risk_score: float
    risk_factors: List[str]
    analysis_timestamp: str

@dataclass
class SentioGenerationResult:
    """Sentio í…ìŠ¤íŠ¸ ìƒì„± ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    employee_id: str
    text_type: str
    generated_text: str
    keywords_used: List[str]
    generation_timestamp: str

def initialize_system():
    """
    Sentio ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    """
    global text_processor, keyword_analyzer, text_generator
    
    try:
        logger.info("Sentio ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
        
        # í‚¤ì›Œë“œ ë¶„ì„ê¸° ì´ˆê¸°í™” (JD-R ëª¨ë¸ í¬í•¨)
        if os.path.exists(DATA_PATH['sample_texts']):
            keyword_analyzer = SentioKeywordAnalyzer(DATA_PATH['sample_texts'])
            keyword_analyzer.load_data()
            logger.info("âœ… í‚¤ì›Œë“œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            logger.warning("âš ï¸ í…ìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            keyword_analyzer = None
        
        # í…ìŠ¤íŠ¸ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” (analyzer ì—°ê²°)
        text_processor = SentioTextProcessor(analyzer=keyword_analyzer)
        logger.info("âœ… í…ìŠ¤íŠ¸ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì™„ë£Œ (JD-R ëª¨ë¸ ì—°ê²°)")
        
        # í…ìŠ¤íŠ¸ ìƒì„±ê¸°ëŠ” í˜ë¥´ì†Œë‚˜ ì •ë³´ ì—†ì´ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œë§Œ ë™ì‘
        api_key = os.environ.get('OPENAI_API_KEY')
        if api_key:
            # í˜ë¥´ì†Œë‚˜ íŒŒì¼ ì—†ì´ í‚¤ì›Œë“œ ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„±ê¸° ì´ˆê¸°í™”
            text_generator = SentioTextGenerator(api_key, None)
            logger.info("âœ… í…ìŠ¤íŠ¸ ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ (í‚¤ì›Œë“œ ê¸°ë°˜)")
        else:
            logger.warning("âš ï¸ OpenAI API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info("ğŸ‰ Sentio ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())
        return False

# ============================================================================
# API ì—”ë“œí¬ì¸íŠ¸
# ============================================================================

@app.route('/')
def home():
    """í™ˆí˜ì´ì§€"""
    return jsonify({
        "service": "Sentio HR Text Analysis API",
        "version": "1.0.0",
        "status": "running",
        "description": "HR í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ ë° í‡´ì§ ìœ„í—˜ ì‹ í˜¸ íƒì§€ ì„œë¹„ìŠ¤",
        "endpoints": {
            "/analyze/text": "í…ìŠ¤íŠ¸ ë¶„ì„ (í‚¤ì›Œë“œ ì¶”ì¶œ + ê°ì • ë¶„ì„)",
            "/analyze/keywords": "í‚¤ì›Œë“œ ë¶„ì„ (í‡´ì§ì vs ì¬ì§ì)",
            "/analyze/risk": "í‡´ì§ ìœ„í—˜ ì‹ í˜¸ ë¶„ì„",
            "/generate/text": "í‚¤ì›Œë“œ ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„±",
            "/health": "ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"
        }
    })

@app.route('/health')
def health_check():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    status = {
        "service": "Sentio",
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "components": {
            "text_processor": text_processor is not None,
            "keyword_analyzer": keyword_analyzer is not None,
            "text_generator": text_generator is not None
        }
    }
    
    # ì „ì²´ ìƒíƒœ í™•ì¸
    all_healthy = all(status["components"].values())
    status["status"] = "healthy" if all_healthy else "degraded"
    
    return jsonify(status)

@app.route('/analyze/text', methods=['POST'])
def analyze_text():
    """
    í…ìŠ¤íŠ¸ ë¶„ì„ API
    ì…ë ¥: í…ìŠ¤íŠ¸, ì§ì›ID (ì„ íƒ)
    ì¶œë ¥: í‚¤ì›Œë“œ, ê°ì •ì ìˆ˜, í‡´ì§ìœ„í—˜ì ìˆ˜
    """
    try:
        data = request.get_json()
        
        if not data or 'text' not in data:
            return jsonify({"error": "í…ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        text = data['text']
        employee_id = data.get('employee_id', 'unknown')
        text_type = data.get('text_type', 'general')
        
        if not text_processor:
            return jsonify({"error": "í…ìŠ¤íŠ¸ í”„ë¡œì„¸ì„œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 500
        
        # í…ìŠ¤íŠ¸ ë¶„ì„ ìˆ˜í–‰
        analysis_result = text_processor.analyze_text(
            text=text,
            employee_id=employee_id,
            text_type=text_type
        )
        
        # ê²°ê³¼ ë°˜í™˜
        result = SentioAnalysisResult(
            employee_id=employee_id,
            text_type=text_type,
            original_text=text,
            keywords=analysis_result['keywords'],
            sentiment_score=analysis_result['sentiment_score'],
            attrition_risk_score=analysis_result['attrition_risk_score'],
            risk_factors=analysis_result['risk_factors'],
            analysis_timestamp=datetime.now().isoformat()
        )
        
        return jsonify(asdict(result))
        
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return jsonify({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}), 500

@app.route('/analyze/keywords', methods=['POST'])
def analyze_keywords():
    """
    í‚¤ì›Œë“œ ë¶„ì„ API
    ì…ë ¥: ë¶„ì„ ì˜µì…˜
    ì¶œë ¥: í‡´ì§ì vs ì¬ì§ì ì°¨ë³„ì  í‚¤ì›Œë“œ
    """
    try:
        data = request.get_json() or {}
        min_frequency = data.get('min_frequency', 5)
        text_columns = data.get('text_columns', None)
        
        if not keyword_analyzer:
            return jsonify({"error": "í‚¤ì›Œë“œ ë¶„ì„ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 500
        
        # í‚¤ì›Œë“œ ë¶„ì„ ìˆ˜í–‰
        results = keyword_analyzer.analyze_text_columns(text_columns)
        if not results:
            return jsonify({"error": "ë¶„ì„í•  í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 400
        
        # ì°¨ë³„ì  í‚¤ì›Œë“œ ì°¾ê¸°
        distinctive_keywords = keyword_analyzer.find_distinctive_keywords(
            results, min_frequency=min_frequency
        )
        
        # ê²°ê³¼ ì •ë¦¬
        analysis_summary = {
            "analysis_timestamp": datetime.now().isoformat(),
            "min_frequency": min_frequency,
            "columns_analyzed": list(results.keys()),
            "distinctive_keywords": distinctive_keywords,
            "summary": {
                col: {
                    "resigned_total_keywords": data['resigned_total'],
                    "stayed_total_keywords": data['stayed_total'],
                    "resigned_unique_count": len(distinctive_keywords[col]['resigned_unique']),
                    "stayed_unique_count": len(distinctive_keywords[col]['stayed_unique'])
                }
                for col, data in results.items()
            }
        }
        
        return jsonify(analysis_summary)
        
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return jsonify({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}), 500

@app.route('/analyze/risk', methods=['POST'])
def analyze_attrition_risk():
    """
    í‡´ì§ ìœ„í—˜ ì‹ í˜¸ ë¶„ì„ API
    ì…ë ¥: ì§ì› í…ìŠ¤íŠ¸ ë°ì´í„°
    ì¶œë ¥: ìœ„í—˜ ì ìˆ˜ ë° ì£¼ìš” ì‹ í˜¸
    """
    try:
        data = request.get_json()
        
        if not data or 'texts' not in data:
            return jsonify({"error": "ë¶„ì„í•  í…ìŠ¤íŠ¸ ëª©ë¡ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        texts = data['texts']  # List of {employee_id, text, text_type}
        
        if not text_processor:
            return jsonify({"error": "í…ìŠ¤íŠ¸ í”„ë¡œì„¸ì„œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 500
        
        risk_analysis_results = []
        
        for text_data in texts:
            employee_id = text_data.get('employee_id', 'unknown')
            text = text_data.get('text', '')
            text_type = text_data.get('text_type', 'general')
            
            if not text:
                continue
            
            # ìœ„í—˜ ì‹ í˜¸ ë¶„ì„
            risk_result = text_processor.analyze_attrition_risk(
                text=text,
                employee_id=employee_id
            )
            
            risk_analysis_results.append({
                "employee_id": employee_id,
                "text_type": text_type,
                "risk_score": risk_result['risk_score'],
                "risk_level": risk_result['risk_level'],
                "risk_factors": risk_result['risk_factors'],
                "keywords_detected": risk_result['keywords_detected']
            })
        
        # ì „ì²´ ìš”ì•½ í†µê³„
        risk_scores = [r['risk_score'] for r in risk_analysis_results]
        summary_stats = {
            "total_analyzed": len(risk_analysis_results),
            "average_risk_score": np.mean(risk_scores) if risk_scores else 0,
            "high_risk_count": len([r for r in risk_analysis_results if r['risk_level'] == 'high']),
            "medium_risk_count": len([r for r in risk_analysis_results if r['risk_level'] == 'medium']),
            "low_risk_count": len([r for r in risk_analysis_results if r['risk_level'] == 'low'])
        }
        
        return jsonify({
            "analysis_timestamp": datetime.now().isoformat(),
            "summary": summary_stats,
            "individual_results": risk_analysis_results
        })
        
    except Exception as e:
        logger.error(f"ìœ„í—˜ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return jsonify({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}), 500

@app.route('/generate/text', methods=['POST'])
def generate_text():
    """
    í‚¤ì›Œë“œ ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„± API
    ì…ë ¥: í…ìŠ¤íŠ¸ íƒ€ì…, í‚¤ì›Œë“œ ëª©ë¡
    ì¶œë ¥: ìƒì„±ëœ í…ìŠ¤íŠ¸
    """
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({"error": "ìš”ì²­ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        if not text_generator:
            return jsonify({"error": "í…ìŠ¤íŠ¸ ìƒì„±ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. OpenAI API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."}), 500
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„±
        if 'keywords' in data:
            keywords = data['keywords']
            text_type = data.get('text_type', 'SELF_REVIEW')
            employee_id = data.get('employee_id', 'unknown')
            
            generated_text = text_generator.generate_text_from_keywords(
                keywords=keywords,
                text_type=text_type
            )
            
            result = SentioGenerationResult(
                employee_id=employee_id,
                text_type=text_type,
                generated_text=generated_text,
                keywords_used=keywords[:5],  # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œë§Œ
                generation_timestamp=datetime.now().isoformat()
            )
            
            return jsonify(asdict(result))
        
        else:
            return jsonify({"error": "keywordsê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return jsonify({"error": f"ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}), 500

@app.route('/data/keywords', methods=['GET'])
def get_available_keywords():
    """ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ì›Œë“œ ëª©ë¡ ì¡°íšŒ API"""
    try:
        if not text_generator:
            return jsonify({"error": "í…ìŠ¤íŠ¸ ìƒì„±ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 500
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ë°˜í™˜
        available_keywords = text_generator.get_all_available_keywords()
        
        return jsonify({
            "keyword_categories": available_keywords,
            "total_categories": len(available_keywords)
        })
        
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return jsonify({"error": f"ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}), 500

@app.route('/analyze/comprehensive_report', methods=['POST'])
def generate_comprehensive_report():
    """
    ê°œë³„ ì§ì›ì˜ ëª¨ë“  ì›Œì»¤ ì—ì´ì „íŠ¸ ê²°ê³¼ë¥¼ ì¢…í•©í•œ ìµœì¢… ë ˆí¬íŠ¸ ìƒì„± API (LLM ì„ íƒì  ì‚¬ìš©)
    ì…ë ¥: í•œ ì§ì›ì˜ ëª¨ë“  ì›Œì»¤ ë¶„ì„ ê²°ê³¼
    ì¶œë ¥: ê°œë³„ ì§ì› ì¢…í•© ë¶„ì„ ë ˆí¬íŠ¸
    """
    try:
        data = request.get_json()
        
        if not data or 'employee_id' not in data or 'worker_results' not in data:
            return jsonify({"error": "ì§ì› IDì™€ ëª¨ë“  ì›Œì»¤ ë¶„ì„ ê²°ê³¼ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        employee_id = data['employee_id']
        worker_results = data['worker_results']  # {structura: {...}, cognita: {...}, chronos: {...}, sentio: {...}}
        use_llm = data.get('use_llm', False)  # LLM ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        
        if not keyword_analyzer:
            return jsonify({"error": "í‚¤ì›Œë“œ ë¶„ì„ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 500
        
        # ì¢…í•© ë ˆí¬íŠ¸ ìƒì„±
        comprehensive_report = keyword_analyzer.generate_individual_comprehensive_report(
            employee_id=employee_id,
            all_worker_results=worker_results,
            use_llm=use_llm
        )
        
        # LLM í•´ì„ ì¶”ê°€ (ì„ íƒì )
        if use_llm:
            llm_interpretation = keyword_analyzer.generate_comprehensive_llm_interpretation(
                comprehensive_report=comprehensive_report,
                use_llm=True
            )
            comprehensive_report['llm_interpretation'] = llm_interpretation
        else:
            # ê·œì¹™ ê¸°ë°˜ í•´ì„
            rule_based_interpretation = keyword_analyzer.generate_comprehensive_llm_interpretation(
                comprehensive_report=comprehensive_report,
                use_llm=False
            )
            comprehensive_report['rule_based_interpretation'] = rule_based_interpretation
        
        return jsonify(comprehensive_report)
        
    except Exception as e:
        logger.error(f"ì¢…í•© ë ˆí¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return jsonify({"error": f"ë ˆí¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}), 500

@app.route('/analyze/batch_csv', methods=['POST'])
def generate_batch_csv():
    """
    ëŒ€ëŸ‰ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ CSVë¡œ ë¹ ë¥´ê²Œ ë¶„ì„ (LLM ì—†ì´)
    ì…ë ¥: í…ìŠ¤íŠ¸ ë°ì´í„° ëª©ë¡
    ì¶œë ¥: CSV íŒŒì¼ ê²½ë¡œ ë° ë¶„ì„ í†µê³„
    """
    try:
        data = request.get_json()
        
        if not data or 'text_data_list' not in data:
            return jsonify({"error": "í…ìŠ¤íŠ¸ ë°ì´í„° ëª©ë¡ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        text_data_list = data['text_data_list']
        output_filename = data.get('output_filename', 'sentio_batch_analysis.csv')
        
        if not keyword_analyzer:
            return jsonify({"error": "í‚¤ì›Œë“œ ë¶„ì„ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 500
        
        logger.info(f"ë°°ì¹˜ CSV ë¶„ì„ ì‹œì‘: {len(text_data_list)}ê°œ ë°ì´í„°")
        
        # ëŒ€ëŸ‰ ë¶„ì„ ìˆ˜í–‰ (LLM ì—†ì´)
        start_time = datetime.now()
        df = keyword_analyzer.generate_csv_batch_analysis(text_data_list)
        
        # CSV ì €ì¥
        output_path = keyword_analyzer.save_analysis_to_csv(df, output_filename)
        end_time = datetime.now()
        
        # í†µê³„ ê³„ì‚°
        processing_time = (end_time - start_time).total_seconds()
        
        # ìœ„í—˜ë„ë³„ ë¶„í¬
        risk_distribution = df['risk_level'].value_counts().to_dict()
        
        # í‰ê·  ì ìˆ˜
        avg_scores = {
            'psychological_risk_score': float(df['psychological_risk_score'].mean()),
            'job_demands_score': float(df['job_demands_score'].mean()),
            'job_resources_deficiency_score': float(df['job_resources_deficiency_score'].mean()),
            'sentiment_score': float(df['sentiment_score'].mean())
        }
        
        # ì˜ˆì¸¡ ë¶„í¬
        prediction_distribution = df['attrition_prediction'].value_counts().to_dict()
        
        result = {
            "status": "success",
            "output_file": output_path,
            "processing_stats": {
                "total_processed": len(df),
                "processing_time_seconds": round(processing_time, 2),
                "records_per_second": round(len(df) / processing_time, 2),
                "analysis_timestamp": end_time.isoformat()
            },
            "analysis_summary": {
                "risk_distribution": risk_distribution,
                "average_scores": avg_scores,
                "prediction_distribution": {
                    "will_leave": prediction_distribution.get(1, 0),
                    "will_stay": prediction_distribution.get(0, 0)
                }
            },
            "message": f"âœ… {len(df)}ëª…ì˜ í…ìŠ¤íŠ¸ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (LLM ë¯¸ì‚¬ìš©ìœ¼ë¡œ ë¹ ë¥¸ ì²˜ë¦¬)"
        }
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"ë°°ì¹˜ CSV ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return jsonify({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}), 500

# ============================================================================
# ì˜¤ë¥˜ ì²˜ë¦¬
# ============================================================================

@app.errorhandler(HTTPException)
def handle_http_exception(e):
    """HTTP ì˜ˆì™¸ ì²˜ë¦¬"""
    return jsonify({
        "error": e.description,
        "status_code": e.code
    }), e.code

@app.errorhandler(Exception)
def handle_general_exception(e):
    """ì¼ë°˜ ì˜ˆì™¸ ì²˜ë¦¬"""
    logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
    logger.error(traceback.format_exc())
    
    return jsonify({
        "error": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "message": str(e)
    }), 500

# ============================================================================
# ì•± ì‹¤í–‰
# ============================================================================

if __name__ == '__main__':
    print("ğŸš€ Sentio HR Text Analysis API ì„œë²„ ì‹œì‘...")
    print("=" * 60)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    if initialize_system():
        print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        print("ğŸŒ ì„œë²„ ì£¼ì†Œ: http://localhost:5004")
        print("ğŸ“š API ë¬¸ì„œ: http://localhost:5004/")
        print("=" * 60)
        
        app.run(
            host='0.0.0.0',
            port=5004,
            debug=True,
            threaded=True
        )
    else:
        print("âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        print("ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
