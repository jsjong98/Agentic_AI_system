"""
Integration Flask ë°±ì—”ë“œ
ì„ê³„ê°’ ì„¤ì •, ê°€ì¤‘ì¹˜ ìµœì í™” ë° LLM ê¸°ë°˜ ë ˆí¬íŠ¸ ìƒì„± API ì„œë²„
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
from werkzeug.utils import secure_filename
import pandas as pd
import numpy as np
import os
import json
from datetime import datetime
import traceback
from typing import Dict, List, Any
from dotenv import load_dotenv

from threshold_calculator import ThresholdCalculator, load_and_process_data
from weight_optimizer import WeightOptimizer
from report_generator import ReportGenerator

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (Sentio/Agoraì™€ ë™ì¼)
load_dotenv()

app = Flask(__name__)
CORS(app)

# ì „ì—­ ë³€ìˆ˜
threshold_calculator = ThresholdCalculator()
weight_optimizer = WeightOptimizer()
report_generator = ReportGenerator()  # í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ìœ¼ë¡œ API í‚¤ ë¡œë“œ ì‹œë„
current_data = None
current_results = {}

# ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'data')
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), 'outputs')

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(OUTPUT_DIR, exist_ok=True)


@app.route('/health', methods=['GET'])
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return jsonify({
        'status': 'healthy',
        'service': 'Integration',
        'timestamp': datetime.now().isoformat(),
        'version': '1.0.0',
        'llm_enabled': hasattr(report_generator, 'llm') and report_generator.llm is not None
    })


@app.route('/set_api_key', methods=['POST'])
def set_api_key():
    """OpenAI API í‚¤ ì„¤ì •"""
    try:
        data = request.get_json()
        api_key = data.get('api_key')
        
        if not api_key:
            return jsonify({
                'success': False,
                'error': 'API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        # ìƒˆë¡œìš´ ReportGenerator ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        global report_generator
        report_generator = ReportGenerator(api_key=api_key)
        
        return jsonify({
            'success': True,
            'message': 'API í‚¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.',
            'llm_enabled': hasattr(report_generator, 'llm') and report_generator.llm is not None
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'API í‚¤ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/load_data', methods=['POST'])
def load_data():
    """ë°ì´í„° ë¡œë“œ"""
    global current_data
    
    try:
        data = request.get_json()
        file_path = data.get('file_path', 'Total_score.csv')
        
        # ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        if not os.path.isabs(file_path):
            file_path = os.path.join(DATA_DIR, file_path)
        
        if not os.path.exists(file_path):
            return jsonify({
                'success': False,
                'error': f'íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}'
            }), 404
        
        # ë°ì´í„° ë¡œë“œ
        current_data, score_columns = load_and_process_data(file_path)
        
        # ê¸°ë³¸ í†µê³„ ê³„ì‚°
        stats = {
            'total_rows': len(current_data),
            'total_columns': len(current_data.columns),
            'score_columns': score_columns,
            'attrition_distribution': current_data['attrition'].value_counts().to_dict() if 'attrition' in current_data.columns else {},
            'missing_values': current_data.isnull().sum().to_dict()
        }
        
        return jsonify({
            'success': True,
            'message': 'ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'file_path': file_path,
            'statistics': stats
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/calculate_thresholds', methods=['POST'])
def calculate_thresholds():
    """ì„ê³„ê°’ ê³„ì‚°"""
    global current_data, current_results
    
    try:
        if current_data is None:
            return jsonify({
                'success': False,
                'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. /load_dataë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.'
            }), 400
        
        data = request.get_json()
        score_columns = data.get('score_columns', None)
        
        # Score ì»¬ëŸ¼ ìë™ ê°ì§€
        if score_columns is None:
            score_columns = [col for col in current_data.columns if col.endswith('_score')]
        
        if not score_columns:
            return jsonify({
                'success': False,
                'error': 'Score ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }), 400
        
        # ìµœì í™” ë°©ë²• ì„¤ì • (ê¸°ë³¸ê°’: Bayesian Optimization)
        optimization_method = data.get('method', 'bayesian')
        
        # ì„ê³„ê°’ ê³„ì‚°
        results = threshold_calculator.calculate_thresholds_for_scores(
            current_data, score_columns, method=optimization_method
        )
        
        # ìš”ì•½ í…Œì´ë¸” ìƒì„±
        summary_df = threshold_calculator.get_summary_table()
        thresholds_dict = threshold_calculator.get_thresholds_dict()
        
        # ì˜ˆì¸¡ ì»¬ëŸ¼ ì¶”ê°€
        data_with_predictions = threshold_calculator.apply_thresholds_to_data(current_data, score_columns)
        
        # ê²°ê³¼ ì €ì¥
        current_results['threshold_results'] = results
        current_results['threshold_summary'] = summary_df.to_dict('records')
        current_results['thresholds_dict'] = thresholds_dict
        current_results['data_with_predictions'] = data_with_predictions
        
        # íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ìš”ì•½ ê²°ê³¼ ì €ì¥
        summary_file = os.path.join(OUTPUT_DIR, f'threshold_summary_{timestamp}.csv')
        summary_df.to_csv(summary_file, index=False, encoding='utf-8')
        
        # ì˜ˆì¸¡ ë°ì´í„° ì €ì¥
        predictions_file = os.path.join(OUTPUT_DIR, f'data_with_predictions_{timestamp}.csv')
        data_with_predictions.to_csv(predictions_file, index=False, encoding='utf-8')
        
        return jsonify({
            'success': True,
            'message': 'ì„ê³„ê°’ ê³„ì‚°ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'results': {
                'summary': current_results['threshold_summary'],
                'thresholds': thresholds_dict,
                'best_score': summary_df.loc[summary_df['F1_Score'].idxmax()].to_dict(),
                'total_predictions': len(data_with_predictions)
            },
            'files': {
                'summary': summary_file,
                'predictions': predictions_file
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ì„ê³„ê°’ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/optimize_weights', methods=['POST'])
def optimize_weights():
    """ê°€ì¤‘ì¹˜ ìµœì í™”"""
    global current_data, current_results
    
    try:
        if current_data is None:
            return jsonify({
                'success': False,
                'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. /load_dataë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.'
            }), 400
        
        # ì˜ˆì¸¡ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        data_to_use = current_results.get('data_with_predictions', current_data)
        prediction_cols = [col for col in data_to_use.columns if col.endswith('_prediction')]
        
        if not prediction_cols:
            return jsonify({
                'success': False,
                'error': 'ì˜ˆì¸¡ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. /calculate_thresholdsë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.'
            }), 400
        
        data = request.get_json()
        method = data.get('method', 'bayesian')  # 'grid', 'bayesian', 'scipy'
        
        # ë°©ë²•ë³„ íŒŒë¼ë¯¸í„°
        method_params = {}
        if method == 'grid':
            method_params['n_points_per_dim'] = data.get('n_points_per_dim', 5)
        elif method == 'bayesian':
            method_params['n_calls'] = data.get('n_calls', 50)  # ë² ì´ì§€ì•ˆ ìµœì í™” 50íšŒë¡œ ì„¤ì •
        
        # ê°€ì¤‘ì¹˜ ìµœì í™” ì‹¤í–‰
        optimization_results = weight_optimizer.optimize_weights(
            data_to_use, method=method, **method_params
        )
        
        if not optimization_results.get('best_weights'):
            return jsonify({
                'success': False,
                'error': 'ê°€ì¤‘ì¹˜ ìµœì í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.',
                'details': optimization_results.get('error', 'Unknown error')
            }), 500
        
        # ìµœì  ê°€ì¤‘ì¹˜ ì ìš©
        data_with_weighted = weight_optimizer.apply_optimal_weights(data_to_use)
        data_with_risk = weight_optimizer.classify_risk_level(data_with_weighted)
        
        # ì„±ëŠ¥ ìš”ì•½
        performance_summary = weight_optimizer.get_performance_summary(data_with_risk)
        
        # ê²°ê³¼ ì €ì¥
        current_results['weight_optimization'] = optimization_results
        current_results['final_data'] = data_with_risk
        current_results['performance_summary'] = performance_summary
        
        # íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ìµœì¢… ë°ì´í„° ì €ì¥
        final_data_file = os.path.join(OUTPUT_DIR, f'final_weighted_predictions_{timestamp}.csv')
        data_with_risk.to_csv(final_data_file, index=False, encoding='utf-8')
        
        # ê°€ì¤‘ì¹˜ ì •ë³´ ì €ì¥
        weights_info = pd.DataFrame([
            {'Variable': col, 'Weight': weight} 
            for col, weight in optimization_results['best_weights'].items()
        ])
        weights_info['Method'] = method
        weights_info['F1_Score'] = optimization_results['best_f1']
        weights_info['Threshold'] = optimization_results['best_threshold']
        
        weights_file = os.path.join(OUTPUT_DIR, f'optimal_weights_{timestamp}.csv')
        weights_info.to_csv(weights_file, index=False, encoding='utf-8')
        
        # ìœ„í—˜ë„ ê¸°ì¤€ ì •ë³´ ì €ì¥
        risk_criteria = pd.DataFrame([
            {'Risk_Level': 'ì•ˆì „êµ°', 'Score_Range': '0.0 ~ 0.3', 'Numeric_Code': 1},
            {'Risk_Level': 'ì£¼ì˜êµ°', 'Score_Range': '0.3 ~ 0.7', 'Numeric_Code': 2},
            {'Risk_Level': 'ê³ ìœ„í—˜êµ°', 'Score_Range': '0.7 ~ 1.0', 'Numeric_Code': 3}
        ])
        
        risk_criteria_file = os.path.join(OUTPUT_DIR, f'risk_criteria_{timestamp}.csv')
        risk_criteria.to_csv(risk_criteria_file, index=False, encoding='utf-8')
        
        return jsonify({
            'success': True,
            'message': 'ê°€ì¤‘ì¹˜ ìµœì í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'results': {
                'method': method,
                'optimal_weights': optimization_results['best_weights'],
                'optimal_threshold': optimization_results['best_threshold'],
                'best_f1_score': optimization_results['best_f1'],
                'performance_metrics': performance_summary['performance_metrics'],
                'risk_statistics': performance_summary['risk_statistics'],
                'total_records': len(data_with_risk)
            },
            'files': {
                'final_data': final_data_file,
                'weights_info': weights_file,
                'risk_criteria': risk_criteria_file
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ê°€ì¤‘ì¹˜ ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/predict_employee', methods=['POST'])
def predict_employee():
    """ê°œë³„ ì§ì› ì˜ˆì¸¡"""
    try:
        data = request.get_json()
        employee_scores = data.get('scores', {})
        
        if not employee_scores:
            return jsonify({
                'success': False,
                'error': 'ì§ì› ì ìˆ˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        results = {}
        
        # ì„ê³„ê°’ ê¸°ë°˜ ì˜ˆì¸¡
        if threshold_calculator.optimal_thresholds:
            threshold_predictions = threshold_calculator.predict_attrition(employee_scores)
            results['threshold_predictions'] = threshold_predictions
        
        # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì˜ˆì¸¡
        if weight_optimizer.optimal_weights:
            # ì˜ˆì¸¡ ì»¬ëŸ¼ ìƒì„± (ì„ê³„ê°’ ê¸°ë°˜)
            prediction_data = {}
            for score_name, score_value in employee_scores.items():
                if score_name in threshold_calculator.optimal_thresholds:
                    threshold = threshold_calculator.optimal_thresholds[score_name]
                    prediction_col = f"{score_name}_prediction"
                    prediction_data[prediction_col] = 1 if score_value >= threshold else 0
            
            if prediction_data:
                # ê°€ì¤‘ ì ìˆ˜ ê³„ì‚°
                weighted_score = sum(
                    prediction_data[col] * weight 
                    for col, weight in weight_optimizer.optimal_weights.items()
                    if col in prediction_data
                )
                
                # ìµœì¢… ì˜ˆì¸¡
                final_prediction = 1 if weighted_score >= weight_optimizer.optimal_threshold else 0
                
                # ìœ„í—˜ë„ ë¶„ë¥˜
                if weighted_score < 0.3:
                    risk_level = 'ì•ˆì „êµ°'
                    risk_numeric = 1
                elif weighted_score < 0.7:
                    risk_level = 'ì£¼ì˜êµ°'
                    risk_numeric = 2
                else:
                    risk_level = 'ê³ ìœ„í—˜êµ°'
                    risk_numeric = 3
                
                results['weighted_prediction'] = {
                    'weighted_score': weighted_score,
                    'final_prediction': final_prediction,
                    'prediction_label': 'ìœ„í—˜' if final_prediction == 1 else 'ì•ˆì „',
                    'risk_level': risk_level,
                    'risk_numeric': risk_numeric,
                    'threshold_used': weight_optimizer.optimal_threshold
                }
        
        if not results:
            return jsonify({
                'success': False,
                'error': 'ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ì„ê³„ê°’ ê³„ì‚° ë° ê°€ì¤‘ì¹˜ ìµœì í™”ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”.'
            }), 400
        
        return jsonify({
            'success': True,
            'employee_scores': employee_scores,
            'predictions': results
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/get_results', methods=['GET'])
def get_results():
    """í˜„ì¬ ê²°ê³¼ ì¡°íšŒ"""
    try:
        if not current_results:
            return jsonify({
                'success': False,
                'error': 'ê³„ì‚°ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 404
        
        # ë¯¼ê°í•œ ë°ì´í„° ì œì™¸í•˜ê³  ìš”ì•½ ì •ë³´ë§Œ ë°˜í™˜
        summary = {
            'has_threshold_results': 'threshold_results' in current_results,
            'has_weight_optimization': 'weight_optimization' in current_results,
            'has_final_data': 'final_data' in current_results
        }
        
        if 'threshold_summary' in current_results:
            summary['threshold_summary'] = current_results['threshold_summary']
            summary['thresholds_dict'] = current_results['thresholds_dict']
        
        if 'performance_summary' in current_results:
            summary['performance_summary'] = current_results['performance_summary']
        
        return jsonify({
            'success': True,
            'results': summary
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/compare_methods', methods=['POST'])
def compare_methods():
    """ì—¬ëŸ¬ ìµœì í™” ë°©ë²• ë¹„êµ"""
    global current_data, current_results
    
    try:
        if current_data is None:
            return jsonify({
                'success': False,
                'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'
            }), 400
        
        data_to_use = current_results.get('data_with_predictions', current_data)
        prediction_cols = [col for col in data_to_use.columns if col.endswith('_prediction')]
        
        if not prediction_cols:
            return jsonify({
                'success': False,
                'error': 'ì˜ˆì¸¡ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. /calculate_thresholdsë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.'
            }), 400
        
        data = request.get_json()
        methods = data.get('methods', ['grid', 'scipy'])  # bayesianì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ
        
        comparison_results = []
        
        for method in methods:
            try:
                print(f"=== {method} ë°©ë²• í…ŒìŠ¤íŠ¸ ì¤‘ ===")
                
                # ìƒˆë¡œìš´ optimizer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                temp_optimizer = WeightOptimizer()
                
                # ë°©ë²•ë³„ íŒŒë¼ë¯¸í„° ì„¤ì •
                method_params = {}
                if method == 'grid':
                    method_params['n_points_per_dim'] = 3  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¤„ì„
                elif method == 'bayesian':
                    method_params['n_calls'] = 50  # ë² ì´ì§€ì•ˆ ìµœì í™” 50íšŒ
                
                # ìµœì í™” ì‹¤í–‰
                result = temp_optimizer.optimize_weights(
                    data_to_use, method=method, **method_params
                )
                
                if result.get('best_weights'):
                    # ì„±ëŠ¥ í‰ê°€
                    data_temp = temp_optimizer.apply_optimal_weights(data_to_use)
                    performance = temp_optimizer.get_performance_summary(data_temp)
                    
                    comparison_results.append({
                        'method': method,
                        'best_weights': result['best_weights'],
                        'best_f1_score': result['best_f1'],
                        'best_threshold': result['best_threshold'],
                        'performance_metrics': performance['performance_metrics'],
                        'success': True
                    })
                else:
                    comparison_results.append({
                        'method': method,
                        'success': False,
                        'error': result.get('error', 'Unknown error')
                    })
                    
            except Exception as method_error:
                comparison_results.append({
                    'method': method,
                    'success': False,
                    'error': str(method_error)
                })
        
        # ìµœê³  ì„±ëŠ¥ ë°©ë²• ì°¾ê¸°
        successful_results = [r for r in comparison_results if r.get('success')]
        best_method = None
        
        if successful_results:
            best_method = max(successful_results, key=lambda x: x['best_f1_score'])
        
        return jsonify({
            'success': True,
            'message': 'ë°©ë²• ë¹„êµê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'comparison_results': comparison_results,
            'best_method': best_method,
            'total_methods_tested': len(methods),
            'successful_methods': len(successful_results)
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ë°©ë²• ë¹„êµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/export_results', methods=['POST'])
def export_results():
    """ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"""
    try:
        if not current_results:
            return jsonify({
                'success': False,
                'error': 'ë‚´ë³´ë‚¼ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 404
        
        data = request.get_json()
        export_format = data.get('format', 'csv')  # 'csv', 'json'
        include_data = data.get('include_data', True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        exported_files = []
        
        if export_format == 'csv':
            # ì„ê³„ê°’ ìš”ì•½
            if 'threshold_summary' in current_results:
                threshold_df = pd.DataFrame(current_results['threshold_summary'])
                threshold_file = os.path.join(OUTPUT_DIR, f'threshold_export_{timestamp}.csv')
                threshold_df.to_csv(threshold_file, index=False, encoding='utf-8')
                exported_files.append(threshold_file)
            
            # ìµœì¢… ë°ì´í„°
            if include_data and 'final_data' in current_results:
                final_data_file = os.path.join(OUTPUT_DIR, f'final_data_export_{timestamp}.csv')
                current_results['final_data'].to_csv(final_data_file, index=False, encoding='utf-8')
                exported_files.append(final_data_file)
        
        elif export_format == 'json':
            # JSON í˜•íƒœë¡œ ê²°ê³¼ ì €ì¥
            export_data = {
                'timestamp': timestamp,
                'threshold_results': current_results.get('threshold_summary', []),
                'thresholds_dict': current_results.get('thresholds_dict', {}),
                'performance_summary': current_results.get('performance_summary', {})
            }
            
            if include_data and 'final_data' in current_results:
                export_data['final_data'] = current_results['final_data'].to_dict('records')
            
            json_file = os.path.join(OUTPUT_DIR, f'results_export_{timestamp}.json')
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2, default=str)
            exported_files.append(json_file)
        
        return jsonify({
            'success': True,
            'message': 'ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ë‚´ë³´ë‚´ì¡ŒìŠµë‹ˆë‹¤.',
            'exported_files': exported_files,
            'format': export_format,
            'timestamp': timestamp
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/generate_report', methods=['POST'])
def generate_report():
    """ê°œë³„ ì§ì› ë ˆí¬íŠ¸ ìƒì„±"""
    try:
        data = request.get_json()
        employee_id = data.get('employee_id')
        agent_scores = data.get('agent_scores', {})
        format_type = data.get('format', 'json')  # 'json', 'text', 'both'
        save_file = data.get('save_file', False)
        use_llm = data.get('use_llm', True)  # LLM ì‚¬ìš© ì—¬ë¶€
        
        if not employee_id:
            return jsonify({
                'success': False,
                'error': 'ì§ì› IDê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        if not agent_scores:
            return jsonify({
                'success': False,
                'error': 'ì—ì´ì „íŠ¸ ì ìˆ˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        # ì ìˆ˜ ì„¤ì •
        report_generator.set_agent_scores(employee_id, agent_scores)
        
        # ë ˆí¬íŠ¸ ìƒì„±
        if format_type == 'text':
            report_content = report_generator.generate_text_report(employee_id, use_llm)
            result = {
                'success': True,
                'employee_id': employee_id,
                'format': 'text',
                'report': report_content,
                'llm_used': use_llm and hasattr(report_generator, 'llm') and report_generator.llm is not None
            }
        else:
            report_content = report_generator.generate_employee_report(employee_id, use_llm)
            result = {
                'success': True,
                'employee_id': employee_id,
                'format': 'json',
                'report': report_content,
                'llm_used': use_llm and hasattr(report_generator, 'llm') and report_generator.llm is not None
            }
        
        # íŒŒì¼ ì €ì¥ ì˜µì…˜
        if save_file:
            saved_files = report_generator.save_report(
                employee_id, 
                os.path.join(OUTPUT_DIR, 'reports'), 
                format_type
            )
            
            if 'error' in saved_files:
                result['file_save_error'] = saved_files['error']
            else:
                result['saved_files'] = saved_files
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ë ˆí¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/generate_batch_reports', methods=['POST'])
def generate_batch_reports():
    """ì—¬ëŸ¬ ì§ì›ì˜ ë ˆí¬íŠ¸ ì¼ê´„ ìƒì„±"""
    try:
        data = request.get_json()
        employees_data = data.get('employees', [])  # [{'employee_id': 'EMP001', 'agent_scores': {...}}, ...]
        
        if not employees_data:
            return jsonify({
                'success': False,
                'error': 'ì§ì› ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        # ê° ì§ì›ì˜ ì ìˆ˜ ì„¤ì •
        employee_ids = []
        for emp_data in employees_data:
            employee_id = emp_data.get('employee_id')
            agent_scores = emp_data.get('agent_scores', {})
            
            if employee_id and agent_scores:
                report_generator.set_agent_scores(employee_id, agent_scores)
                employee_ids.append(employee_id)
        
        if not employee_ids:
            return jsonify({
                'success': False,
                'error': 'ìœ íš¨í•œ ì§ì› ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 400
        
        # ì¼ê´„ ë ˆí¬íŠ¸ ìƒì„±
        batch_results = report_generator.generate_batch_reports(
            employee_ids, 
            os.path.join(OUTPUT_DIR, 'reports')
        )
        
        return jsonify({
            'success': True,
            'message': 'ì¼ê´„ ë ˆí¬íŠ¸ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'results': batch_results
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ì¼ê´„ ë ˆí¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/load_employee_data', methods=['POST'])
def load_employee_data():
    """ì§ì› ê¸°ë³¸ ë°ì´í„° ë¡œë“œ (ë ˆí¬íŠ¸ ìƒì„±ìš©)"""
    try:
        data = request.get_json()
        file_path = data.get('file_path')
        
        if not file_path:
            return jsonify({
                'success': False,
                'error': 'íŒŒì¼ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        # ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        if not os.path.isabs(file_path):
            file_path = os.path.join(DATA_DIR, file_path)
        
        # ë°ì´í„° ë¡œë“œ
        success = report_generator.load_employee_data(file_path)
        
        if success:
            return jsonify({
                'success': True,
                'message': 'ì§ì› ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.',
                'file_path': file_path,
                'total_employees': len(report_generator.employee_data) if report_generator.employee_data is not None else 0
            })
        else:
            return jsonify({
                'success': False,
                'error': 'ì§ì› ë°ì´í„° ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'
            }), 500
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ì§ì› ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/upload/employee_data', methods=['POST'])
def upload_employee_data():
    """ì§ì› ë°ì´í„° CSV íŒŒì¼ ì—…ë¡œë“œ"""
    try:
        # íŒŒì¼ í™•ì¸
        if 'file' not in request.files:
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        if not file.filename.lower().endswith('.csv'):
            return jsonify({
                "success": False,
                "error": "CSV íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            }), 400
        
        # íŒŒì¼ ì €ì¥
        filename = secure_filename(file.filename)
        upload_dir = os.path.join(os.path.dirname(__file__), '..', 'uploads', 'Integration')
        os.makedirs(upload_dir, exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•œ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥ (ê¸°ì¡´ íŒŒì¼ ë³´ì¡´)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = os.path.splitext(filename)[0]
        new_filename = f"{base_name}_{timestamp}.csv"
        file_path = os.path.join(upload_dir, new_filename)
        
        # ìµœì‹  íŒŒì¼ ë§í¬ë„ ìƒì„±
        latest_link = os.path.join(upload_dir, 'latest_employee_data.csv')
        file.save(file_path)
        
        # ìµœì‹  íŒŒì¼ ë§í¬ ìƒì„±
        try:
            if os.path.exists(latest_link):
                os.remove(latest_link)
            import shutil
            shutil.copy2(file_path, latest_link)
        except Exception as e:
            print(f"ìµœì‹  íŒŒì¼ ë§í¬ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # ë°ì´í„° ê²€ì¦ ë° ë¡œë“œ
        try:
            df = pd.read_csv(file_path)
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_columns = ['employee_id']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                return jsonify({
                    "success": False,
                    "error": f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing_columns)}",
                    "required_columns": required_columns,
                    "found_columns": list(df.columns)
                }), 400
            
            # ReportGeneratorì— ë°ì´í„° ë¡œë“œ
            success = report_generator.load_employee_data(file_path)
            
            if success:
                # ë°ì´í„° í†µê³„
                employee_stats = {
                    "total_employees": len(df),
                    "unique_employees": df['employee_id'].nunique(),
                    "columns": len(df.columns),
                    "column_names": list(df.columns)
                }
                
                # ë¶€ì„œë³„ í†µê³„ (ë¶€ì„œ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)
                if 'Department' in df.columns:
                    employee_stats["departments"] = df['Department'].value_counts().to_dict()
                
                return jsonify({
                    "success": True,
                    "message": "ì§ì› ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ê³  ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.",
                    "file_info": {
                        "original_filename": filename,
                        "saved_filename": new_filename,
                        "upload_path": upload_dir,
                        "file_path": file_path,
                        "latest_link": latest_link
                    },
                    "employee_stats": employee_stats,
                    "note": "ì´ì œ ì—ì´ì „íŠ¸ ì ìˆ˜ë¥¼ ì„¤ì •í•˜ê³  ë ˆí¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                })
            else:
                return jsonify({
                    "success": False,
                    "error": "ë°ì´í„° ì—…ë¡œë“œëŠ” ì„±ê³µí–ˆì§€ë§Œ ì‹œìŠ¤í…œ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                }), 500
                
        except Exception as e:
            return jsonify({
                "success": False,
                "error": f"ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"
            }), 400
            
    except Exception as e:
        return jsonify({
            "success": False,
            "error": f"íŒŒì¼ ì—…ë¡œë“œ ì˜¤ë¥˜: {str(e)}"
        }), 500


@app.route('/api/post-analysis/bayesian-optimization', methods=['POST'])
def bayesian_optimization():
    """ì‚¬í›„ ë¶„ì„ìš© ë² ì´ì§€ì•ˆ ìµœì í™” (PostAnalysis.js ì „ìš©)"""
    global current_data, current_results
    
    try:
        data = request.get_json()
        print(f"ğŸ”§ ë² ì´ì§€ì•ˆ ìµœì í™” ìš”ì²­ ë°›ìŒ")
        print(f"ğŸ“Š ìš”ì²­ ë°ì´í„° í‚¤: {list(data.keys()) if data else 'None'}")
        
        agent_results = data.get('agent_results', {})
        optimization_config = data.get('optimization_config', {})
        
        print(f"ğŸ“Š agent_results í‚¤: {list(agent_results.keys()) if agent_results else 'None'}")
        print(f"ğŸ“Š optimization_config: {optimization_config}")
        
        if not agent_results:
            return jsonify({
                'success': False,
                'error': 'ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        # ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ì—ì„œ ì§ì ‘ ë°ì´í„° ìƒì„± (Total_score.csv ë¶ˆí•„ìš”)
        print("âœ… ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ë² ì´ì§€ì•ˆ ìµœì í™” ìˆ˜í–‰")
        
        # ì„ê³„ê°’ì´ ê³„ì‚°ë˜ì§€ ì•Šì€ ê²½ìš° ìë™ ê³„ì‚°
        if 'threshold_results' not in current_results:
            try:
                # ê¸°ë³¸ ì„ê³„ê°’ ì„¤ì •
                current_results['threshold_results'] = {
                    'structura_threshold': 0.5,
                    'cognita_threshold': 0.5,
                    'chronos_threshold': 0.5,
                    'sentio_threshold': 0.5,
                    'agora_threshold': 0.5,
                    'high_risk_threshold': 0.7,
                    'medium_risk_threshold': 0.4
                }
                print("âœ… ê¸°ë³¸ ì„ê³„ê°’ ì„¤ì • ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ ì„ê³„ê°’ ì„¤ì • ì‹¤íŒ¨: {str(e)}")
                return jsonify({
                    'success': False,
                    'error': f'ì„ê³„ê°’ ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}'
                }), 500
        
        # ë² ì´ì§€ì•ˆ ìµœì í™” ì„¤ì •
        n_trials = optimization_config.get('n_trials', 50)  # ë² ì´ì§€ì•ˆ ìµœì í™” 50íšŒë¡œ ì„¤ì •
        optimization_target = optimization_config.get('optimization_target', 'f1_score')
        
        # ì—ì´ì „íŠ¸ ê²°ê³¼ì—ì„œ ì‹¤ì œ ì˜ˆì¸¡ ë°ì´í„° ì¶”ì¶œ
        print("ğŸ”§ ì‹¤ì œ ë² ì´ì§€ì•ˆ ìµœì í™” ì‹œì‘...")
        print(f"ğŸ“Š ì—ì´ì „íŠ¸ ê²°ê³¼ ë¶„ì„: {list(agent_results.keys())}")
        
        # 1. ì—ì´ì „íŠ¸ë³„ ì˜ˆì¸¡ ê²°ê³¼ ì¶”ì¶œ (0~1 ì‚¬ì´ ê°’)
        agent_predictions = {}
        actual_labels = []
        employee_ids = []
        
        for agent_name, result in agent_results.items():
            # ì˜ˆì¸¡ ê²°ê³¼ ì¶”ì¶œ (PostAnalysis.js êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
            predictions = None
            
            # PostAnalysis.jsì—ì„œ ìƒì„±í•˜ëŠ” êµ¬ì¡°: result.raw_result.data.predictions
            if result.get('raw_result', {}).get('data', {}).get('predictions'):
                predictions = result['raw_result']['data']['predictions']
                print(f"   - {agent_name}: raw_result.data.predictionsì—ì„œ ë°œê²¬")
            elif result.get('predictions'):
                predictions = result['predictions']
                print(f"   - {agent_name}: predictionsì—ì„œ ë°œê²¬")
            elif result.get('data', {}).get('predictions'):
                predictions = result['data']['predictions']
                print(f"   - {agent_name}: data.predictionsì—ì„œ ë°œê²¬")
            else:
                print(f"   - {agent_name}: ì˜ˆì¸¡ ë°ì´í„° êµ¬ì¡° í™•ì¸")
                print(f"     result í‚¤: {list(result.keys()) if isinstance(result, dict) else 'not dict'}")
                if result.get('raw_result'):
                    print(f"     raw_result í‚¤: {list(result['raw_result'].keys()) if isinstance(result['raw_result'], dict) else 'not dict'}")
                    if result['raw_result'].get('data'):
                        print(f"     raw_result.data í‚¤: {list(result['raw_result']['data'].keys()) if isinstance(result['raw_result']['data'], dict) else 'not dict'}")
            
            if predictions:
                print(f"   - {agent_name}: {len(predictions)}ê°œ ì˜ˆì¸¡ ê²°ê³¼")
                
                # ì²« ë²ˆì§¸ ì—ì´ì „íŠ¸ì—ì„œ employee_idì™€ actual_attrition ì¶”ì¶œ (ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš©)
                if not employee_ids:
                    employee_ids = [pred['employee_id'] for pred in predictions]
                    # actual_attritionì´ ìˆë‹¤ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë°˜í™˜
                    if predictions and len(predictions) > 0 and 'actual_attrition' in predictions[0]:
                        actual_labels = [pred['actual_attrition'] for pred in predictions]
                        print(f"âœ… ì‹¤ì œ ë¼ë²¨ ì‚¬ìš©: {sum(actual_labels)}/{len(actual_labels)} ì´íƒˆ")
                    else:
                        return jsonify({
                            'success': False,
                            'error': 'ì‹¤ì œ ì´íƒˆ ë¼ë²¨(actual_attrition)ì´ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.'
                        }), 400
                
                # ì—ì´ì „íŠ¸ë³„ ìœ„í—˜ë„ ì ìˆ˜ (0~1)
                agent_predictions[agent_name] = [pred['risk_score'] for pred in predictions]
            else:
                print(f"   âš ï¸ {agent_name}: ì˜ˆì¸¡ ê²°ê³¼ ì—†ìŒ")
        
        if not agent_predictions:
            return jsonify({
                'success': False,
                'error': 'ì—ì´ì „íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 1ë‹¨ê³„ ì—ì´ì „íŠ¸ ë¶„ì„ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”.'
            }), 400
        
        print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(employee_ids)}ëª…, {len(agent_predictions)}ê°œ ì—ì´ì „íŠ¸")
        
        # ì—ì´ì „íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ Total_score.csv í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        optimization_data = []
        for i, emp_id in enumerate(employee_ids):
            row = {'employee_id': emp_id}
            
            # Total_score.csv ì»¬ëŸ¼ëª…ì— ë§ê²Œ ë³€í™˜ (ëŒ€ë¬¸ì ì‹œì‘)
            agent_name_mapping = {
                'structura': 'Structura_score',
                'cognita': 'Cognita_score', 
                'chronos': 'Chronos_score',
                'sentio': 'Sentio_score',
                'agora': 'Agora_score'
            }
            
            # ê° ì—ì´ì „íŠ¸ì˜ ì˜ˆì¸¡ ì ìˆ˜ ì¶”ê°€ (Total_score.csv í˜•ì‹)
            for agent_name, predictions in agent_predictions.items():
                if i < len(predictions):
                    column_name = agent_name_mapping.get(agent_name.lower(), f'{agent_name}_score')
                    row[column_name] = predictions[i]
            
            # ëˆ„ë½ëœ ì—ì´ì „íŠ¸ ì ìˆ˜ëŠ” ê¸°ë³¸ê°’ 0.5ë¡œ ì„¤ì •
            for column_name in agent_name_mapping.values():
                if column_name not in row:
                    row[column_name] = 0.5
            
            # ì‹¤ì œ ë¼ë²¨ì„ Total_score.csv í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (Yes/No)
            if i < len(actual_labels):
                row['attrition'] = 'Yes' if actual_labels[i] == 1 else 'No'
            else:
                row['attrition'] = 'No'  # ê¸°ë³¸ê°’
            
            optimization_data.append(row)
        
        # DataFrameìœ¼ë¡œ ë³€í™˜
        current_data = pd.DataFrame(optimization_data)
        print(f"ğŸ“Š Total_score.csv í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(current_data)}í–‰, {len(current_data.columns)}ì—´")
        print(f"ğŸ“Š ì»¬ëŸ¼: {list(current_data.columns)}")
        print(f"ğŸ“Š ìƒ˜í”Œ ë°ì´í„°:")
        print(current_data.head(3).to_string())
        
        # 2. ë² ì´ì§€ì•ˆ ìµœì í™”ë¥¼ ìœ„í•œ ëª©ì  í•¨ìˆ˜ ì •ì˜
        from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score
        
        def objective_function(weights):
            """ê°€ì¤‘ì¹˜ ì¡°í•©ì˜ F1-Score ê³„ì‚°"""
            # ê°€ì¤‘ì¹˜ ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡)
            weights = np.array(weights)
            weights = weights / weights.sum()
            
            # ì•™ìƒë¸” ì˜ˆì¸¡ ê³„ì‚°
            ensemble_scores = np.zeros(len(employee_ids))
            agent_names = list(agent_predictions.keys())
            
            for i, agent_name in enumerate(agent_names):
                if i < len(weights):  # ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì—ì´ì „íŠ¸ë§Œ
                    ensemble_scores += np.array(agent_predictions[agent_name]) * weights[i]
            
            # ìµœì  ì„ê³„ê°’ ì°¾ê¸° (ROC ê³¡ì„  ê¸°ë°˜)
            from sklearn.metrics import roc_curve
            fpr, tpr, thresholds = roc_curve(actual_labels, ensemble_scores)
            
            # Youden's J statisticìœ¼ë¡œ ìµœì  ì„ê³„ê°’ ì°¾ê¸°
            j_scores = tpr - fpr
            best_threshold_idx = np.argmax(j_scores)
            optimal_threshold = thresholds[best_threshold_idx]
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            predictions = (ensemble_scores >= optimal_threshold).astype(int)
            
            # F1-Score ê³„ì‚° (ìµœì í™” ëª©í‘œ)
            f1 = f1_score(actual_labels, predictions)
            
            return -f1  # ìµœì†Œí™” ë¬¸ì œë¡œ ë³€í™˜ (ìŒìˆ˜)
        
        # 3. ë² ì´ì§€ì•ˆ ìµœì í™” ì‹¤í–‰
        try:
            from skopt import gp_minimize
            from skopt.space import Real
            
            # ê°€ì¤‘ì¹˜ ê³µê°„ ì •ì˜ (ê°ê° 0.1~0.9, í•©ì´ 1ì´ ë˜ë„ë¡ ì œì•½)
            n_agents = len(agent_predictions)
            dimensions = [Real(0.1, 0.9, name=f'weight_{i}') for i in range(n_agents)]
            
            print(f"ğŸ¯ ë² ì´ì§€ì•ˆ ìµœì í™” ì‹¤í–‰: {n_trials}íšŒ ì‹œë„, {n_agents}ê°œ ì—ì´ì „íŠ¸")
            
            # ë² ì´ì§€ì•ˆ ìµœì í™” ì‹¤í–‰
            result = gp_minimize(
                func=objective_function,
                dimensions=dimensions,
                n_calls=n_trials,
                n_initial_points=10,
                random_state=42,
                acq_func='EI'  # Expected Improvement
            )
            
            # ìµœì  ê°€ì¤‘ì¹˜ ì¶”ì¶œ ë° ì •ê·œí™”
            optimal_weights_raw = result.x
            optimal_weights_raw = np.array(optimal_weights_raw)
            optimal_weights_normalized = optimal_weights_raw / optimal_weights_raw.sum()
            
            # ì—ì´ì „íŠ¸ë³„ ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            agent_names = list(agent_predictions.keys())
            optimal_weights = {}
            for i, agent_name in enumerate(agent_names):
                optimal_weights[f'{agent_name}_weight'] = float(optimal_weights_normalized[i])
            
            print(f"âœ… ìµœì  ê°€ì¤‘ì¹˜: {optimal_weights}")
            
        except ImportError:
            print("âš ï¸ scikit-optimizeê°€ ì—†ì–´ ê°œì„ ëœ ëœë¤ ì„œì¹˜ë¡œ ëŒ€ì²´")
            # scikit-optimizeê°€ ì—†ëŠ” ê²½ìš° ê°œì„ ëœ ëœë¤ ì„œì¹˜
            best_f1 = -1
            optimal_weights = {}
            agent_names = list(agent_predictions.keys())
            n_agents = len(agent_names)
            
            # ëª¨ë“  ì‹œë„ ê¸°ë¡
            all_trials = []
            
            print(f"ğŸ” ê°œì„ ëœ ëœë¤ ì„œì¹˜ ì‹¤í–‰: {min(n_trials, 100)}íšŒ ì‹œë„")
            
            for trial in range(min(n_trials, 100)):  # ìµœëŒ€ 100íšŒ
                # ì œì•½ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê°€ì¤‘ì¹˜ ìƒì„±
                attempts = 0
                while attempts < 10:  # ìµœëŒ€ 10ë²ˆ ì‹œë„
                    # ë””ë¦¬í´ë ˆ ë¶„í¬ë¡œ í•©ì´ 1ì¸ ê°€ì¤‘ì¹˜ ìƒì„±
                    weights = np.random.dirichlet(np.ones(n_agents) * 2)  # alpha=2ë¡œ ë” ê· ë“±í•˜ê²Œ
                    
                    # ê²½ê³„ ì¡°ê±´ í™•ì¸ (0.1 ~ 0.9)
                    if np.all(weights >= 0.1) and np.all(weights <= 0.9):
                        break
                    
                    # ê²½ê³„ ì¡°ê±´ ìœ„ë°˜ ì‹œ í´ë¦¬í•‘ í›„ ì¬ì •ê·œí™”
                    weights = np.clip(weights, 0.1, 0.9)
                    weights = weights / weights.sum()
                    attempts += 1
                
                # F1-Score ê³„ì‚°
                f1 = -objective_function(weights)
                all_trials.append(f1)
                
                if f1 > best_f1:
                    best_f1 = f1
                    optimal_weights = {f'{agent_names[i]}_weight': float(weights[i]) for i in range(n_agents)}
                
                # ì§„í–‰ë¥  ì¶œë ¥
                if (trial + 1) % 20 == 0:
                    print(f"   ì§„í–‰ë¥ : {trial + 1}/{min(n_trials, 100)}, í˜„ì¬ ìµœê³  F1: {best_f1:.4f}")
            
            print(f"âœ… ëœë¤ ì„œì¹˜ ì™„ë£Œ: ìµœê³  F1-Score {best_f1:.4f}")
            
            # ê²°ê³¼ ê°ì²´ ìƒì„±
            result = type('Result', (), {
                'fun': -best_f1, 
                'func_vals': [-f1 for f1 in all_trials],
                'x': [optimal_weights[f'{agent_names[i]}_weight'] for i in range(n_agents)]
            })()
        
        # 4. ìµœì  ê°€ì¤‘ì¹˜ë¡œ ì•™ìƒë¸” ì˜ˆì¸¡ ë° ì„ê³„ê°’ ê³„ì‚°
        ensemble_scores = np.zeros(len(employee_ids))
        for agent_name, predictions in agent_predictions.items():
            weight_key = f'{agent_name}_weight'
            if weight_key in optimal_weights:
                ensemble_scores += np.array(predictions) * optimal_weights[weight_key]
        
        # ROC ê³¡ì„ ìœ¼ë¡œ ìµœì  ì„ê³„ê°’ ê³„ì‚°
        from sklearn.metrics import roc_curve
        fpr, tpr, thresholds = roc_curve(actual_labels, ensemble_scores)
        j_scores = tpr - fpr
        best_threshold_idx = np.argmax(j_scores)
        optimal_ensemble_threshold = float(thresholds[best_threshold_idx])
        
        # ì—ì´ì „íŠ¸ë³„ ê°œë³„ ì„ê³„ê°’ë„ ê³„ì‚°
        optimal_thresholds = {}
        for agent_name, predictions in agent_predictions.items():
            fpr_agent, tpr_agent, thresholds_agent = roc_curve(actual_labels, predictions)
            j_scores_agent = tpr_agent - fpr_agent
            best_idx_agent = np.argmax(j_scores_agent)
            optimal_thresholds[f'{agent_name}_threshold'] = float(thresholds_agent[best_idx_agent])
        
        # ìœ„í—˜ë„ ë¶„ë¥˜ ì„ê³„ê°’
        optimal_thresholds['high_risk_threshold'] = optimal_ensemble_threshold + 0.1
        optimal_thresholds['medium_risk_threshold'] = optimal_ensemble_threshold - 0.1
        
        # 5. ìµœì¢… ì„±ëŠ¥ ê³„ì‚°
        final_predictions = (ensemble_scores >= optimal_ensemble_threshold).astype(int)
        best_performance = {
            'f1_score': float(f1_score(actual_labels, final_predictions)),
            'precision': float(precision_score(actual_labels, final_predictions)),
            'recall': float(recall_score(actual_labels, final_predictions)),
            'accuracy': float(accuracy_score(actual_labels, final_predictions)),
            'auc': float(roc_auc_score(actual_labels, ensemble_scores))
        }
        
        # 6. ìµœì í™” íˆìŠ¤í† ë¦¬ ìƒì„±
        optimization_history = []
        if hasattr(result, 'func_vals'):
            for i, score in enumerate(result.func_vals[:20]):  # ìµœëŒ€ 20ê°œ
                optimization_history.append({
                    'trial': i + 1,
                    'score': float(-score),  # ë‹¤ì‹œ ì–‘ìˆ˜ë¡œ ë³€í™˜
                    'f1_score': float(-score)
                })
        
        optimization_history.sort(key=lambda x: x['score'], reverse=True)
        
        print(f"âœ… ë² ì´ì§€ì•ˆ ìµœì í™” ì™„ë£Œ!")
        print(f"   ìµœì  F1-Score: {best_performance['f1_score']:.4f}")
        print(f"   ìµœì  ì„ê³„ê°’: {optimal_ensemble_threshold:.4f}")
        print(f"   ê°€ì¤‘ì¹˜ í•©: {sum(optimal_weights.values()):.4f}")
        
        # ê²°ê³¼ ì €ì¥
        current_results['bayesian_optimization'] = {
            'optimal_weights': optimal_weights,
            'optimal_thresholds': optimal_thresholds,
            'best_performance': best_performance,
            'optimization_history': optimization_history
        }
        
        # ìµœì í™”ëœ ê²°ê³¼ë¥¼ Total_score.csv í˜•ì‹ìœ¼ë¡œ ì €ì¥
        try:
            # ìµœì  ê°€ì¤‘ì¹˜ë¡œ ì•™ìƒë¸” ì ìˆ˜ ê³„ì‚°
            final_ensemble_scores = np.zeros(len(employee_ids))
            agent_names = list(agent_predictions.keys())
            
            for i, agent_name in enumerate(agent_names):
                weight_key = f'{agent_name}_weight'
                if weight_key in optimal_weights:
                    final_ensemble_scores += np.array(agent_predictions[agent_name]) * optimal_weights[weight_key]
            
            # ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼
            final_predictions = (final_ensemble_scores >= optimal_ensemble_threshold).astype(int)
            
            # Total_score.csv í˜•ì‹ìœ¼ë¡œ ìµœì¢… ê²°ê³¼ DataFrame ìƒì„±
            final_results = []
            for i, emp_id in enumerate(employee_ids):
                row = {'employee_id': emp_id}
                
                # ê° ì—ì´ì „íŠ¸ ì ìˆ˜ (Total_score.csv ì»¬ëŸ¼ëª…)
                agent_name_mapping = {
                    'structura': 'Structura_score',
                    'cognita': 'Cognita_score', 
                    'chronos': 'Chronos_score',
                    'sentio': 'Sentio_score',
                    'agora': 'Agora_score'
                }
                
                for agent_name, predictions in agent_predictions.items():
                    if i < len(predictions):
                        column_name = agent_name_mapping.get(agent_name.lower(), f'{agent_name}_score')
                        row[column_name] = predictions[i]
                
                # ì•™ìƒë¸” ì ìˆ˜ ë° ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
                row['ensemble_score'] = final_ensemble_scores[i]
                row['ensemble_prediction'] = final_predictions[i]
                row['attrition'] = 'Yes' if actual_labels[i] == 1 else 'No'
                
                final_results.append(row)
            
            # DataFrameìœ¼ë¡œ ë³€í™˜
            final_df = pd.DataFrame(final_results)
            
            # íŒŒì¼ ì €ì¥
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = os.path.join('app/results', f'optimized_total_score_{timestamp}.csv')
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            final_df.to_csv(output_file, index=False, encoding='utf-8')
            
            print(f"âœ… ìµœì í™” ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
            print(f"   í˜•ì‹: Total_score.csv í˜¸í™˜")
            print(f"   í–‰ ìˆ˜: {len(final_df)}")
            print(f"   ì»¬ëŸ¼: {list(final_df.columns)}")
            
            # current_dataë¥¼ ìµœì¢… ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
            current_data = final_df
            
        except Exception as e:
            print(f"âš ï¸ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {str(e)}")
        
        # ìœ„í—˜ë„ ë¶„ë¥˜ í†µê³„ ì‹œë®¬ë ˆì´ì…˜
        total_employees = len(current_data)
        risk_distribution = {
            'ì•ˆì „êµ°': int(total_employees * 0.6),
            'ì£¼ì˜êµ°': int(total_employees * 0.25),
            'ê³ ìœ„í—˜êµ°': int(total_employees * 0.15)
        }
        
        return jsonify({
            'success': True,
            'message': 'ë² ì´ì§€ì•ˆ ìµœì í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'optimal_thresholds': optimal_thresholds,
            'optimal_weights': optimal_weights,
            'best_performance': best_performance,
            'optimization_history': optimization_history,
            'cv_results': {
                'mean_f1_score': best_performance['f1_score'],
                'std_f1_score': 0.02,
                'mean_precision': best_performance['precision'],
                'std_precision': 0.03,
                'mean_recall': best_performance['recall'],
                'std_recall': 0.025
            },
            'n_trials': len(optimization_history),
            'risk_distribution': risk_distribution,
            'total_employees': total_employees
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ë² ì´ì§€ì•ˆ ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/get_employee_list', methods=['GET'])
def get_employee_list():
    """ë¡œë“œëœ ì§ì› ëª©ë¡ ì¡°íšŒ"""
    try:
        if report_generator.employee_data is None:
            return jsonify({
                'success': False,
                'error': 'ì§ì› ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'
            }), 400
        
        # ì§ì› ëª©ë¡ ì¶”ì¶œ
        if 'employee_id' in report_generator.employee_data.columns:
            employee_list = report_generator.employee_data['employee_id'].tolist()
        else:
            # employee_id ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ ì‚¬ìš©
            employee_list = report_generator.employee_data.index.tolist()
        
        return jsonify({
            'success': True,
            'total_employees': len(employee_list),
            'employee_ids': employee_list[:100],  # ì²˜ìŒ 100ëª…ë§Œ ë°˜í™˜
            'has_more': len(employee_list) > 100
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ì§ì› ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500

@app.route('/save_agent_models', methods=['POST'])
def save_agent_models():
    """ì—ì´ì „íŠ¸ ëª¨ë¸ì„ app/results/modelsì— ì €ì¥"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'ìš”ì²­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'
            }), 400
        
        models = data.get('models')
        save_path = data.get('save_path', 'app/results/models/agent_models.json')
        
        if not models:
            return jsonify({
                'success': False,
                'error': 'ì €ì¥í•  ëª¨ë¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'
            }), 400
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        import os
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        # íŒŒì¼ ì €ì¥
        import json
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(models, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ì—ì´ì „íŠ¸ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
        
        return jsonify({
            'success': True,
            'message': 'ì—ì´ì „íŠ¸ ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤',
            'file_path': save_path,
            'agents_saved': len(models.get('saved_models', {})),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        print(f"âŒ ì—ì´ì „íŠ¸ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'ëª¨ë¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
        }), 500

@app.route('/save_optimized_models', methods=['POST'])
def save_optimized_models():
    """ìµœì í™”ëœ ëª¨ë¸ê³¼ ì„ê³„ê°’/ê°€ì¤‘ì¹˜ë¥¼ app/results/modelsì— ì €ì¥"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'ìš”ì²­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'
            }), 400
        
        complete_model = data.get('complete_model')
        save_path = data.get('save_path', 'app/results/models/optimized_models.json')
        
        if not complete_model:
            return jsonify({
                'success': False,
                'error': 'ì €ì¥í•  ëª¨ë¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'
            }), 400
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        import os
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        # ë°°ì¹˜ ë¶„ì„ì—ì„œ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬ì¡°í™”
        optimized_data = {
            'metadata': {
                'created_at': datetime.now().isoformat(),
                'stage': complete_model.get('training_metadata', {}).get('stage', 'optimization_completed'),
                'agents_used': complete_model.get('training_metadata', {}).get('agents_used', []),
                'training_data_size': complete_model.get('training_metadata', {}).get('training_data_size', 0)
            },
            'agent_models': complete_model.get('saved_models', {}),
            'agent_results': complete_model.get('agent_results', {}),
            'optimization_results': complete_model.get('optimization_results', {}),
            'ready_for_batch_analysis': True
        }
        
        # íŒŒì¼ ì €ì¥
        import json
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(optimized_data, f, indent=2, ensure_ascii=False)
        
        # ë°°ì¹˜ ë¶„ì„ìš© ì‹¬ë³¼ë¦­ ë§í¬ ë˜ëŠ” ë³µì‚¬ë³¸ ìƒì„±
        batch_ready_path = 'app/results/models/batch_ready_models.json'
        with open(batch_ready_path, 'w', encoding='utf-8') as f:
            json.dump(optimized_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ìµœì í™”ëœ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
        print(f"âœ… ë°°ì¹˜ ë¶„ì„ìš© ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ: {batch_ready_path}")
        
        return jsonify({
            'success': True,
            'message': 'ìµœì í™”ëœ ëª¨ë¸ê³¼ ì„ê³„ê°’/ê°€ì¤‘ì¹˜ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤',
            'file_path': save_path,
            'batch_ready_path': batch_ready_path,
            'agents_count': len(optimized_data['agent_models']),
            'has_optimization': bool(optimized_data['optimization_results']),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        print(f"âŒ ìµœì í™”ëœ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'ëª¨ë¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
        }), 500


if __name__ == '__main__':
    print("ğŸš€ Integration Flask ì„œë²„ ì‹œì‘")
    print(f"ğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬: {DATA_DIR}")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_DIR}")
    print("ğŸŒ ì„œë²„ ì£¼ì†Œ: http://localhost:5007")
    
    # API í‚¤ ìƒíƒœ í™•ì¸ (Sentio/Agoraì™€ ë™ì¼)
    if report_generator.client:
        print("âœ… OpenAI API ì—°ê²° ì„±ê³µ - LLM ê¸°ë°˜ ë¶„ì„ ê°€ëŠ¥")
    else:
        print("âš ï¸  OpenAI API í‚¤ ì—†ìŒ - ê¸°ë³¸ ë¶„ì„ ëª¨ë“œë¡œ ë™ì‘")
        print("   ğŸ’¡ .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ /set_api_key ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
    print("\nì‚¬ìš© ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸:")
    print("  GET  /health - ì„œë²„ ìƒíƒœ í™•ì¸")
    print("  POST /set_api_key - OpenAI API í‚¤ ì„¤ì •")
    print("  POST /load_data - ë°ì´í„° ë¡œë“œ")
    print("  POST /calculate_thresholds - ì„ê³„ê°’ ê³„ì‚°")
    print("  POST /optimize_weights - ê°€ì¤‘ì¹˜ ìµœì í™”")
    print("  POST /predict_employee - ê°œë³„ ì§ì› ì˜ˆì¸¡")
    print("  GET  /get_results - í˜„ì¬ ê²°ê³¼ ì¡°íšŒ")
    print("  POST /compare_methods - ìµœì í™” ë°©ë²• ë¹„êµ")
    print("  POST /export_results - ê²°ê³¼ ë‚´ë³´ë‚´ê¸°")
    print("  POST /load_employee_data - ì§ì› ê¸°ë³¸ ë°ì´í„° ë¡œë“œ")
    print("  GET  /get_employee_list - ì§ì› ëª©ë¡ ì¡°íšŒ")
    print("  POST /generate_report - ê°œë³„ ì§ì› ë ˆí¬íŠ¸ ìƒì„± (LLM ì§€ì›)")
    print("  POST /generate_batch_reports - ì¼ê´„ ë ˆí¬íŠ¸ ìƒì„±")
    print("  POST /save_agent_models - ì—ì´ì „íŠ¸ ëª¨ë¸ ì €ì¥")
    print("  POST /save_optimized_models - ìµœì í™”ëœ ëª¨ë¸ ì €ì¥")
    
    app.run(host='0.0.0.0', port=5007, debug=True)
