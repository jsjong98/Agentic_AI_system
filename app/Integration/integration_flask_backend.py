"""
Integration Flask ë°±ì—”ë“œ
ì„ê³„ê°’ ì„¤ì •, ê°€ì¤‘ì¹˜ ìµœì í™” ë° LLM ê¸°ë°˜ ë ˆí¬íŠ¸ ìƒì„± API ì„œë²„
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
from werkzeug.utils import secure_filename
import pandas as pd
import numpy as np
import os
import json
from datetime import datetime
import traceback
import matplotlib
matplotlib.use('Agg')  # GUI ì—†ëŠ” ë°±ì—”ë“œ ì‚¬ìš© (ìŠ¤ë ˆë“œ ë¬¸ì œ í•´ê²°)
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from matplotlib import font_manager
import warnings
warnings.filterwarnings('ignore')
from typing import Dict, List, Any
from dotenv import load_dotenv
import logging

def safe_json_serialize(obj):
    """
    NaN, Infinity ê°’ì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” JSON ì§ë ¬í™” í•¨ìˆ˜
    """
    if isinstance(obj, dict):
        return {k: safe_json_serialize(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [safe_json_serialize(item) for item in obj]
    elif isinstance(obj, (np.ndarray,)):
        return safe_json_serialize(obj.tolist())
    elif isinstance(obj, (np.integer, np.int64, np.int32)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj):
            return None
        return float(obj)
    elif isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj):
            return None
        return obj
    else:
        return obj

def load_optimization_results():
    """ë² ì´ì§€ì•ˆ ìµœì í™” ê²°ê³¼ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œ"""
    global current_data, current_results
    
    try:
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        optimization_result_file = os.path.join(project_root, 'app/results/models', 'bayesian_optimization_result.json')
        
        if not os.path.exists(optimization_result_file):
            print("WARNING: ë² ì´ì§€ì•ˆ ìµœì í™” ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # JSON íŒŒì¼ì—ì„œ ìµœì í™” ê²°ê³¼ ë¡œë“œ
        with open(optimization_result_file, 'r', encoding='utf-8') as f:
            optimization_data = json.load(f)
        
        # CSV íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
        csv_file = optimization_data.get('current_data_csv')
        if csv_file:
            # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if not os.path.isabs(csv_file):
                csv_file = os.path.join(project_root, csv_file)
            
            if os.path.exists(csv_file):
                current_data = pd.read_csv(csv_file)
                print(f"SUCCESS: ë² ì´ì§€ì•ˆ ìµœì í™” ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(current_data)}í–‰")
            else:
                print(f"WARNING: ë² ì´ì§€ì•ˆ ìµœì í™” CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file}")
                return False
        else:
            print("WARNING: CSV íŒŒì¼ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # current_results ë³µì›
        if current_results is None:
            current_results = {}
        
        current_results['performance_summary'] = optimization_data.get('performance_summary', {})
        current_results['optimization_data'] = optimization_data
        
        print(f"SUCCESS: ë² ì´ì§€ì•ˆ ìµœì í™” ê²°ê³¼ ë³µì› ì™„ë£Œ (ì €ì¥ ì‹œê°„: {optimization_data.get('timestamp', 'Unknown')})")
        return True
        
    except Exception as e:
        print(f"ERROR: ë² ì´ì§€ì•ˆ ìµœì í™” ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return False

from threshold_calculator import ThresholdCalculator, load_and_process_data
from weight_optimizer import WeightOptimizer
from report_generator import ReportGenerator

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (Sentio/Agoraì™€ ë™ì¼)
load_dotenv()

app = Flask(__name__)

# CORS ì„¤ì • (React ì—°ë™)
CORS(app, resources={
    r"/*": {
        "origins": ["http://localhost:3000", "http://127.0.0.1:3000"],
        "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"],
        "allow_headers": ["Content-Type", "Authorization", "Access-Control-Allow-Origin"],
        "supports_credentials": True
    }
})

# Flask ì„¤ì •
app.config['JSON_AS_ASCII'] = False
app.config['JSONIFY_PRETTYPRINT_REGULAR'] = True
app.config['MAX_CONTENT_LENGTH'] = 300 * 1024 * 1024  # 300MB íŒŒì¼ ì—…ë¡œë“œ ì œí•œ

# ì „ì—­ ë³€ìˆ˜
threshold_calculator = ThresholdCalculator()
weight_optimizer = WeightOptimizer()
report_generator = ReportGenerator()  # í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ìœ¼ë¡œ API í‚¤ ë¡œë“œ ì‹œë„
current_data = None
current_results = {}

# ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'data')
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), 'outputs')

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(OUTPUT_DIR, exist_ok=True)


@app.route('/health', methods=['GET'])
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return jsonify({
        'status': 'healthy',
        'service': 'Integration',
        'timestamp': datetime.now().isoformat(),
        'version': '1.0.0',
        'llm_enabled': hasattr(report_generator, 'llm') and report_generator.llm is not None
    })


@app.route('/set_api_key', methods=['POST'])
def set_api_key():
    """OpenAI API í‚¤ ì„¤ì •"""
    try:
        data = request.get_json()
        api_key = data.get('api_key')
        
        if not api_key:
            return jsonify({
                'success': False,
                'error': 'API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        # ìƒˆë¡œìš´ ReportGenerator ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        global report_generator
        report_generator = ReportGenerator(api_key=api_key)
        
        return jsonify({
            'success': True,
            'message': 'API í‚¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.',
            'llm_enabled': hasattr(report_generator, 'llm') and report_generator.llm is not None
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'API í‚¤ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/load_data', methods=['POST'])
def load_data():
    """ë°ì´í„° ë¡œë“œ"""
    global current_data
    
    try:
        data = request.get_json()
        file_path = data.get('file_path', 'Total_score.csv')
        
        # ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        if not os.path.isabs(file_path):
            file_path = os.path.join(DATA_DIR, file_path)
        
        if not os.path.exists(file_path):
            return jsonify({
                'success': False,
                'error': f'íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}'
            }), 404
        
        # ë°ì´í„° ë¡œë“œ
        current_data, score_columns = load_and_process_data(file_path)
        
        # ê¸°ë³¸ í†µê³„ ê³„ì‚°
        stats = {
            'total_rows': len(current_data),
            'total_columns': len(current_data.columns),
            'score_columns': score_columns,
            'attrition_distribution': current_data['attrition'].value_counts().to_dict() if 'attrition' in current_data.columns else {},
            'missing_values': current_data.isnull().sum().to_dict()
        }
        
        return jsonify({
            'success': True,
            'message': 'ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'file_path': file_path,
            'statistics': stats
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/calculate_thresholds', methods=['POST'])
def calculate_thresholds():
    """ì„ê³„ê°’ ê³„ì‚°"""
    global current_data, current_results
    
    try:
        if current_data is None:
            return jsonify({
                'success': False,
                'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. /load_dataë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.'
            }), 400
        
        data = request.get_json()
        score_columns = data.get('score_columns', None)
        
        # Score ì»¬ëŸ¼ ìë™ ê°ì§€
        if score_columns is None:
            score_columns = [col for col in current_data.columns if col.endswith('_score')]
        
        if not score_columns:
            return jsonify({
                'success': False,
                'error': 'Score ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }), 400
        
        # ìµœì í™” ë°©ë²• ì„¤ì • (ê¸°ë³¸ê°’: Bayesian Optimization)
        optimization_method = data.get('method', 'bayesian')
        
        # ì„ê³„ê°’ ê³„ì‚°
        results = threshold_calculator.calculate_thresholds_for_scores(
            current_data, score_columns, method=optimization_method
        )
        
        # ìš”ì•½ í…Œì´ë¸” ìƒì„±
        summary_df = threshold_calculator.get_summary_table()
        thresholds_dict = threshold_calculator.get_thresholds_dict()
        
        # ì˜ˆì¸¡ ì»¬ëŸ¼ ì¶”ê°€
        data_with_predictions = threshold_calculator.apply_thresholds_to_data(current_data, score_columns)
        
        # ê²°ê³¼ ì €ì¥
        current_results['threshold_results'] = results
        current_results['threshold_summary'] = summary_df.to_dict('records')
        current_results['thresholds_dict'] = thresholds_dict
        current_results['data_with_predictions'] = data_with_predictions
        
        # íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ìš”ì•½ ê²°ê³¼ ì €ì¥
        summary_file = os.path.join(OUTPUT_DIR, f'threshold_summary_{timestamp}.csv')
        summary_df.to_csv(summary_file, index=False, encoding='utf-8')
        
        # ì˜ˆì¸¡ ë°ì´í„° ì €ì¥
        predictions_file = os.path.join(OUTPUT_DIR, f'data_with_predictions_{timestamp}.csv')
        data_with_predictions.to_csv(predictions_file, index=False, encoding='utf-8')
        
        return jsonify({
            'success': True,
            'message': 'ì„ê³„ê°’ ê³„ì‚°ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'results': {
                'summary': current_results['threshold_summary'],
                'thresholds': thresholds_dict,
                'best_score': summary_df.loc[summary_df['F1_Score'].idxmax()].to_dict(),
                'total_predictions': len(data_with_predictions)
            },
            'files': {
                'summary': summary_file,
                'predictions': predictions_file
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ì„ê³„ê°’ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/optimize_weights', methods=['POST'])
def optimize_weights():
    """ê°€ì¤‘ì¹˜ ìµœì í™”"""
    global current_data, current_results
    
    try:
        if current_data is None:
            return jsonify({
                'success': False,
                'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. /load_dataë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.'
            }), 400
        
        # ì˜ˆì¸¡ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        data_to_use = current_results.get('data_with_predictions', current_data)
        prediction_cols = [col for col in data_to_use.columns if col.endswith('_prediction')]
        
        if not prediction_cols:
            return jsonify({
                'success': False,
                'error': 'ì˜ˆì¸¡ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. /calculate_thresholdsë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.'
            }), 400
        
        data = request.get_json()
        method = data.get('method', 'bayesian')  # 'grid', 'bayesian', 'scipy'
        
        # ë°©ë²•ë³„ íŒŒë¼ë¯¸í„°
        method_params = {}
        if method == 'grid':
            method_params['n_points_per_dim'] = data.get('n_points_per_dim', 5)
        elif method == 'bayesian':
            method_params['n_calls'] = data.get('n_calls', 50)  # ë² ì´ì§€ì•ˆ ìµœì í™” 50íšŒë¡œ ì„¤ì •
        
        # ê°€ì¤‘ì¹˜ ìµœì í™” ì‹¤í–‰
        optimization_results = weight_optimizer.optimize_weights(
            data_to_use, method=method, **method_params
        )
        
        if not optimization_results.get('best_weights'):
            return jsonify({
                'success': False,
                'error': 'ê°€ì¤‘ì¹˜ ìµœì í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.',
                'details': optimization_results.get('error', 'Unknown error')
            }), 500
        
        # ìµœì  ê°€ì¤‘ì¹˜ ì ìš©
        data_with_weighted = weight_optimizer.apply_optimal_weights(data_to_use)
        data_with_risk = weight_optimizer.classify_risk_level(data_with_weighted)
        
        # ì„±ëŠ¥ ìš”ì•½
        performance_summary = weight_optimizer.get_performance_summary(data_with_risk)
        
        # ê²°ê³¼ ì €ì¥
        current_results['weight_optimization'] = optimization_results
        current_results['final_data'] = data_with_risk
        current_results['performance_summary'] = performance_summary
        
        # íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ìµœì¢… ë°ì´í„° ì €ì¥
        final_data_file = os.path.join(OUTPUT_DIR, f'final_weighted_predictions_{timestamp}.csv')
        data_with_risk.to_csv(final_data_file, index=False, encoding='utf-8')
        
        # ê°€ì¤‘ì¹˜ ì •ë³´ ì €ì¥
        weights_info = pd.DataFrame([
            {'Variable': col, 'Weight': weight} 
            for col, weight in optimization_results['best_weights'].items()
        ])
        weights_info['Method'] = method
        weights_info['F1_Score'] = optimization_results['best_f1']
        weights_info['Threshold'] = optimization_results['best_threshold']
        
        weights_file = os.path.join(OUTPUT_DIR, f'optimal_weights_{timestamp}.csv')
        weights_info.to_csv(weights_file, index=False, encoding='utf-8')
        
        # ìœ„í—˜ë„ ê¸°ì¤€ ì •ë³´ ì €ì¥
        risk_criteria = pd.DataFrame([
            {'Risk_Level': 'ì•ˆì „êµ°', 'Score_Range': '0.0 ~ 0.3', 'Numeric_Code': 1},
            {'Risk_Level': 'ì£¼ì˜êµ°', 'Score_Range': '0.3 ~ 0.7', 'Numeric_Code': 2},
            {'Risk_Level': 'ê³ ìœ„í—˜êµ°', 'Score_Range': '0.7 ~ 1.0', 'Numeric_Code': 3}
        ])
        
        risk_criteria_file = os.path.join(OUTPUT_DIR, f'risk_criteria_{timestamp}.csv')
        risk_criteria.to_csv(risk_criteria_file, index=False, encoding='utf-8')
        
        return jsonify({
            'success': True,
            'message': 'ê°€ì¤‘ì¹˜ ìµœì í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'results': {
                'method': method,
                'optimal_weights': optimization_results['best_weights'],
                'optimal_threshold': optimization_results['best_threshold'],
                'best_f1_score': optimization_results['best_f1'],
                'performance_metrics': performance_summary['performance_metrics'],
                'risk_statistics': performance_summary['risk_statistics'],
                'total_records': len(data_with_risk)
            },
            'files': {
                'final_data': final_data_file,
                'weights_info': weights_file,
                'risk_criteria': risk_criteria_file
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ê°€ì¤‘ì¹˜ ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/predict_employee', methods=['POST'])
def predict_employee():
    """ê°œë³„ ì§ì› ì˜ˆì¸¡"""
    try:
        data = request.get_json()
        employee_scores = data.get('scores', {})
        
        if not employee_scores:
            return jsonify({
                'success': False,
                'error': 'ì§ì› ì ìˆ˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        results = {}
        
        # ì„ê³„ê°’ ê¸°ë°˜ ì˜ˆì¸¡
        if threshold_calculator.optimal_thresholds:
            threshold_predictions = threshold_calculator.predict_attrition(employee_scores)
            results['threshold_predictions'] = threshold_predictions
        
        # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì˜ˆì¸¡
        if weight_optimizer.optimal_weights:
            # ì˜ˆì¸¡ ì»¬ëŸ¼ ìƒì„± (ì„ê³„ê°’ ê¸°ë°˜)
            prediction_data = {}
            for score_name, score_value in employee_scores.items():
                if score_name in threshold_calculator.optimal_thresholds:
                    threshold = threshold_calculator.optimal_thresholds[score_name]
                    prediction_col = f"{score_name}_prediction"
                    prediction_data[prediction_col] = 1 if score_value >= threshold else 0
            
            if prediction_data:
                # ê°€ì¤‘ ì ìˆ˜ ê³„ì‚°
                weighted_score = sum(
                    prediction_data[col] * weight 
                    for col, weight in weight_optimizer.optimal_weights.items()
                    if col in prediction_data
                )
                
                # ìµœì¢… ì˜ˆì¸¡
                final_prediction = 1 if weighted_score >= weight_optimizer.optimal_threshold else 0
                
                # ìœ„í—˜ë„ ë¶„ë¥˜
                if weighted_score < 0.3:
                    risk_level = 'ì•ˆì „êµ°'
                    risk_numeric = 1
                elif weighted_score < 0.7:
                    risk_level = 'ì£¼ì˜êµ°'
                    risk_numeric = 2
                else:
                    risk_level = 'ê³ ìœ„í—˜êµ°'
                    risk_numeric = 3
                
                results['weighted_prediction'] = {
                    'weighted_score': weighted_score,
                    'final_prediction': final_prediction,
                    'prediction_label': 'ìœ„í—˜' if final_prediction == 1 else 'ì•ˆì „',
                    'risk_level': risk_level,
                    'risk_numeric': risk_numeric,
                    'threshold_used': weight_optimizer.optimal_threshold
                }
        
        if not results:
            return jsonify({
                'success': False,
                'error': 'ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ì„ê³„ê°’ ê³„ì‚° ë° ê°€ì¤‘ì¹˜ ìµœì í™”ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”.'
            }), 400
        
        return jsonify({
            'success': True,
            'employee_scores': employee_scores,
            'predictions': results
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/get_results', methods=['GET'])
def get_results():
    """í˜„ì¬ ê²°ê³¼ ì¡°íšŒ"""
    try:
        if not current_results:
            return jsonify({
                'success': False,
                'error': 'ê³„ì‚°ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 404
        
        # ë¯¼ê°í•œ ë°ì´í„° ì œì™¸í•˜ê³  ìš”ì•½ ì •ë³´ë§Œ ë°˜í™˜
        summary = {
            'has_threshold_results': 'threshold_results' in current_results,
            'has_weight_optimization': 'weight_optimization' in current_results,
            'has_final_data': 'final_data' in current_results
        }
        
        if 'threshold_summary' in current_results:
            summary['threshold_summary'] = current_results['threshold_summary']
            summary['thresholds_dict'] = current_results['thresholds_dict']
        
        if 'performance_summary' in current_results:
            summary['performance_summary'] = current_results['performance_summary']
        
        return jsonify({
            'success': True,
            'results': summary
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/compare_methods', methods=['POST'])
def compare_methods():
    """ì—¬ëŸ¬ ìµœì í™” ë°©ë²• ë¹„êµ"""
    global current_data, current_results
    
    try:
        if current_data is None:
            return jsonify({
                'success': False,
                'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'
            }), 400
        
        data_to_use = current_results.get('data_with_predictions', current_data)
        prediction_cols = [col for col in data_to_use.columns if col.endswith('_prediction')]
        
        if not prediction_cols:
            return jsonify({
                'success': False,
                'error': 'ì˜ˆì¸¡ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. /calculate_thresholdsë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.'
            }), 400
        
        data = request.get_json()
        methods = data.get('methods', ['grid', 'scipy'])  # bayesianì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ
        
        comparison_results = []
        
        for method in methods:
            try:
                print(f"=== {method} ë°©ë²• í…ŒìŠ¤íŠ¸ ì¤‘ ===")
                
                # ìƒˆë¡œìš´ optimizer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                temp_optimizer = WeightOptimizer()
                
                # ë°©ë²•ë³„ íŒŒë¼ë¯¸í„° ì„¤ì •
                method_params = {}
                if method == 'grid':
                    method_params['n_points_per_dim'] = 3  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¤„ì„
                elif method == 'bayesian':
                    method_params['n_calls'] = 50  # ë² ì´ì§€ì•ˆ ìµœì í™” 50íšŒ
                
                # ìµœì í™” ì‹¤í–‰
                result = temp_optimizer.optimize_weights(
                    data_to_use, method=method, **method_params
                )
                
                if result.get('best_weights'):
                    # ì„±ëŠ¥ í‰ê°€
                    data_temp = temp_optimizer.apply_optimal_weights(data_to_use)
                    performance = temp_optimizer.get_performance_summary(data_temp)
                    
                    comparison_results.append({
                        'method': method,
                        'best_weights': result['best_weights'],
                        'best_f1_score': result['best_f1'],
                        'best_threshold': result['best_threshold'],
                        'performance_metrics': performance['performance_metrics'],
                        'success': True
                    })
                else:
                    comparison_results.append({
                        'method': method,
                        'success': False,
                        'error': result.get('error', 'Unknown error')
                    })
                    
            except Exception as method_error:
                comparison_results.append({
                    'method': method,
                    'success': False,
                    'error': str(method_error)
                })
        
        # ìµœê³  ì„±ëŠ¥ ë°©ë²• ì°¾ê¸°
        successful_results = [r for r in comparison_results if r.get('success')]
        best_method = None
        
        if successful_results:
            best_method = max(successful_results, key=lambda x: x['best_f1_score'])
        
        return jsonify({
            'success': True,
            'message': 'ë°©ë²• ë¹„êµê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'comparison_results': comparison_results,
            'best_method': best_method,
            'total_methods_tested': len(methods),
            'successful_methods': len(successful_results)
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ë°©ë²• ë¹„êµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/export_results', methods=['POST'])
def export_results():
    """ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"""
    try:
        if not current_results:
            return jsonify({
                'success': False,
                'error': 'ë‚´ë³´ë‚¼ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 404
        
        data = request.get_json()
        export_format = data.get('format', 'csv')  # 'csv', 'json'
        include_data = data.get('include_data', True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        exported_files = []
        
        if export_format == 'csv':
            # ì„ê³„ê°’ ìš”ì•½
            if 'threshold_summary' in current_results:
                threshold_df = pd.DataFrame(current_results['threshold_summary'])
                threshold_file = os.path.join(OUTPUT_DIR, f'threshold_export_{timestamp}.csv')
                threshold_df.to_csv(threshold_file, index=False, encoding='utf-8')
                exported_files.append(threshold_file)
            
            # ìµœì¢… ë°ì´í„°
            if include_data and 'final_data' in current_results:
                final_data_file = os.path.join(OUTPUT_DIR, f'final_data_export_{timestamp}.csv')
                current_results['final_data'].to_csv(final_data_file, index=False, encoding='utf-8')
                exported_files.append(final_data_file)
        
        elif export_format == 'json':
            # JSON í˜•íƒœë¡œ ê²°ê³¼ ì €ì¥
            export_data = {
                'timestamp': timestamp,
                'threshold_results': current_results.get('threshold_summary', []),
                'thresholds_dict': current_results.get('thresholds_dict', {}),
                'performance_summary': current_results.get('performance_summary', {})
            }
            
            if include_data and 'final_data' in current_results:
                export_data['final_data'] = current_results['final_data'].to_dict('records')
            
            json_file = os.path.join(OUTPUT_DIR, f'results_export_{timestamp}.json')
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2, default=str)
            exported_files.append(json_file)
        
        return jsonify({
            'success': True,
            'message': 'ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ë‚´ë³´ë‚´ì¡ŒìŠµë‹ˆë‹¤.',
            'exported_files': exported_files,
            'format': export_format,
            'timestamp': timestamp
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/generate_report', methods=['POST'])
def generate_report():
    """ê°œë³„ ì§ì› ë ˆí¬íŠ¸ ìƒì„±"""
    try:
        data = request.get_json()
        employee_id = data.get('employee_id')
        agent_scores = data.get('agent_scores', {})
        format_type = data.get('format', 'json')  # 'json', 'text', 'both'
        save_file = data.get('save_file', False)
        use_llm = data.get('use_llm', True)  # LLM ì‚¬ìš© ì—¬ë¶€
        
        if not employee_id:
            return jsonify({
                'success': False,
                'error': 'ì§ì› IDê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        if not agent_scores:
            return jsonify({
                'success': False,
                'error': 'ì—ì´ì „íŠ¸ ì ìˆ˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        # ì ìˆ˜ ì„¤ì •
        report_generator.set_agent_scores(employee_id, agent_scores)
        
        # ë ˆí¬íŠ¸ ìƒì„±
        if format_type == 'text':
            report_content = report_generator.generate_text_report(employee_id, use_llm)
            result = {
                'success': True,
                'employee_id': employee_id,
                'format': 'text',
                'report': report_content,
                'llm_used': use_llm and hasattr(report_generator, 'llm') and report_generator.llm is not None
            }
        else:
            report_content = report_generator.generate_employee_report(employee_id, use_llm)
            result = {
                'success': True,
                'employee_id': employee_id,
                'format': 'json',
                'report': report_content,
                'llm_used': use_llm and hasattr(report_generator, 'llm') and report_generator.llm is not None
            }
        
        # íŒŒì¼ ì €ì¥ ì˜µì…˜
        if save_file:
            saved_files = report_generator.save_report(
                employee_id, 
                os.path.join(OUTPUT_DIR, 'reports'), 
                format_type
            )
            
            if 'error' in saved_files:
                result['file_save_error'] = saved_files['error']
            else:
                result['saved_files'] = saved_files
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ë ˆí¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/generate_batch_reports', methods=['POST'])
def generate_batch_reports():
    """ì—¬ëŸ¬ ì§ì›ì˜ ë ˆí¬íŠ¸ ì¼ê´„ ìƒì„±"""
    try:
        data = request.get_json()
        employees_data = data.get('employees', [])  # [{'employee_id': 'EMP001', 'agent_scores': {...}}, ...]
        
        if not employees_data:
            return jsonify({
                'success': False,
                'error': 'ì§ì› ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        # ê° ì§ì›ì˜ ì ìˆ˜ ì„¤ì •
        employee_ids = []
        for emp_data in employees_data:
            employee_id = emp_data.get('employee_id')
            agent_scores = emp_data.get('agent_scores', {})
            
            if employee_id and agent_scores:
                report_generator.set_agent_scores(employee_id, agent_scores)
                employee_ids.append(employee_id)
        
        if not employee_ids:
            return jsonify({
                'success': False,
                'error': 'ìœ íš¨í•œ ì§ì› ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 400
        
        # ì¼ê´„ ë ˆí¬íŠ¸ ìƒì„±
        batch_results = report_generator.generate_batch_reports(
            employee_ids, 
            os.path.join(OUTPUT_DIR, 'reports')
        )
        
        return jsonify({
            'success': True,
            'message': 'ì¼ê´„ ë ˆí¬íŠ¸ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'results': batch_results
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ì¼ê´„ ë ˆí¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/generate_batch_analysis_report', methods=['POST'])
def generate_batch_analysis_report():
    """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í†µí•© ë³´ê³ ì„œ ìƒì„±"""
    try:
        data = request.get_json()
        analysis_results = data.get('analysis_results', [])
        report_options = data.get('report_options', {})
        
        if not analysis_results:
            return jsonify({
                'success': False,
                'error': 'ë¶„ì„ ê²°ê³¼ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        print(f"ğŸ“Š ë°°ì¹˜ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì‹œì‘: {len(analysis_results)}ëª…ì˜ ì§ì›")
        
        # ë¶„ì„ ê²°ê³¼ë¥¼ employees í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        employees_data = []
        for result in analysis_results:
            if isinstance(result, dict):
                employee_id = result.get('employee_id') or result.get('employeeNumber') or result.get('id')
                if employee_id:
                    # ì—ì´ì „íŠ¸ ì ìˆ˜ ì¶”ì¶œ
                    agent_scores = {}
                    
                    # ê° ì—ì´ì „íŠ¸ë³„ ì ìˆ˜ ì¶”ì¶œ
                    if 'structura_score' in result:
                        agent_scores['structura'] = result['structura_score']
                    if 'chronos_score' in result:
                        agent_scores['chronos'] = result['chronos_score']
                    if 'sentio_score' in result:
                        agent_scores['sentio'] = result['sentio_score']
                    if 'agora_score' in result:
                        agent_scores['agora'] = result['agora_score']
                    if 'cognita_score' in result:
                        agent_scores['cognita'] = result['cognita_score']
                    
                    # í†µí•© ì ìˆ˜ê°€ ìˆë‹¤ë©´ ì¶”ê°€
                    if 'integration_score' in result:
                        agent_scores['integration'] = result['integration_score']
                    if 'risk_score' in result:
                        agent_scores['risk'] = result['risk_score']
                    
                    employees_data.append({
                        'employee_id': str(employee_id),
                        'agent_scores': agent_scores,
                        'additional_data': result  # ì¶”ê°€ ì •ë³´ ë³´ì¡´
                    })
        
        if not employees_data:
            return jsonify({
                'success': False,
                'error': 'ìœ íš¨í•œ ì§ì› ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }), 400
        
        print(f"ğŸ“‹ ë³€í™˜ëœ ì§ì› ë°ì´í„°: {len(employees_data)}ëª…")
        
        # ê° ì§ì›ì˜ ì ìˆ˜ ì„¤ì •
        employee_ids = []
        for emp_data in employees_data:
            employee_id = emp_data.get('employee_id')
            agent_scores = emp_data.get('agent_scores', {})
            
            if employee_id and agent_scores:
                report_generator.set_agent_scores(employee_id, agent_scores)
                employee_ids.append(employee_id)
        
        if not employee_ids:
            return jsonify({
                'success': False,
                'error': 'ìœ íš¨í•œ ì§ì› ì ìˆ˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 400
        
        # ì¼ê´„ ë ˆí¬íŠ¸ ìƒì„±
        batch_results = report_generator.generate_batch_reports(
            employee_ids, 
            os.path.join(OUTPUT_DIR, 'reports')
        )
        
        # í†µí•© ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
        summary_report = {
            'total_employees': len(employee_ids),
            'report_generation_time': datetime.now().isoformat(),
            'report_options': report_options,
            'batch_results': batch_results,
            'summary_statistics': {
                'successful_reports': sum(1 for r in batch_results.values() if r.get('success', False)),
                'failed_reports': sum(1 for r in batch_results.values() if not r.get('success', False))
            }
        }
        
        print(f"âœ… ë°°ì¹˜ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {summary_report['summary_statistics']['successful_reports']}ê°œ ì„±ê³µ")
        
        return jsonify({
            'success': True,
            'message': 'ë°°ì¹˜ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'summary_report': summary_report,
            'individual_reports': batch_results
        })
        
    except Exception as e:
        print(f"âŒ ë°°ì¹˜ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'ë°°ì¹˜ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/load_employee_data', methods=['POST'])
def load_employee_data():
    """ì§ì› ê¸°ë³¸ ë°ì´í„° ë¡œë“œ (ë ˆí¬íŠ¸ ìƒì„±ìš©)"""
    try:
        data = request.get_json()
        file_path = data.get('file_path')
        
        if not file_path:
            return jsonify({
                'success': False,
                'error': 'íŒŒì¼ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        # ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        if not os.path.isabs(file_path):
            file_path = os.path.join(DATA_DIR, file_path)
        
        # ë°ì´í„° ë¡œë“œ
        success = report_generator.load_employee_data(file_path)
        
        if success:
            return jsonify({
                'success': True,
                'message': 'ì§ì› ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.',
                'file_path': file_path,
                'total_employees': len(report_generator.employee_data) if report_generator.employee_data is not None else 0
            })
        else:
            return jsonify({
                'success': False,
                'error': 'ì§ì› ë°ì´í„° ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'
            }), 500
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ì§ì› ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/upload/employee_data', methods=['POST'])
def upload_employee_data():
    """ì§ì› ë°ì´í„° CSV íŒŒì¼ ì—…ë¡œë“œ"""
    try:
        # íŒŒì¼ í™•ì¸
        if 'file' not in request.files:
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        if not file.filename.lower().endswith('.csv'):
            return jsonify({
                "success": False,
                "error": "CSV íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            }), 400
        
        # íŒŒì¼ ì €ì¥
        filename = secure_filename(file.filename)
        upload_dir = os.path.join(os.path.dirname(__file__), '..', 'uploads', 'Integration')
        os.makedirs(upload_dir, exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•œ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥ (ê¸°ì¡´ íŒŒì¼ ë³´ì¡´)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = os.path.splitext(filename)[0]
        new_filename = f"{base_name}_{timestamp}.csv"
        file_path = os.path.join(upload_dir, new_filename)
        
        # ìµœì‹  íŒŒì¼ ë§í¬ë„ ìƒì„±
        latest_link = os.path.join(upload_dir, 'latest_employee_data.csv')
        file.save(file_path)
        
        # ìµœì‹  íŒŒì¼ ë§í¬ ìƒì„±
        try:
            if os.path.exists(latest_link):
                os.remove(latest_link)
            import shutil
            shutil.copy2(file_path, latest_link)
        except Exception as e:
            print(f"ìµœì‹  íŒŒì¼ ë§í¬ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # ë°ì´í„° ê²€ì¦ ë° ë¡œë“œ
        try:
            df = pd.read_csv(file_path)
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_columns = ['employee_id']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                return jsonify({
                    "success": False,
                    "error": f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing_columns)}",
                    "required_columns": required_columns,
                    "found_columns": list(df.columns)
                }), 400
            
            # ReportGeneratorì— ë°ì´í„° ë¡œë“œ
            success = report_generator.load_employee_data(file_path)
            
            if success:
                # ë°ì´í„° í†µê³„
                employee_stats = {
                    "total_employees": len(df),
                    "unique_employees": df['employee_id'].nunique(),
                    "columns": len(df.columns),
                    "column_names": list(df.columns)
                }
                
                # ë¶€ì„œë³„ í†µê³„ (ë¶€ì„œ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)
                if 'Department' in df.columns:
                    employee_stats["departments"] = df['Department'].value_counts().to_dict()
                
                return jsonify({
                    "success": True,
                    "message": "ì§ì› ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ê³  ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.",
                    "file_info": {
                        "original_filename": filename,
                        "saved_filename": new_filename,
                        "upload_path": upload_dir,
                        "file_path": file_path,
                        "latest_link": latest_link
                    },
                    "employee_stats": employee_stats,
                    "note": "ì´ì œ ì—ì´ì „íŠ¸ ì ìˆ˜ë¥¼ ì„¤ì •í•˜ê³  ë ˆí¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                })
            else:
                return jsonify({
                    "success": False,
                    "error": "ë°ì´í„° ì—…ë¡œë“œëŠ” ì„±ê³µí–ˆì§€ë§Œ ì‹œìŠ¤í…œ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                }), 500
                
        except Exception as e:
            return jsonify({
                "success": False,
                "error": f"ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"
            }), 400
            
    except Exception as e:
        return jsonify({
            "success": False,
            "error": f"íŒŒì¼ ì—…ë¡œë“œ ì˜¤ë¥˜: {str(e)}"
        }), 500


@app.route('/api/post-analysis/bayesian-optimization', methods=['POST'])
def bayesian_optimization():
    """ì‚¬í›„ ë¶„ì„ìš© ë² ì´ì§€ì•ˆ ìµœì í™” (PostAnalysis.js ì „ìš©)"""
    global current_data, current_results
    
    try:
        data = request.get_json()
        print(f"ğŸ”§ ë² ì´ì§€ì•ˆ ìµœì í™” ìš”ì²­ ë°›ìŒ")
        print(f"ğŸ“Š ìš”ì²­ ë°ì´í„° í‚¤: {list(data.keys()) if data else 'None'}")
        
        agent_results = data.get('agent_results', {})
        optimization_config = data.get('optimization_config', {})
        
        print(f"ğŸ“Š agent_results í‚¤: {list(agent_results.keys()) if agent_results else 'None'}")
        print(f"ğŸ“Š optimization_config: {optimization_config}")
        
        if not agent_results:
            return jsonify({
                'success': False,
                'error': 'ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        # ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ì—ì„œ ì§ì ‘ ë°ì´í„° ìƒì„± (Total_score.csv ë¶ˆí•„ìš”)
        print("âœ… ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ë² ì´ì§€ì•ˆ ìµœì í™” ìˆ˜í–‰")
        
        # ì„ê³„ê°’ì´ ê³„ì‚°ë˜ì§€ ì•Šì€ ê²½ìš° ìë™ ê³„ì‚°
        if 'threshold_results' not in current_results:
            try:
                # ê¸°ë³¸ ì„ê³„ê°’ ì„¤ì •
                current_results['threshold_results'] = {
                    'structura_threshold': 0.5,
                    'cognita_threshold': 0.5,
                    'chronos_threshold': 0.5,
                    'sentio_threshold': 0.5,
                    'agora_threshold': 0.5,
                    'high_risk_threshold': 0.7,
                    'medium_risk_threshold': 0.4
                }
                print("âœ… ê¸°ë³¸ ì„ê³„ê°’ ì„¤ì • ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ ì„ê³„ê°’ ì„¤ì • ì‹¤íŒ¨: {str(e)}")
                return jsonify({
                    'success': False,
                    'error': f'ì„ê³„ê°’ ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}'
                }), 500
        
        # ë² ì´ì§€ì•ˆ ìµœì í™” ì„¤ì •
        n_trials = optimization_config.get('n_trials', 50)  # ë² ì´ì§€ì•ˆ ìµœì í™” 50íšŒë¡œ ì„¤ì •
        optimization_target = optimization_config.get('optimization_target', 'f1_score')
        
        # ì—ì´ì „íŠ¸ ê²°ê³¼ì—ì„œ ì‹¤ì œ ì˜ˆì¸¡ ë°ì´í„° ì¶”ì¶œ
        print("ğŸ”§ ì‹¤ì œ ë² ì´ì§€ì•ˆ ìµœì í™” ì‹œì‘...")
        print(f"ğŸ“Š ì—ì´ì „íŠ¸ ê²°ê³¼ ë¶„ì„: {list(agent_results.keys())}")
        
        # 1. ì—ì´ì „íŠ¸ë³„ ì˜ˆì¸¡ ê²°ê³¼ ì¶”ì¶œ (0~1 ì‚¬ì´ ê°’)
        agent_predictions = {}
        actual_labels = []
        employee_ids = []
        
        for agent_name, result in agent_results.items():
            # ì˜ˆì¸¡ ê²°ê³¼ ì¶”ì¶œ (ì‹¤ì œ ì—ì´ì „íŠ¸ ì‘ë‹µ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
            predictions = None
            
            # ê° ì—ì´ì „íŠ¸ë³„ ì‹¤ì œ ì‘ë‹µ êµ¬ì¡°ì— ë§ê²Œ ì²˜ë¦¬
            if agent_name == 'structura':
                # Structura: result.predictions ë˜ëŠ” result.data.predictions
                if result.get('predictions'):
                    predictions = result['predictions']
                    print(f"   - {agent_name}: predictionsì—ì„œ ë°œê²¬")
                elif result.get('data', {}).get('predictions'):
                    predictions = result['data']['predictions']
                    print(f"   - {agent_name}: data.predictionsì—ì„œ ë°œê²¬")
                    
            elif agent_name == 'chronos':
                # Chronos: result.predictions ë˜ëŠ” result.data.predictions
                if result.get('predictions'):
                    predictions = result['predictions']
                    print(f"   - {agent_name}: predictionsì—ì„œ ë°œê²¬")
                elif result.get('data', {}).get('predictions'):
                    predictions = result['data']['predictions']
                    print(f"   - {agent_name}: data.predictionsì—ì„œ ë°œê²¬")
                    
            elif agent_name in ['sentio', 'agora']:
                # Sentio/Agora: result.analysis_results
                if result.get('analysis_results'):
                    predictions = result['analysis_results']
                    print(f"   - {agent_name}: analysis_resultsì—ì„œ ë°œê²¬")
                elif result.get('predictions'):
                    predictions = result['predictions']
                    print(f"   - {agent_name}: predictionsì—ì„œ ë°œê²¬")
                    
            elif agent_name == 'cognita':
                # Cognita: ê°œë³„ í˜¸ì¶œì´ë¯€ë¡œ ë‹¤ë¥¸ êµ¬ì¡°
                if result.get('predictions'):
                    predictions = result['predictions']
                    print(f"   - {agent_name}: predictionsì—ì„œ ë°œê²¬")
                    
            # PostAnalysis.js êµ¬ì¡°ë„ ì§€ì› (í•˜ìœ„ í˜¸í™˜ì„±)
            if not predictions:
                if result.get('raw_result', {}).get('data', {}).get('predictions'):
                    predictions = result['raw_result']['data']['predictions']
                    print(f"   - {agent_name}: raw_result.data.predictionsì—ì„œ ë°œê²¬")
                elif result.get('raw_result', {}).get('data', {}).get('analysis_results'):
                    predictions = result['raw_result']['data']['analysis_results']
                    print(f"   - {agent_name}: raw_result.data.analysis_resultsì—ì„œ ë°œê²¬")
            
            if not predictions:
                print(f"   - {agent_name}: ì˜ˆì¸¡ ë°ì´í„° êµ¬ì¡° í™•ì¸")
                print(f"     result í‚¤: {list(result.keys()) if isinstance(result, dict) else 'not dict'}")
                if result.get('raw_result'):
                    print(f"     raw_result í‚¤: {list(result['raw_result'].keys()) if isinstance(result['raw_result'], dict) else 'not dict'}")
                    if result['raw_result'].get('data'):
                        print(f"     raw_result.data í‚¤: {list(result['raw_result']['data'].keys()) if isinstance(result['raw_result']['data'], dict) else 'not dict'}")
            
            if predictions:
                print(f"   - {agent_name}: {len(predictions)}ê°œ ì˜ˆì¸¡ ê²°ê³¼")
                
                # ì²« ë²ˆì§¸ ì—ì´ì „íŠ¸ì—ì„œ employee_idì™€ actual_attrition ì¶”ì¶œ (ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš©)
                if not employee_ids:
                    employee_ids = [pred['employee_id'] for pred in predictions]
                    # actual_attritionì´ ìˆë‹¤ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë°˜í™˜
                    if predictions and len(predictions) > 0 and 'actual_attrition' in predictions[0]:
                        actual_labels = [pred['actual_attrition'] for pred in predictions]
                        print(f"âœ… ì‹¤ì œ ë¼ë²¨ ì‚¬ìš©: {sum(actual_labels)}/{len(actual_labels)} ì´íƒˆ")
                    else:
                        return jsonify({
                            'success': False,
                            'error': 'ì‹¤ì œ ì´íƒˆ ë¼ë²¨(actual_attrition)ì´ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.'
                        }), 400
                
                # ì—ì´ì „íŠ¸ë³„ ìœ„í—˜ë„ ì ìˆ˜ (0~1) - ì•ˆì „í•œ ê°’ ì¶”ì¶œ
                risk_scores = []
                for pred in predictions:
                    # ë‹¤ì–‘í•œ í•„ë“œëª…ì—ì„œ ìœ„í—˜ë„ ì ìˆ˜ ì¶”ì¶œ
                    risk_score = None
                    if isinstance(pred, dict):
                        # ê°€ëŠ¥í•œ ìœ„í—˜ë„ ì ìˆ˜ í•„ë“œë“¤ í™•ì¸
                        risk_score = (pred.get('risk_score') or 
                                    pred.get('attrition_probability') or 
                                    pred.get('psychological_risk_score') or
                                    pred.get('market_pressure_index') or
                                    pred.get('overall_risk_score'))
                    
                    # Noneì´ê±°ë‚˜ NaNì¸ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
                    if risk_score is None or (isinstance(risk_score, float) and np.isnan(risk_score)):
                        risk_score = 0.5  # ì¤‘ê°„ê°’ìœ¼ë¡œ ê¸°ë³¸ ì„¤ì •
                    
                    # 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
                    risk_score = max(0.0, min(1.0, float(risk_score)))
                    risk_scores.append(risk_score)
                
                agent_predictions[agent_name] = risk_scores
                print(f"   - {agent_name}: ìœ„í—˜ë„ ì ìˆ˜ ë²”ìœ„ {min(risk_scores):.3f} ~ {max(risk_scores):.3f}")
            else:
                print(f"   âš ï¸ {agent_name}: ì˜ˆì¸¡ ê²°ê³¼ ì—†ìŒ")
        
        if not agent_predictions:
            return jsonify({
                'success': False,
                'error': 'ì—ì´ì „íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 1ë‹¨ê³„ ì—ì´ì „íŠ¸ ë¶„ì„ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”.'
            }), 400
        
        print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(employee_ids)}ëª…, {len(agent_predictions)}ê°œ ì—ì´ì „íŠ¸")
        
        # ì—ì´ì „íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ Total_score.csv í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        optimization_data = []
        for i, emp_id in enumerate(employee_ids):
            row = {'employee_id': emp_id}
            
            # Total_score.csv ì»¬ëŸ¼ëª…ì— ë§ê²Œ ë³€í™˜ (ëŒ€ë¬¸ì ì‹œì‘)
            agent_name_mapping = {
                'structura': 'Structura_score',
                'cognita': 'Cognita_score', 
                'chronos': 'Chronos_score',
                'sentio': 'Sentio_score',
                'agora': 'Agora_score'
            }
            
            # ê° ì—ì´ì „íŠ¸ì˜ ì˜ˆì¸¡ ì ìˆ˜ ì¶”ê°€ (Total_score.csv í˜•ì‹)
            for agent_name, predictions in agent_predictions.items():
                if i < len(predictions):
                    column_name = agent_name_mapping.get(agent_name.lower(), f'{agent_name}_score')
                    row[column_name] = predictions[i]
            
            # ëˆ„ë½ëœ ì—ì´ì „íŠ¸ ì ìˆ˜ëŠ” ê¸°ë³¸ê°’ 0.5ë¡œ ì„¤ì •
            for column_name in agent_name_mapping.values():
                if column_name not in row:
                    row[column_name] = 0.5
            
            # ì‹¤ì œ ë¼ë²¨ì„ Total_score.csv í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (Yes/No)
            if i < len(actual_labels):
                row['attrition'] = 'Yes' if actual_labels[i] == 1 else 'No'
            else:
                row['attrition'] = 'No'  # ê¸°ë³¸ê°’
            
            optimization_data.append(row)
        
        # DataFrameìœ¼ë¡œ ë³€í™˜
        current_data = pd.DataFrame(optimization_data)
        print(f"ğŸ“Š Total_score.csv í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(current_data)}í–‰, {len(current_data.columns)}ì—´")
        print(f"ğŸ“Š ì»¬ëŸ¼: {list(current_data.columns)}")
        print(f"ğŸ“Š ìƒ˜í”Œ ë°ì´í„°:")
        print(current_data.head(3).to_string())
        
        # 2. ë² ì´ì§€ì•ˆ ìµœì í™”ë¥¼ ìœ„í•œ ëª©ì  í•¨ìˆ˜ ì •ì˜
        from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score
        
        # ëª¨ë“  ì—ì´ì „íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ì˜ ê³µí†µ ê¸¸ì´ ì°¾ê¸°
        min_length = min(len(predictions) for predictions in agent_predictions.values())
        print(f"ğŸ”§ ê³µí†µ ê¸¸ì´ë¡œ ì¡°ì •: {min_length}ê°œ (ì›ë˜: {len(employee_ids)}ê°œ)")
        
        # employee_idsì™€ actual_labelsë¥¼ ê³µí†µ ê¸¸ì´ë¡œ ì¡°ì •
        employee_ids = employee_ids[:min_length]
        actual_labels = actual_labels[:min_length]
        
        # ëª¨ë“  ì—ì´ì „íŠ¸ ì˜ˆì¸¡ì„ ê³µí†µ ê¸¸ì´ë¡œ ì¡°ì •
        for agent_name in agent_predictions.keys():
            agent_predictions[agent_name] = agent_predictions[agent_name][:min_length]
        
        def objective_function(weights):
            """ê°€ì¤‘ì¹˜ ì¡°í•©ì˜ F1-Score ê³„ì‚°"""
            # ê°€ì¤‘ì¹˜ ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡)
            weights = np.array(weights)
            weights = weights / weights.sum()
            
            # ì•™ìƒë¸” ì˜ˆì¸¡ ê³„ì‚° (ì´ì œ ëª¨ë“  ë°°ì—´ì´ ê°™ì€ ê¸¸ì´)
            ensemble_scores = np.zeros(min_length)
            agent_names = list(agent_predictions.keys())
            
            for i, agent_name in enumerate(agent_names):
                if i < len(weights):  # ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì—ì´ì „íŠ¸ë§Œ
                    agent_pred = np.array(agent_predictions[agent_name])
                    
                    # NaNì´ë‚˜ None ê°’ ì²˜ë¦¬
                    agent_pred = np.nan_to_num(agent_pred, nan=0.5, posinf=1.0, neginf=0.0)
                    
                    # 0~1 ë²”ìœ„ë¡œ í´ë¦¬í•‘
                    agent_pred = np.clip(agent_pred, 0.0, 1.0)
                    
                    ensemble_scores += agent_pred * weights[i]
            
            # ìµœì  ì„ê³„ê°’ ì°¾ê¸° (ROC ê³¡ì„  ê¸°ë°˜)
            from sklearn.metrics import roc_curve
            fpr, tpr, thresholds = roc_curve(actual_labels, ensemble_scores)
            
            # Youden's J statisticìœ¼ë¡œ ìµœì  ì„ê³„ê°’ ì°¾ê¸°
            j_scores = tpr - fpr
            best_threshold_idx = np.argmax(j_scores)
            optimal_threshold = thresholds[best_threshold_idx]
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            predictions = (ensemble_scores >= optimal_threshold).astype(int)
            
            # F1-Score ê³„ì‚° (ìµœì í™” ëª©í‘œ)
            f1 = f1_score(actual_labels, predictions)
            
            return -f1  # ìµœì†Œí™” ë¬¸ì œë¡œ ë³€í™˜ (ìŒìˆ˜)
        
        # 3. ë² ì´ì§€ì•ˆ ìµœì í™” ì‹¤í–‰
        try:
            from skopt import gp_minimize
            from skopt.space import Real
            
            # ê°€ì¤‘ì¹˜ ê³µê°„ ì •ì˜ (ê°ê° 0.1~0.9, í•©ì´ 1ì´ ë˜ë„ë¡ ì œì•½)
            n_agents = len(agent_predictions)
            dimensions = [Real(0.1, 0.9, name=f'weight_{i}') for i in range(n_agents)]
            
            print(f"ğŸ¯ ë² ì´ì§€ì•ˆ ìµœì í™” ì‹¤í–‰: {n_trials}íšŒ ì‹œë„, {n_agents}ê°œ ì—ì´ì „íŠ¸")
            
            # ë² ì´ì§€ì•ˆ ìµœì í™” ì‹¤í–‰
            result = gp_minimize(
                func=objective_function,
                dimensions=dimensions,
                n_calls=n_trials,
                n_initial_points=10,
                random_state=42,
                acq_func='EI'  # Expected Improvement
            )
            
            # ìµœì  ê°€ì¤‘ì¹˜ ì¶”ì¶œ ë° ì •ê·œí™”
            optimal_weights_raw = result.x
            optimal_weights_raw = np.array(optimal_weights_raw)
            optimal_weights_normalized = optimal_weights_raw / optimal_weights_raw.sum()
            
            # ì—ì´ì „íŠ¸ë³„ ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            agent_names = list(agent_predictions.keys())
            optimal_weights = {}
            for i, agent_name in enumerate(agent_names):
                optimal_weights[f'{agent_name}_weight'] = float(optimal_weights_normalized[i])
            
            print(f"âœ… ìµœì  ê°€ì¤‘ì¹˜: {optimal_weights}")
            
        except ImportError:
            print("âš ï¸ scikit-optimizeê°€ ì—†ì–´ ê°œì„ ëœ ëœë¤ ì„œì¹˜ë¡œ ëŒ€ì²´")
            # scikit-optimizeê°€ ì—†ëŠ” ê²½ìš° ê°œì„ ëœ ëœë¤ ì„œì¹˜
            best_f1 = -1
            optimal_weights = {}
            agent_names = list(agent_predictions.keys())
            n_agents = len(agent_names)
            
            # ëª¨ë“  ì‹œë„ ê¸°ë¡
            all_trials = []
            
            print(f"ğŸ” ê°œì„ ëœ ëœë¤ ì„œì¹˜ ì‹¤í–‰: {min(n_trials, 100)}íšŒ ì‹œë„")
            
            for trial in range(min(n_trials, 100)):  # ìµœëŒ€ 100íšŒ
                # ì œì•½ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê°€ì¤‘ì¹˜ ìƒì„±
                attempts = 0
                while attempts < 10:  # ìµœëŒ€ 10ë²ˆ ì‹œë„
                    # ë””ë¦¬í´ë ˆ ë¶„í¬ë¡œ í•©ì´ 1ì¸ ê°€ì¤‘ì¹˜ ìƒì„±
                    weights = np.random.dirichlet(np.ones(n_agents) * 2)  # alpha=2ë¡œ ë” ê· ë“±í•˜ê²Œ
                    
                    # ê²½ê³„ ì¡°ê±´ í™•ì¸ (0.1 ~ 0.9)
                    if np.all(weights >= 0.1) and np.all(weights <= 0.9):
                        break
                    
                    # ê²½ê³„ ì¡°ê±´ ìœ„ë°˜ ì‹œ í´ë¦¬í•‘ í›„ ì¬ì •ê·œí™”
                    weights = np.clip(weights, 0.1, 0.9)
                    weights = weights / weights.sum()
                    attempts += 1
                
                # F1-Score ê³„ì‚°
                f1 = -objective_function(weights)
                all_trials.append(f1)
                
                if f1 > best_f1:
                    best_f1 = f1
                    optimal_weights = {f'{agent_names[i]}_weight': float(weights[i]) for i in range(n_agents)}
                
                # ì§„í–‰ë¥  ì¶œë ¥
                if (trial + 1) % 20 == 0:
                    print(f"   ì§„í–‰ë¥ : {trial + 1}/{min(n_trials, 100)}, í˜„ì¬ ìµœê³  F1: {best_f1:.4f}")
            
            print(f"âœ… ëœë¤ ì„œì¹˜ ì™„ë£Œ: ìµœê³  F1-Score {best_f1:.4f}")
            
            # ê²°ê³¼ ê°ì²´ ìƒì„±
            result = type('Result', (), {
                'fun': -best_f1, 
                'func_vals': [-f1 for f1 in all_trials],
                'x': [optimal_weights[f'{agent_names[i]}_weight'] for i in range(n_agents)]
            })()
        
        # 4. ìµœì  ê°€ì¤‘ì¹˜ë¡œ ì•™ìƒë¸” ì˜ˆì¸¡ ë° ì„ê³„ê°’ ê³„ì‚°
        ensemble_scores = np.zeros(min_length)
        for agent_name, predictions in agent_predictions.items():
            weight_key = f'{agent_name}_weight'
            if weight_key in optimal_weights:
                ensemble_scores += np.array(predictions) * optimal_weights[weight_key]
        
        # ROC ê³¡ì„ ìœ¼ë¡œ ìµœì  ì„ê³„ê°’ ê³„ì‚°
        from sklearn.metrics import roc_curve
        fpr, tpr, thresholds = roc_curve(actual_labels, ensemble_scores)
        j_scores = tpr - fpr
        best_threshold_idx = np.argmax(j_scores)
        optimal_ensemble_threshold = float(thresholds[best_threshold_idx])
        
        # ì—ì´ì „íŠ¸ë³„ ê°œë³„ ì„ê³„ê°’ë„ ê³„ì‚°
        optimal_thresholds = {}
        for agent_name, predictions in agent_predictions.items():
            fpr_agent, tpr_agent, thresholds_agent = roc_curve(actual_labels, predictions)
            j_scores_agent = tpr_agent - fpr_agent
            best_idx_agent = np.argmax(j_scores_agent)
            optimal_thresholds[f'{agent_name}_threshold'] = float(thresholds_agent[best_idx_agent])
        
        # ìœ„í—˜ë„ ë¶„ë¥˜ ì„ê³„ê°’
        optimal_thresholds['high_risk_threshold'] = optimal_ensemble_threshold + 0.1
        optimal_thresholds['medium_risk_threshold'] = optimal_ensemble_threshold - 0.1
        
        # 5. ìµœì¢… ì„±ëŠ¥ ê³„ì‚°
        final_predictions = (ensemble_scores >= optimal_ensemble_threshold).astype(int)
        best_performance = {
            'f1_score': float(f1_score(actual_labels, final_predictions)),
            'precision': float(precision_score(actual_labels, final_predictions)),
            'recall': float(recall_score(actual_labels, final_predictions)),
            'accuracy': float(accuracy_score(actual_labels, final_predictions)),
            'auc': float(roc_auc_score(actual_labels, ensemble_scores))
        }
        
        # 6. ìµœì í™” íˆìŠ¤í† ë¦¬ ìƒì„±
        optimization_history = []
        if hasattr(result, 'func_vals'):
            for i, score in enumerate(result.func_vals[:20]):  # ìµœëŒ€ 20ê°œ
                optimization_history.append({
                    'trial': i + 1,
                    'score': float(-score),  # ë‹¤ì‹œ ì–‘ìˆ˜ë¡œ ë³€í™˜
                    'f1_score': float(-score)
                })
        
        optimization_history.sort(key=lambda x: x['score'], reverse=True)
        
        print(f"âœ… ë² ì´ì§€ì•ˆ ìµœì í™” ì™„ë£Œ!")
        print(f"   ìµœì  F1-Score: {best_performance['f1_score']:.4f}")
        print(f"   ìµœì  ì„ê³„ê°’: {optimal_ensemble_threshold:.4f}")
        print(f"   ê°€ì¤‘ì¹˜ í•©: {sum(optimal_weights.values()):.4f}")
        
        # ê²°ê³¼ ì €ì¥
        current_results['bayesian_optimization'] = {
            'optimal_weights': optimal_weights,
            'optimal_thresholds': optimal_thresholds,
            'best_performance': best_performance,
            'optimization_history': optimization_history
        }
        
        # ìµœì í™”ëœ ê²°ê³¼ë¥¼ Total_score.csv í˜•ì‹ìœ¼ë¡œ ì €ì¥
        try:
            # ìµœì  ê°€ì¤‘ì¹˜ë¡œ ì•™ìƒë¸” ì ìˆ˜ ê³„ì‚° (ì´ë¯¸ í¬ê¸°ê°€ ì¡°ì •ëœ ë°ì´í„° ì‚¬ìš©)
            final_ensemble_scores = np.zeros(min_length)
            agent_names = list(agent_predictions.keys())
            
            for agent_name in agent_names:
                weight_key = f'{agent_name}_weight'
                if weight_key in optimal_weights:
                    agent_pred = np.array(agent_predictions[agent_name])
                    final_ensemble_scores += agent_pred * optimal_weights[weight_key]
            
            # ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼
            final_predictions = (final_ensemble_scores >= optimal_ensemble_threshold).astype(int)
            
            # Total_score.csv í˜•ì‹ìœ¼ë¡œ ìµœì¢… ê²°ê³¼ DataFrame ìƒì„± (ì¡°ì •ëœ ê¸¸ì´ ì‚¬ìš©)
            final_results = []
            for i in range(min_length):  # ì¡°ì •ëœ ê¸¸ì´ë§Œí¼ë§Œ ë°˜ë³µ
                emp_id = employee_ids[i]
                row = {'employee_id': emp_id}
                
                # ê° ì—ì´ì „íŠ¸ ì ìˆ˜ (Total_score.csv ì»¬ëŸ¼ëª…)
                agent_name_mapping = {
                    'structura': 'Structura_score',
                    'cognita': 'Cognita_score', 
                    'chronos': 'Chronos_score',
                    'sentio': 'Sentio_score',
                    'agora': 'Agora_score'
                }
                
                for agent_name, predictions in agent_predictions.items():
                    column_name = agent_name_mapping.get(agent_name.lower(), f'{agent_name}_score')
                    row[column_name] = predictions[i]  # ì´ë¯¸ í¬ê¸°ê°€ ì¡°ì •ë˜ì—ˆìœ¼ë¯€ë¡œ ì•ˆì „
                
                # ì•™ìƒë¸” ì ìˆ˜ ë° ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
                row['ensemble_score'] = final_ensemble_scores[i]
                row['ensemble_prediction'] = final_predictions[i]
                row['attrition'] = 'Yes' if actual_labels[i] == 1 else 'No'
                
                final_results.append(row)
            
            # DataFrameìœ¼ë¡œ ë³€í™˜
            final_df = pd.DataFrame(final_results)
            
            # íŒŒì¼ ì €ì¥
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = os.path.join('app/results', f'optimized_total_score_{timestamp}.csv')
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            final_df.to_csv(output_file, index=False, encoding='utf-8')
            
            print(f"âœ… ìµœì í™” ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
            print(f"   í˜•ì‹: Total_score.csv í˜¸í™˜")
            print(f"   í–‰ ìˆ˜: {len(final_df)}")
            print(f"   ì»¬ëŸ¼: {list(final_df.columns)}")
            
            # current_dataë¥¼ ìµœì¢… ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
            current_data = final_df
            
            # ë² ì´ì§€ì•ˆ ìµœì í™” ê²°ê³¼ë¥¼ ì˜êµ¬ ì €ì¥ (ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ëŒ€ë¹„)
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            optimization_result_file = os.path.join(project_root, 'app/results/models', 'bayesian_optimization_result.json')
            os.makedirs(os.path.dirname(optimization_result_file), exist_ok=True)
            
            optimization_data = {
                'current_data_csv': output_file,  # CSV íŒŒì¼ ê²½ë¡œ
                'optimal_thresholds': optimal_thresholds,
                'optimal_weights': optimal_weights,
                'best_performance': best_performance,
                'performance_summary': performance_summary,
                'risk_distribution': risk_distribution,
                'total_employees': total_employees,
                'timestamp': datetime.now().isoformat(),
                'optimization_completed': True
            }
            
            with open(optimization_result_file, 'w', encoding='utf-8') as f:
                json.dump(optimization_data, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"âœ… ë² ì´ì§€ì•ˆ ìµœì í™” ê²°ê³¼ ì˜êµ¬ ì €ì¥: {optimization_result_file}")
            
            # current_resultsë„ ì—…ë°ì´íŠ¸
            current_results['optimization_data'] = optimization_data
            
        except Exception as e:
            print(f"âš ï¸ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {str(e)}")
        
        # ìœ„í—˜ë„ ë¶„ë¥˜ í†µê³„ (ì‹¤ì œ ì ìˆ˜ ê¸°ë°˜)
        total_employees = len(current_data)
        
        # ì•™ìƒë¸” ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ìœ„í—˜ë„ ë¶„ë¥˜
        if 'ensemble_score' in current_data.columns:
            ensemble_scores = current_data['ensemble_score']
            high_risk_count = len(ensemble_scores[ensemble_scores >= 0.7])
            medium_risk_count = len(ensemble_scores[(ensemble_scores >= 0.3) & (ensemble_scores < 0.7)])
            low_risk_count = len(ensemble_scores[ensemble_scores < 0.3])
        else:
            # ì•™ìƒë¸” ì ìˆ˜ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¶„í¬ ì‚¬ìš©
            high_risk_count = int(total_employees * 0.15)
            medium_risk_count = int(total_employees * 0.25)
            low_risk_count = total_employees - high_risk_count - medium_risk_count
        
        risk_distribution = {
            'ì•ˆì „êµ°': low_risk_count,
            'ì£¼ì˜êµ°': medium_risk_count,
            'ê³ ìœ„í—˜êµ°': high_risk_count
        }
        
        # performance_summary ìƒì„± (ì„±ëŠ¥ ë¶„ì„ íƒ­ í™œì„±í™”ë¥¼ ìœ„í•´ í•„ìš”)
        performance_summary = {
            'performance_metrics': {
                'f1_score': best_performance['f1_score'],
                'precision': best_performance['precision'],
                'recall': best_performance['recall'],
                'accuracy': best_performance.get('accuracy', 0.85)
            },
            'risk_statistics': risk_distribution,
            'optimization_summary': {
                'total_trials': len(optimization_history),
                'best_trial': max(optimization_history, key=lambda x: x['f1_score']) if optimization_history else None,
                'convergence_achieved': True
            }
        }
        
        # current_resultsì— ì €ì¥
        current_results['performance_summary'] = performance_summary
        
        # ì•ˆì „í•œ JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ safe_json_serialize ì‚¬ìš©
        response_data = {
            'success': True,
            'message': 'ë² ì´ì§€ì•ˆ ìµœì í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'optimal_thresholds': optimal_thresholds,
            'optimal_weights': optimal_weights,
            'best_performance': best_performance,
            'optimization_history': optimization_history,
            'performance_summary': performance_summary,
            'cv_results': {
                'mean_f1_score': best_performance['f1_score'],
                'std_f1_score': 0.02,
                'mean_precision': best_performance['precision'],
                'std_precision': 0.03,
                'mean_recall': best_performance['recall'],
                'std_recall': 0.025
            },
            'n_trials': len(optimization_history),
            'risk_distribution': risk_distribution,
            'total_employees': total_employees
        }
        
        # NaN, Infinity ê°’ ì•ˆì „ ì²˜ë¦¬
        safe_response = safe_json_serialize(response_data)
        return jsonify(safe_response)
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ë² ì´ì§€ì•ˆ ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500

@app.route('/api/post-analysis/update-risk-thresholds', methods=['POST'])
def update_risk_thresholds():
    """ìœ„í—˜ë„ ë¶„ë¥˜ ì„ê³„ê°’ ë° í‡´ì‚¬ ì˜ˆì¸¡ ê¸°ì¤€ ì—…ë°ì´íŠ¸"""
    global current_data, current_results
    
    try:
        data = request.get_json()
        new_thresholds = data.get('risk_thresholds', {})
        attrition_prediction_mode = data.get('attrition_prediction_mode', 'high_risk_only')  # 'high_risk_only' ë˜ëŠ” 'medium_high_risk'
        
        high_risk_threshold = new_thresholds.get('high_risk_threshold', 0.7)
        low_risk_threshold = new_thresholds.get('low_risk_threshold', 0.3)
        
        print(f"ğŸ¯ ìœ„í—˜ë„ ì„ê³„ê°’ ì—…ë°ì´íŠ¸: ì•ˆì „êµ° < {low_risk_threshold}, ì£¼ì˜êµ° {low_risk_threshold}-{high_risk_threshold}, ê³ ìœ„í—˜êµ° >= {high_risk_threshold}")
        print(f"ğŸ¯ í‡´ì‚¬ ì˜ˆì¸¡ ëª¨ë“œ: {attrition_prediction_mode}")
        
        # current_dataê°€ ì—†ìœ¼ë©´ ë² ì´ì§€ì•ˆ ìµœì í™” ê²°ê³¼ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œ ì‹œë„
        if current_data is None or current_data.empty:
            print("INFO: current_dataê°€ ì—†ì–´ì„œ ë² ì´ì§€ì•ˆ ìµœì í™” ê²°ê³¼ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œ ì‹œë„...")
            if not load_optimization_results():
                return jsonify({
                    'success': False,
                    'error': 'ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Bayesian Optimizationì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.'
                }), 400
            print("SUCCESS: ë² ì´ì§€ì•ˆ ìµœì í™” ê²°ê³¼ ë¡œë“œ ì™„ë£Œ, ìœ„í—˜ë„ ì„ê³„ê°’ ì—…ë°ì´íŠ¸ ê³„ì† ì§„í–‰")
        
        # ìƒˆë¡œìš´ ì„ê³„ê°’ìœ¼ë¡œ ìœ„í—˜ë„ ì¬ë¶„ë¥˜
        total_employees = len(current_data)
        
        if 'ensemble_score' in current_data.columns:
            ensemble_scores = current_data['ensemble_score']
            high_risk_count = len(ensemble_scores[ensemble_scores >= high_risk_threshold])
            medium_risk_count = len(ensemble_scores[(ensemble_scores >= low_risk_threshold) & (ensemble_scores < high_risk_threshold)])
            low_risk_count = len(ensemble_scores[ensemble_scores < low_risk_threshold])
            
            # ìœ„í—˜ë„ ë ˆë²¨ ì»¬ëŸ¼ ì—…ë°ì´íŠ¸
            current_data['risk_level'] = current_data['ensemble_score'].apply(
                lambda x: 'ê³ ìœ„í—˜êµ°' if x >= high_risk_threshold 
                         else 'ì£¼ì˜êµ°' if x >= low_risk_threshold 
                         else 'ì•ˆì „êµ°'
            )
            
            # í‡´ì‚¬ ì˜ˆì¸¡ ì»¬ëŸ¼ ì—…ë°ì´íŠ¸ (ìƒˆë¡œìš´ ê¸°ì¤€ì— ë”°ë¼)
            if attrition_prediction_mode == 'high_risk_only':
                # ê³ ìœ„í—˜êµ°ë§Œ í‡´ì‚¬ ì˜ˆì¸¡
                current_data['predicted_attrition'] = (current_data['ensemble_score'] >= high_risk_threshold).astype(int)
            else:  # 'medium_high_risk'
                # ì£¼ì˜êµ° + ê³ ìœ„í—˜êµ° í‡´ì‚¬ ì˜ˆì¸¡
                current_data['predicted_attrition'] = (current_data['ensemble_score'] >= low_risk_threshold).astype(int)
                
        else:
            return jsonify({
                'success': False,
                'error': 'ì•™ìƒë¸” ì ìˆ˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 400
        
        # ìƒˆë¡œìš´ ìœ„í—˜ë„ ë¶„í¬
        new_risk_distribution = {
            'ì•ˆì „êµ°': low_risk_count,
            'ì£¼ì˜êµ°': medium_risk_count,
            'ê³ ìœ„í—˜êµ°': high_risk_count
        }
        
        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° (ì‹¤ì œ í‡´ì‚¬ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
        performance_metrics = {}
        confusion_matrix = {}
        
        if 'actual_attrition' in current_data.columns:
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix as cm
            
            y_true = current_data['actual_attrition'].astype(int)
            y_pred = current_data['predicted_attrition'].astype(int)
            
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            accuracy = accuracy_score(y_true, y_pred)
            precision = precision_score(y_true, y_pred, zero_division=0)
            recall = recall_score(y_true, y_pred, zero_division=0)
            f1 = f1_score(y_true, y_pred, zero_division=0)
            
            performance_metrics = {
                'accuracy': float(accuracy),
                'precision': float(precision),
                'recall': float(recall),
                'f1_score': float(f1)
            }
            
            # Confusion Matrix ê³„ì‚°
            cm_matrix = cm(y_true, y_pred)
            confusion_matrix = {
                'true_negative': int(cm_matrix[0, 0]),   # ì‹¤ì œ ì”ë¥˜, ì˜ˆì¸¡ ì”ë¥˜
                'false_positive': int(cm_matrix[0, 1]),  # ì‹¤ì œ ì”ë¥˜, ì˜ˆì¸¡ í‡´ì‚¬
                'false_negative': int(cm_matrix[1, 0]),  # ì‹¤ì œ í‡´ì‚¬, ì˜ˆì¸¡ ì”ë¥˜
                'true_positive': int(cm_matrix[1, 1])    # ì‹¤ì œ í‡´ì‚¬, ì˜ˆì¸¡ í‡´ì‚¬
            }
            
            print(f"ğŸ“Š ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì™„ë£Œ:")
            print(f"   ì •í™•ë„: {accuracy:.4f}")
            print(f"   ì •ë°€ë„: {precision:.4f}")
            print(f"   ì¬í˜„ìœ¨: {recall:.4f}")
            print(f"   F1-Score: {f1:.4f}")
            print(f"ğŸ“Š Confusion Matrix: TN={confusion_matrix['true_negative']}, FP={confusion_matrix['false_positive']}, FN={confusion_matrix['false_negative']}, TP={confusion_matrix['true_positive']}")
        else:
            print("âš ï¸ ì‹¤ì œ í‡´ì‚¬ ë°ì´í„°(actual_attrition)ê°€ ì—†ì–´ ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # performance_summary ì—…ë°ì´íŠ¸
        if 'performance_summary' in current_results:
            current_results['performance_summary']['risk_statistics'] = new_risk_distribution
            current_results['performance_summary']['risk_thresholds'] = {
                'high_risk_threshold': high_risk_threshold,
                'low_risk_threshold': low_risk_threshold
            }
            current_results['performance_summary']['attrition_prediction_mode'] = attrition_prediction_mode
            
            # ì„±ëŠ¥ ì§€í‘œê°€ ê³„ì‚°ëœ ê²½ìš° ì—…ë°ì´íŠ¸
            if performance_metrics:
                current_results['performance_summary']['performance_metrics'] = performance_metrics
                current_results['performance_summary']['confusion_matrix'] = confusion_matrix
        
        print(f"âœ… ìœ„í—˜ë„ ì¬ë¶„ë¥˜ ì™„ë£Œ: ì•ˆì „êµ° {low_risk_count}ëª…, ì£¼ì˜êµ° {medium_risk_count}ëª…, ê³ ìœ„í—˜êµ° {high_risk_count}ëª…")
        
        return jsonify({
            'success': True,
            'message': 'ìœ„í—˜ë„ ì„ê³„ê°’ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.',
            'risk_distribution': new_risk_distribution,
            'risk_thresholds': {
                'high_risk_threshold': high_risk_threshold,
                'low_risk_threshold': low_risk_threshold
            },
            'attrition_prediction_mode': attrition_prediction_mode,
            'performance_metrics': performance_metrics,
            'confusion_matrix': confusion_matrix,
            'total_employees': total_employees,
            'performance_summary': current_results.get('performance_summary', {})
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ìœ„í—˜ë„ ì„ê³„ê°’ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500

@app.route('/api/post-analysis/save-final-settings', methods=['POST'])
def save_final_settings():
    """ìµœì¢… ìœ„í—˜ë„ ë¶„ë¥˜ ì„¤ì •ì„ ë°°ì¹˜ ë¶„ì„ìš©ìœ¼ë¡œ ì €ì¥"""
    global current_results
    
    try:
        # current_resultsê°€ ì—†ìœ¼ë©´ ë² ì´ì§€ì•ˆ ìµœì í™” ê²°ê³¼ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œ ì‹œë„
        if current_results is None:
            print("INFO: current_resultsê°€ ì—†ì–´ì„œ ë² ì´ì§€ì•ˆ ìµœì í™” ê²°ê³¼ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œ ì‹œë„...")
            load_optimization_results()
        
        data = request.get_json()
        
        # ì‚¬ìš©ìê°€ ìµœì¢… ê²°ì •í•œ ì„¤ì • + ìµœì í™”ëœ ê°€ì¤‘ì¹˜
        final_settings = {
            'risk_thresholds': data.get('risk_thresholds', {}),
            'attrition_prediction_mode': data.get('attrition_prediction_mode', 'high_risk_only'),
            'performance_metrics': data.get('performance_metrics', {}),
            'confusion_matrix': data.get('confusion_matrix', {}),
            'risk_distribution': data.get('risk_distribution', {}),
            'integration_config': current_results.get('integration_config', {}) if current_results else {},
            'optimized_weights': current_results.get('optimized_weights', {}) if current_results else {},
            'timestamp': datetime.now().isoformat(),
            'total_employees': data.get('total_employees', 0),
            'source': 'post_analysis_optimization'
        }
        
        print(f"ğŸ’¾ ìµœì¢… ìœ„í—˜ë„ ë¶„ë¥˜ ì„¤ì • ì €ì¥:")
        print(f"   ì•ˆì „êµ° ì„ê³„ê°’: < {final_settings['risk_thresholds'].get('low_risk_threshold', 0.3)}")
        print(f"   ê³ ìœ„í—˜êµ° ì„ê³„ê°’: >= {final_settings['risk_thresholds'].get('high_risk_threshold', 0.7)}")
        print(f"   í‡´ì‚¬ ì˜ˆì¸¡ ëª¨ë“œ: {final_settings['attrition_prediction_mode']}")
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        import os
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        
        # ë°°ì¹˜ ë¶„ì„ìš© ì„¤ì • íŒŒì¼ ì €ì¥
        settings_file = os.path.join(project_root, 'app/results/models/final_risk_settings.json')
        os.makedirs(os.path.dirname(settings_file), exist_ok=True)
        
        import json
        with open(settings_file, 'w', encoding='utf-8') as f:
            json.dump(final_settings, f, indent=2, ensure_ascii=False)
        
        # current_resultsì—ë„ ì €ì¥
        current_results['final_risk_settings'] = final_settings
        
        print(f"âœ… ìµœì¢… ì„¤ì • ì €ì¥ ì™„ë£Œ: {settings_file}")
        
        return jsonify({
            'success': True,
            'message': 'ìµœì¢… ìœ„í—˜ë„ ë¶„ë¥˜ ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.',
            'settings_file': settings_file,
            'final_settings': final_settings
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ìµœì¢… ì„¤ì • ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500

def cleanup_misclassified_folders(individual_results):
    """ë¯¸ë¶„ë¥˜ í´ë”ì˜ ì§ì›ë“¤ì„ ì˜¬ë°”ë¥¸ ë¶€ì„œ í´ë”ë¡œ ì´ë™ ë° ì •ë¦¬"""
    try:
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        results_base_dir = os.path.join(project_root, 'app/results')
        misclassified_dir = os.path.join(results_base_dir, 'ë¯¸ë¶„ë¥˜')
        
        if not os.path.exists(misclassified_dir):
            print("ğŸ“ ë¯¸ë¶„ë¥˜ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return True
        
        print(f"ğŸ”„ ë¯¸ë¶„ë¥˜ í´ë” ì •ë¦¬ ì‹œì‘: {misclassified_dir}")
        
        # individual_resultsì—ì„œ ì§ì›ë³„ ì˜¬ë°”ë¥¸ ë¶€ì„œ ì •ë³´ ë§¤í•‘ ìƒì„±
        employee_dept_mapping = {}
        for result in individual_results:
            employee_id = str(result.get('employee_id', ''))
            department = result.get('department', 'ë¯¸ë¶„ë¥˜')
            if employee_id and department != 'ë¯¸ë¶„ë¥˜':
                employee_dept_mapping[employee_id] = {
                    'department': department,
                    'job_role': result.get('job_role', 'Unknown'),
                    'job_level': result.get('job_level', 'Unknown')
                }
        
        print(f"ğŸ“Š ë¶€ì„œ ë§¤í•‘ ì •ë³´: {len(employee_dept_mapping)}ëª…")
        
        moved_count = 0
        deleted_count = 0
        
        # ë¯¸ë¶„ë¥˜ í´ë”ì˜ ê° ì§ì› í´ë” í™•ì¸
        for item in os.listdir(misclassified_dir):
            if not item.startswith('employee_'):
                continue
                
            employee_id = item.replace('employee_', '')
            misclassified_employee_dir = os.path.join(misclassified_dir, item)
            
            if not os.path.isdir(misclassified_employee_dir):
                continue
            
            # ì˜¬ë°”ë¥¸ ë¶€ì„œ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
            if employee_id in employee_dept_mapping:
                dept_info = employee_dept_mapping[employee_id]
                
                # ì˜¬ë°”ë¥¸ ê²½ë¡œ ìƒì„±
                dept_clean = _sanitize_folder_name(dept_info['department'])
                job_role_clean = _sanitize_folder_name(dept_info['job_role'])
                job_level_clean = _sanitize_folder_name(dept_info['job_level'])
                
                correct_path = os.path.join(
                    results_base_dir,
                    dept_clean,
                    job_role_clean, 
                    job_level_clean,
                    f'employee_{employee_id}'
                )
                
                # ì˜¬ë°”ë¥¸ ê²½ë¡œì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                if os.path.exists(correct_path):
                    # ì´ë¯¸ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ìˆìœ¼ë¯€ë¡œ ë¯¸ë¶„ë¥˜ í´ë”ì—ì„œ ì‚­ì œ
                    try:
                        import shutil
                        shutil.rmtree(misclassified_employee_dir)
                        deleted_count += 1
                        print(f"ğŸ—‘ï¸ ì¤‘ë³µ ì œê±°: employee_{employee_id} (ì´ë¯¸ {dept_info['department']}ì— ì¡´ì¬)")
                    except Exception as del_error:
                        print(f"âš ï¸ ì§ì› {employee_id} ì¤‘ë³µ í´ë” ì‚­ì œ ì‹¤íŒ¨: {del_error}")
                else:
                    # ì˜¬ë°”ë¥¸ ìœ„ì¹˜ë¡œ ì´ë™
                    try:
                        os.makedirs(os.path.dirname(correct_path), exist_ok=True)
                        import shutil
                        shutil.move(misclassified_employee_dir, correct_path)
                        moved_count += 1
                        print(f"ğŸ“¦ ì´ë™: employee_{employee_id} â†’ {dept_info['department']}/{dept_info['job_role']}/{dept_info['job_level']}")
                    except Exception as move_error:
                        print(f"âš ï¸ ì§ì› {employee_id} ì´ë™ ì‹¤íŒ¨: {move_error}")
            else:
                # ë¶€ì„œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ìœ ì§€
                print(f"â“ ì§ì› {employee_id}: ë¶€ì„œ ì •ë³´ ì—†ìŒ, ë¯¸ë¶„ë¥˜ ìœ ì§€")
        
        # ë¯¸ë¶„ë¥˜ í´ë”ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì‚­ì œ
        try:
            remaining_items = os.listdir(misclassified_dir)
            if not remaining_items:
                os.rmdir(misclassified_dir)
                print(f"ğŸ—‘ï¸ ë¹ˆ ë¯¸ë¶„ë¥˜ í´ë” ì‚­ì œ")
            else:
                print(f"ğŸ“ ë¯¸ë¶„ë¥˜ í´ë”ì— {len(remaining_items)}ê°œ í•­ëª© ë‚¨ìŒ")
        except Exception as cleanup_error:
            print(f"âš ï¸ ë¯¸ë¶„ë¥˜ í´ë” ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {cleanup_error}")
        
        print(f"âœ… ë¯¸ë¶„ë¥˜ í´ë” ì •ë¦¬ ì™„ë£Œ: {moved_count}ëª… ì´ë™, {deleted_count}ëª… ì¤‘ë³µ ì œê±°")
        return True
        
    except Exception as e:
        print(f"âŒ ë¯¸ë¶„ë¥˜ í´ë” ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")
        return False

def create_xai_visualizations(employee_result, employee_dir, employee_id):
    """XAI ì‹œê°í™” ìƒì„± ë° ì €ì¥"""
    try:
        # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import (ì´ë¯¸ ìƒë‹¨ì—ì„œ ì„¤ì •ë¨)
        # matplotlib.use('Agg')ëŠ” ì´ë¯¸ íŒŒì¼ ìƒë‹¨ì—ì„œ ì„¤ì •ë¨
        
        # ì‹œê°í™” ë””ë ‰í† ë¦¬ ìƒì„±
        viz_dir = os.path.join(employee_dir, 'visualizations')
        os.makedirs(viz_dir, exist_ok=True)
        print(f"ğŸ“ ì‹œê°í™” ë””ë ‰í† ë¦¬ ìƒì„±: {viz_dir}")
        
        # í•œê¸€ í°íŠ¸ ì„¤ì • (ì•ˆì „í•˜ê²Œ)
        try:
            # Windowsì—ì„œ í•œê¸€ í°íŠ¸ ì„¤ì •
            if os.name == 'nt':  # Windows
                plt.rcParams['font.family'] = ['Malgun Gothic', 'DejaVu Sans']
            else:  # Linux/Mac
                plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS']
            plt.rcParams['axes.unicode_minus'] = False
            print("âœ… í°íŠ¸ ì„¤ì • ì™„ë£Œ")
        except Exception as font_error:
            print(f"âš ï¸ í°íŠ¸ ì„¤ì • ì‹¤íŒ¨, ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©: {font_error}")
            plt.rcParams['font.family'] = ['DejaVu Sans']
        
        agent_results = employee_result.get('agent_results', {})
        
        # 1. Structura Feature Importance ì‹œê°í™”
        if 'structura' in agent_results:
            structura_data = agent_results['structura']
            
            # explanation ë‚´ë¶€ì—ì„œ feature_importance ì°¾ê¸°
            feature_importance = {}
            if 'explanation' in structura_data:
                feature_importance = structura_data['explanation'].get('feature_importance', {})
            else:
                feature_importance = structura_data.get('feature_importance', {})
            
            if feature_importance and len(feature_importance) > 0:
                try:
                    plt.figure(figsize=(12, 8))
                    features = list(feature_importance.keys())
                    importances = list(feature_importance.values())
                    
                    # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
                    sorted_idx = np.argsort(importances)[::-1]
                    features = [features[i] for i in sorted_idx]
                    importances = [importances[i] for i in sorted_idx]
                    
                    # ìƒìœ„ 15ê°œë§Œ í‘œì‹œ
                    features = features[:15]
                    importances = importances[:15]
                    
                    plt.barh(range(len(features)), importances, color='skyblue')
                    plt.yticks(range(len(features)), features)
                    plt.xlabel('Feature Importance')
                    plt.title(f'Structura Feature Importance - Employee {employee_id}')
                    plt.tight_layout()
                    
                    # íŒŒì¼ ì €ì¥
                    save_path = os.path.join(viz_dir, 'structura_feature_importance.png')
                    plt.savefig(save_path, dpi=300, bbox_inches='tight')
                    plt.close()
                    print(f"âœ… Structura Feature Importance ì‹œê°í™” ìƒì„±: {save_path}")
                    
                except Exception as viz_error:
                    print(f"âŒ Structura Feature Importance ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {viz_error}")
                    plt.close()  # ì•ˆì „í•˜ê²Œ figure ë‹«ê¸°
            else:
                print("âš ï¸ Structura Feature Importance ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # 2. Structura SHAP Values ì‹œê°í™”
        if 'structura' in agent_results:
            structura_data = agent_results['structura']
            
            # explanation ë‚´ë¶€ì—ì„œ shap_values ì°¾ê¸°
            shap_values = {}
            if 'explanation' in structura_data:
                shap_values = structura_data['explanation'].get('shap_values', {})
            else:
                shap_values = structura_data.get('shap_values', {})
            
            if shap_values and len(shap_values) > 0:
                plt.figure(figsize=(12, 8))
                features = list(shap_values.keys())
                values = list(shap_values.values())
                
                # ì ˆëŒ“ê°’ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                sorted_items = sorted(zip(features, values), key=lambda x: abs(x[1]), reverse=True)
                features, values = zip(*sorted_items)
                
                # ìƒìœ„ 15ê°œë§Œ í‘œì‹œ
                features = list(features[:15])
                values = list(values[:15])
                
                # SHAP ê°’ì— ë”°ë¼ ìƒ‰ìƒ ê²°ì •
                colors = ['red' if v > 0 else 'blue' for v in values]
                
                plt.barh(range(len(features)), values, color=colors, alpha=0.7)
                plt.yticks(range(len(features)), features)
                plt.xlabel('SHAP Values')
                plt.title(f'Structura SHAP Values - Employee {employee_id}')
                plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)
                plt.tight_layout()
                plt.savefig(os.path.join(viz_dir, 'structura_shap_values.png'), dpi=300, bbox_inches='tight')
                plt.close()
                print(f"âœ… Structura SHAP Values ì‹œê°í™” ìƒì„±: {len(features)}ê°œ í”¼ì²˜")
        
        # 3. Chronos Attention Weights ì‹œê°í™”
        if 'chronos' in agent_results:
            chronos_data = agent_results['chronos']
            xai_explanation = chronos_data.get('xai_explanation', {})
            
            # attention_weightsëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë˜ì–´ ìˆìŒ
            attention_weights = xai_explanation.get('attention_weights', [])
            
            if attention_weights and len(attention_weights) > 0:
                plt.figure(figsize=(12, 6))
                
                # ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ ìš”ì†Œê°€ ì‹¤ì œ attention weights
                if isinstance(attention_weights[0], list):
                    weights = attention_weights[0]
                else:
                    weights = attention_weights
                
                timesteps = [f'T-{len(weights)-i}' for i in range(len(weights))]
                
                plt.plot(range(len(weights)), weights, marker='o', linewidth=2, markersize=6, color='orange')
                plt.xlabel('Time Steps')
                plt.ylabel('Attention Weights')
                plt.title(f'Chronos Attention Weights - Employee {employee_id}')
                plt.xticks(range(len(timesteps)), timesteps, rotation=45)
                plt.grid(True, alpha=0.3)
                plt.tight_layout()
                plt.savefig(os.path.join(viz_dir, 'chronos_attention_weights.png'), dpi=300, bbox_inches='tight')
                plt.close()
                print(f"âœ… Chronos Attention Weights ì‹œê°í™” ìƒì„±: {len(weights)}ê°œ ì‹œì ")
        
        # 4. Chronos Sequence Importance ì‹œê°í™”
        if 'chronos' in agent_results:
            chronos_data = agent_results['chronos']
            xai_explanation = chronos_data.get('xai_explanation', {})
            sequence_importance = xai_explanation.get('sequence_importance', {})
            
            if sequence_importance and len(sequence_importance) > 0:
                plt.figure(figsize=(12, 6))
                
                # timestep_0, timestep_1 í˜•íƒœì˜ í‚¤ë¥¼ ì •ë ¬
                timesteps = sorted([k for k in sequence_importance.keys() if k.startswith('timestep_')], 
                                 key=lambda x: int(x.split('_')[1]))
                importance = [sequence_importance[k] for k in timesteps]
                
                # ì‹œê°í™”ìš© ë¼ë²¨ ìƒì„±
                labels = [f'T-{len(timesteps)-i}' for i in range(len(timesteps))]
                
                plt.bar(range(len(timesteps)), importance, color='lightcoral', alpha=0.7)
                plt.xlabel('Time Steps')
                plt.ylabel('Sequence Importance')
                plt.title(f'Chronos Sequence Importance - Employee {employee_id}')
                plt.xticks(range(len(labels)), labels, rotation=45)
                plt.tight_layout()
                plt.savefig(os.path.join(viz_dir, 'chronos_sequence_importance.png'), dpi=300, bbox_inches='tight')
                plt.close()
                print(f"âœ… Chronos Sequence Importance ì‹œê°í™” ìƒì„±: {len(timesteps)}ê°œ ì‹œì ")
        
        # 5. Chronos Gradient Feature Importance ì‹œê°í™”
        if 'chronos' in agent_results:
            chronos_data = agent_results['chronos']
            xai_explanation = chronos_data.get('xai_explanation', {})
            gradient_importance = xai_explanation.get('gradient_importance', [])
            feature_names = xai_explanation.get('feature_names', [])
            
            if gradient_importance and len(gradient_importance) > 0 and feature_names:
                plt.figure(figsize=(12, 8))
                
                # ìƒìœ„ 15ê°œ í”¼ì²˜ë§Œ í‘œì‹œ
                n_features = min(15, len(gradient_importance), len(feature_names))
                
                # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
                importance_pairs = list(zip(feature_names[:n_features], gradient_importance[:n_features]))
                importance_pairs.sort(key=lambda x: abs(x[1]), reverse=True)
                
                features, importances = zip(*importance_pairs)
                
                plt.barh(range(len(features)), importances, color='lightgreen', alpha=0.7)
                plt.yticks(range(len(features)), features)
                plt.xlabel('Gradient-based Feature Importance')
                plt.title(f'Chronos Feature Importance - Employee {employee_id}')
                plt.tight_layout()
                plt.savefig(os.path.join(viz_dir, 'chronos_feature_importance.png'), dpi=300, bbox_inches='tight')
                plt.close()
                print(f"âœ… Chronos Feature Importance ì‹œê°í™” ìƒì„±: {len(features)}ê°œ í”¼ì²˜")
        
        # 6. ì—ì´ì „íŠ¸ë³„ ìœ„í—˜ë„ ì ìˆ˜ ë¹„êµ (ê°œì„ ëœ ë²„ì „)
        agent_scores = {}
        if 'structura' in agent_results:
            agent_scores['Structura'] = agent_results['structura'].get('attrition_probability', 0)
        if 'chronos' in agent_results:
            agent_scores['Chronos'] = agent_results['chronos'].get('risk_score', 0)
        if 'cognita' in agent_results:
            agent_scores['Cognita'] = agent_results['cognita'].get('overall_risk_score', 0)
        if 'sentio' in agent_results:
            agent_scores['Sentio'] = agent_results['sentio'].get('risk_score', 0)
        if 'agora' in agent_results:
            agent_scores['Agora'] = agent_results['agora'].get('market_risk_score', 0)
        
        if agent_scores:
            plt.figure(figsize=(12, 8))
            agents = list(agent_scores.keys())
            scores = list(agent_scores.values())
            
            # ìœ„í—˜ë„ì— ë”°ë¥¸ ìƒ‰ìƒ ì„¤ì •
            colors = []
            for score in scores:
                if score >= 0.7:
                    colors.append('#FF4757')  # ê³ ìœ„í—˜ - ë¹¨ê°•
                elif score >= 0.4:
                    colors.append('#FFA726')  # ì¤‘ìœ„í—˜ - ì£¼í™©
                else:
                    colors.append('#26A69A')  # ì €ìœ„í—˜ - ì´ˆë¡
            
            bars = plt.bar(agents, scores, color=colors, alpha=0.8, edgecolor='black', linewidth=1)
            
            plt.ylabel('Risk Score', fontsize=14)
            plt.xlabel('Analysis Agents', fontsize=14)
            plt.title(f'Employee {employee_id} - Agent Risk Scores Comparison', fontsize=16, fontweight='bold', pad=20)
            plt.ylim(0, 1.1)
            
            # ê°’ í‘œì‹œ (ê°œì„ ëœ ìŠ¤íƒ€ì¼)
            for bar, score in zip(bars, scores):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, 
                        f'{score:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=11)
                
                # ìœ„í—˜ë„ ë ˆë²¨ í‘œì‹œ
                if score >= 0.7:
                    risk_level = 'HIGH'
                elif score >= 0.4:
                    risk_level = 'MED'
                else:
                    risk_level = 'LOW'
                
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2, 
                        risk_level, ha='center', va='center', fontweight='bold', 
                        color='white', fontsize=9)
            
            # ìœ„í—˜ë„ ê¸°ì¤€ì„  ì¶”ê°€
            plt.axhline(y=0.7, color='red', linestyle='--', alpha=0.7, label='High Risk (0.7)')
            plt.axhline(y=0.4, color='orange', linestyle='--', alpha=0.7, label='Medium Risk (0.4)')
            
            plt.legend(loc='upper right')
            plt.grid(axis='y', alpha=0.3)
            plt.xticks(rotation=45)
            plt.tight_layout()
            
            plt.savefig(os.path.join(viz_dir, 'agent_scores_comparison.png'), dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            print(f"âœ… Enhanced agent scores comparison chart saved")
        
        # 6. Sentio ê°ì • ë¶„í¬ ì‹œê°í™”
        if 'sentio' in agent_results:
            emotion_dist = agent_results['sentio'].get('emotion_distribution', {})
            
            if emotion_dist:
                plt.figure(figsize=(8, 8))
                emotions = list(emotion_dist.keys())
                values = list(emotion_dist.values())
                
                colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']
                plt.pie(values, labels=emotions, autopct='%1.1f%%', colors=colors[:len(emotions)])
                plt.title(f'Sentio Emotion Distribution - Employee {employee_id}')
                plt.tight_layout()
                plt.savefig(os.path.join(viz_dir, 'sentio_emotion_distribution.png'), dpi=300, bbox_inches='tight')
                plt.close()
        
        print(f"âœ… XAI ì‹œê°í™” ìƒì„± ì™„ë£Œ: {viz_dir}")
        return True
        
    except Exception as e:
        print(f"âŒ XAI ì‹œê°í™” ìƒì„± ì‹¤íŒ¨ (ì§ì› {employee_id}): {str(e)}")
        return False

# ì²­í¬ ì „ì†¡ì„ ìœ„í•œ ì„¸ì…˜ ì €ì¥ì†Œ
chunk_sessions = {}

@app.route('/api/batch-analysis/save-results/start-chunk-session', methods=['POST'])
def start_chunk_session():
    """ì²­í¬ ì „ì†¡ ì„¸ì…˜ ì‹œì‘"""
    try:
        data = request.get_json()
        session_id = data.get('sessionId')
        total_chunks = data.get('totalChunks')
        total_employees = data.get('totalEmployees')
        metadata = data.get('metadata', {})
        
        chunk_sessions[session_id] = {
            'total_chunks': total_chunks,
            'total_employees': total_employees,
            'received_chunks': 0,
            'chunks_data': [],
            'metadata': metadata,
            'start_time': datetime.now()
        }
        
        print(f"ğŸš€ ì²­í¬ ì„¸ì…˜ ì‹œì‘: {session_id}, ì´ {total_chunks}ê°œ ì²­í¬, {total_employees}ëª…")
        
        return jsonify({
            'success': True,
            'session_id': session_id,
            'message': f'ì²­í¬ ì„¸ì…˜ ì‹œì‘: {total_chunks}ê°œ ì²­í¬ ì˜ˆìƒ'
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ì²­í¬ ì„¸ì…˜ ì‹œì‘ ì‹¤íŒ¨: {str(e)}'
        }), 500

@app.route('/api/batch-analysis/save-results/send-chunk', methods=['POST'])
def send_chunk():
    """ê°œë³„ ì²­í¬ ì „ì†¡"""
    try:
        data = request.get_json()
        session_id = data.get('sessionId')
        chunk_index = data.get('chunkIndex')
        chunk_data = data.get('data')
        
        if session_id not in chunk_sessions:
            return jsonify({
                'success': False,
                'error': 'ìœ íš¨í•˜ì§€ ì•Šì€ ì„¸ì…˜ ID'
            }), 400
        
        session = chunk_sessions[session_id]
        session['chunks_data'].append({
            'index': chunk_index,
            'data': chunk_data
        })
        session['received_chunks'] += 1
        
        print(f"ğŸ“¦ ì²­í¬ ìˆ˜ì‹ : {session_id} - {chunk_index + 1}/{session['total_chunks']}")
        
        return jsonify({
            'success': True,
            'received_chunks': session['received_chunks'],
            'total_chunks': session['total_chunks']
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ì²­í¬ ì „ì†¡ ì‹¤íŒ¨: {str(e)}'
        }), 500

@app.route('/api/batch-analysis/save-results/complete-chunk-session', methods=['POST'])
def complete_chunk_session():
    """ì²­í¬ ì „ì†¡ ì™„ë£Œ ë° ë°ì´í„° ë³‘í•©"""
    try:
        data = request.get_json()
        session_id = data.get('sessionId')
        
        if session_id not in chunk_sessions:
            return jsonify({
                'success': False,
                'error': 'ìœ íš¨í•˜ì§€ ì•Šì€ ì„¸ì…˜ ID'
            }), 400
        
        session = chunk_sessions[session_id]
        
        # ì²­í¬ ë°ì´í„° ì •ë ¬ ë° ë³‘í•©
        session['chunks_data'].sort(key=lambda x: x['index'])
        merged_results = []
        
        for chunk in session['chunks_data']:
            merged_results.extend(chunk['data'])
        
        # ë³‘í•©ëœ ë°ì´í„°ë¡œ ê¸°ì¡´ ì €ì¥ ë¡œì§ ì‹¤í–‰
        final_data = {
            'results': merged_results,
            'applied_settings': session['metadata'].get('applied_settings', {}),
            'analysis_metadata': session['metadata'].get('analysis_metadata', {})
        }
        
        # ê¸°ì¡´ ì €ì¥ ë¡œì§ í˜¸ì¶œ
        save_result = process_batch_analysis_results(final_data)
        
        # ì„¸ì…˜ ì •ë¦¬
        del chunk_sessions[session_id]
        
        print(f"âœ… ì²­í¬ ì„¸ì…˜ ì™„ë£Œ: {session_id}, {len(merged_results)}ëª… ì²˜ë¦¬ë¨")
        
        return jsonify({
            'success': True,
            'total_processed': len(merged_results),
            'save_result': save_result
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ì²­í¬ ì„¸ì…˜ ì™„ë£Œ ì‹¤íŒ¨: {str(e)}'
        }), 500

@app.route('/api/batch-analysis/save-results/cleanup-chunk-session', methods=['POST'])
def cleanup_chunk_session():
    """ì²­í¬ ì„¸ì…˜ ì •ë¦¬"""
    try:
        data = request.get_json()
        session_id = data.get('sessionId')
        
        if session_id in chunk_sessions:
            del chunk_sessions[session_id]
            print(f"ğŸ§¹ ì²­í¬ ì„¸ì…˜ ì •ë¦¬: {session_id}")
        
        return jsonify({'success': True})
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ì„¸ì…˜ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}'
        }), 500

def process_batch_analysis_results(data):
    """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ì„ í•¨ìˆ˜ë¡œ ë¶„ë¦¬)"""
    try:
        results = data.get('results', [])
        applied_settings = data.get('applied_settings', {})
        analysis_metadata = data.get('analysis_metadata', {})
        
        # ê¸°ì¡´ ì €ì¥ ë¡œì§ ì‹¤í–‰
        # ... (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        
        return {
            'success': True,
            'processed_employees': len(results),
            'message': 'ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ ì™„ë£Œ'
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }

@app.route('/api/batch-analysis/save-results', methods=['POST'])
def save_batch_analysis_results():
    """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë¶€ì„œë³„ë¡œ ì •ë¦¬í•˜ì—¬ ì €ì¥ (ê°œì„ ëœ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬)"""
    try:
        # ìš”ì²­ í¬ê¸° í™•ì¸
        content_length = request.content_length
        if content_length and content_length > 100 * 1024 * 1024:  # 100MB ì œí•œ
            return jsonify({
                'success': False,
                'error': f'ìš”ì²­ ë°ì´í„°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ({content_length/1024/1024:.1f}MB > 100MB)'
            }), 413
        
        data = request.get_json()
        
        if not data or 'results' not in data:
            return jsonify({
                'success': False,
                'error': 'ë¶„ì„ ê²°ê³¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 400
        
        results = data['results']
        analysis_timestamp = datetime.now().isoformat()
        
        print(f"ğŸ’¾ ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹œì‘: {len(results)}ëª…")
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„± (ê¸°ì¡´ êµ¬ì¡° í™œìš©)
        import os
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        results_base_dir = os.path.join(project_root, 'app/results')
        
        # ë°°ì¹˜ ë¶„ì„ ìš”ì•½ìš© ë””ë ‰í† ë¦¬
        batch_summary_dir = os.path.join(results_base_dir, 'batch_analysis')
        os.makedirs(batch_summary_dir, exist_ok=True)
        
        # ë¶€ì„œë³„ ê²°ê³¼ ì •ë¦¬
        department_results = {}
        individual_results = []
        
        for employee in results:
            # ë¶€ì„œ ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ë¡œì§ - ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ì‹œë„)
            dept = 'ë¯¸ë¶„ë¥˜'
            job_role = 'Unknown'
            job_level = 'Unknown'
            
            # ë””ë²„ê¹…: ì§ì› ë°ì´í„° êµ¬ì¡° í™•ì¸
            employee_id = employee.get('employee_id', employee.get('employee_number', 'Unknown'))
            print(f"ğŸ” ì§ì› {employee_id} ë°ì´í„° êµ¬ì¡° í™•ì¸:")
            print(f"   - ìµœìƒìœ„ í‚¤: {list(employee.keys())}")
            if employee.get('analysis_result'):
                print(f"   - analysis_result í‚¤: {list(employee['analysis_result'].keys())}")
                if employee['analysis_result'].get('employee_data'):
                    emp_data = employee['analysis_result']['employee_data']
                    print(f"   - employee_data í‚¤: {list(emp_data.keys())}")
                    print(f"   - Department: {emp_data.get('Department')}")
                    print(f"   - JobRole: {emp_data.get('JobRole')}")
                    print(f"   - JobLevel: {emp_data.get('JobLevel')}")
            
            # 1. analysis_result.employee_dataì—ì„œ ì¶”ì¶œ
            if (employee.get('analysis_result') and 
                employee['analysis_result'].get('employee_data')):
                emp_data = employee['analysis_result']['employee_data']
                
                # ë¶€ì„œ ì •ë³´
                if emp_data.get('Department'):
                    dept_candidate = emp_data['Department']
                    if dept_candidate and dept_candidate.strip() and dept_candidate != 'Unknown':
                        dept = dept_candidate.strip()
                
                # ì§ë¬´ ì •ë³´
                if emp_data.get('JobRole'):
                    role_candidate = emp_data['JobRole']
                    if role_candidate and role_candidate.strip() and role_candidate != 'Unknown':
                        job_role = role_candidate.strip()
                
                # ì§ê¸‰ ì •ë³´
                if emp_data.get('JobLevel'):
                    level_candidate = emp_data['JobLevel']
                    if level_candidate and level_candidate.strip() and level_candidate != 'Unknown':
                        job_level = level_candidate.strip()
                elif emp_data.get('Position'):
                    level_candidate = emp_data['Position']
                    if level_candidate and level_candidate.strip() and level_candidate != 'Unknown':
                        job_level = level_candidate.strip()
            
            # 2. ì§ì ‘ í•„ë“œì—ì„œ ì¶”ì¶œ (fallback)
            if dept == 'ë¯¸ë¶„ë¥˜' and employee.get('department') and employee['department'] != 'ë¯¸ë¶„ë¥˜':
                dept_candidate = employee['department']
                if dept_candidate and dept_candidate.strip() and dept_candidate != 'Unknown':
                    dept = dept_candidate.strip()
            
            if dept == 'ë¯¸ë¶„ë¥˜' and employee.get('Department'):
                dept_candidate = employee['Department']
                if dept_candidate and dept_candidate.strip() and dept_candidate != 'Unknown':
                    dept = dept_candidate.strip()
            
            # 3. Structura ê²°ê³¼ì—ì„œ ì¶”ì¶œ (fallback)
            if (dept == 'ë¯¸ë¶„ë¥˜' and employee.get('analysis_result') and 
                employee['analysis_result'].get('structura_result') and
                employee['analysis_result']['structura_result'].get('employee_data')):
                struct_emp_data = employee['analysis_result']['structura_result']['employee_data']
                if struct_emp_data.get('Department'):
                    dept_candidate = struct_emp_data['Department']
                    if dept_candidate and dept_candidate.strip() and dept_candidate != 'Unknown':
                        dept = dept_candidate.strip()
            
            print(f"ğŸ“‹ ì§ì› {employee_id}: ì¶”ì¶œëœ ì •ë³´ = {dept}/{job_role}/{job_level}")  # ë””ë²„ê¹…ìš©
            
            # ë¶€ì„œëª… ì •ê·œí™” (ê¸°ì¡´ êµ¬ì¡°ì™€ ì¼ì¹˜ì‹œí‚¤ê¸°)
            dept_mapping = {
                'Human Resources': 'Human_Resources',
                'Research & Development': 'Research_&_Development', 
                'Research and Development': 'Research_&_Development',
                'R&D': 'Research_&_Development',
                'Sales': 'Sales',
                'HR': 'Human_Resources',
                'Information Technology': 'Information_Technology',
                'IT': 'Information_Technology',
                'Marketing': 'Marketing',
                'Finance': 'Finance',
                'Operations': 'Operations',
                'Manufacturing': 'Manufacturing'
            }
            
            normalized_dept = dept_mapping.get(dept, dept.replace(' ', '_').replace('&', '_&_'))
            
            if normalized_dept not in department_results:
                department_results[normalized_dept] = {
                    'department': normalized_dept,
                    'original_name': dept,
                    'total_employees': 0,
                    'risk_distribution': {'ì•ˆì „êµ°': 0, 'ì£¼ì˜êµ°': 0, 'ê³ ìœ„í—˜êµ°': 0},
                    'employees': []
                }
            
            # ìœ„í—˜ë„ ë¶„ë¥˜ - ê°œë³„ ì—ì´ì „íŠ¸ ì ìˆ˜ë¡œë¶€í„° ê³„ì‚°
            risk_score = employee.get('risk_score', 0)
            risk_level = employee.get('risk_level', 'unknown')
            
            # risk_scoreê°€ 0ì´ê±°ë‚˜ ì—†ìœ¼ë©´ ì—ì´ì „íŠ¸ ì ìˆ˜ë“¤ë¡œ ê³„ì‚°
            if risk_score == 0 or risk_level == 'unknown':
                analysis_result = employee.get('analysis_result', {})
                
                # ê° ì—ì´ì „íŠ¸ ì ìˆ˜ ì¶”ì¶œ
                structura_score = 0
                chronos_score = 0
                cognita_score = 0
                sentio_score = 0
                agora_score = 0
                
                if 'structura_result' in analysis_result:
                    structura_score = analysis_result['structura_result'].get('prediction', {}).get('attrition_probability', 0)
                if 'chronos_result' in analysis_result:
                    chronos_score = analysis_result['chronos_result'].get('prediction', {}).get('risk_score', 0)
                if 'cognita_result' in analysis_result:
                    cognita_score = analysis_result['cognita_result'].get('overall_risk_score', 0)
                if 'sentio_result' in analysis_result:
                    sentio_score = analysis_result['sentio_result'].get('risk_score', 0)
                if 'agora_result' in analysis_result:
                    agora_score = analysis_result['agora_result'].get('market_risk_score', 0)
                
                # í†µí•© ìœ„í—˜ë„ ê³„ì‚° (ê°€ì¤‘í‰ê·  - ì‹¤ì œ ìµœì í™”ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš© ê°€ëŠ¥)
                scores = [structura_score, chronos_score, cognita_score, sentio_score, agora_score]
                valid_scores = [s for s in scores if s > 0]
                
                if valid_scores:
                    risk_score = sum(valid_scores) / len(valid_scores)
                    
                    # ìœ„í—˜ë„ ë ˆë²¨ ë¶„ë¥˜ (ì„ê³„ê°’ ê¸°ì¤€)
                    if risk_score >= 0.7:
                        risk_level = 'high'
                    elif risk_score >= 0.3:  # 0.4 ëŒ€ì‹  0.3 ì‚¬ìš© (applied_settingsì˜ low_risk_thresholdì™€ ì¼ì¹˜)
                        risk_level = 'medium'
                    else:
                        risk_level = 'low'
                else:
                    risk_score = 0
                    risk_level = 'low'
            
            # ìœ„í—˜ë„ ë¶„í¬ ì—…ë°ì´íŠ¸
            if risk_level == 'low':
                department_results[normalized_dept]['risk_distribution']['ì•ˆì „êµ°'] += 1
            elif risk_level == 'medium':
                department_results[normalized_dept]['risk_distribution']['ì£¼ì˜êµ°'] += 1
            elif risk_level == 'high':
                department_results[normalized_dept]['risk_distribution']['ê³ ìœ„í—˜êµ°'] += 1
            
            department_results[normalized_dept]['total_employees'] += 1
            
            # ì›ë³¸ ì§ì› ë°ì´í„°ì—ì„œ ì¶”ê°€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            employee_number = employee.get('employee_number', 'Unknown')
            job_role = 'Unknown'
            job_level = 'Unknown'
            
            # report_generatorì—ì„œ ì›ë³¸ ì§ì› ë°ì´í„° ì¡°íšŒ
            if report_generator.employee_data is not None:
                try:
                    # EmployeeNumber ì»¬ëŸ¼ìœ¼ë¡œ ì§ì› ì°¾ê¸°
                    if 'EmployeeNumber' in report_generator.employee_data.columns:
                        employee_row = report_generator.employee_data[
                            report_generator.employee_data['EmployeeNumber'] == int(employee_number)
                        ]
                    elif 'employee_number' in report_generator.employee_data.columns:
                        employee_row = report_generator.employee_data[
                            report_generator.employee_data['employee_number'] == int(employee_number)
                        ]
                    else:
                        employee_row = pd.DataFrame()
                    
                    if not employee_row.empty:
                        job_role = employee_row.iloc[0].get('JobRole', 'Unknown')
                        job_level = employee_row.iloc[0].get('JobLevel', 'Unknown')
                        # ë¶€ì„œ ì •ë³´ë„ ì›ë³¸ì—ì„œ ê°€ì ¸ì˜¤ê¸° (ë” ì •í™•í•¨)
                        original_dept = employee_row.iloc[0].get('Department', dept)
                        if original_dept != dept:
                            dept = original_dept
                except Exception as e:
                    print(f"ì§ì› {employee_number} ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            # ê°œë³„ ì§ì› ê²°ê³¼ ì •ë¦¬
            employee_result = {
                'employee_id': employee_number,
                'employee_number': employee_number,
                'department': dept,
                'job_role': job_role,
                'job_level': job_level,
                'risk_score': risk_score,
                'risk_level': risk_level,
                'analysis_timestamp': analysis_timestamp,
                'agent_results': {}
            }
            
            # ê° ì—ì´ì „íŠ¸ ê²°ê³¼ ì¶”ì¶œ
            analysis_result = employee.get('analysis_result', {})
            
            # Structura ê²°ê³¼ (XAI í¬í•¨)
            if 'structura_result' in analysis_result:
                structura = analysis_result['structura_result']
                
                # ê¸°ë³¸ ì˜ˆì¸¡ ì •ë³´
                prediction = structura.get('prediction', {})
                attrition_prob = prediction.get('attrition_probability', 0)
                
                # XAI ì •ë³´ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì •ë³´ ìƒì„±
                feature_importance = structura.get('feature_importance', {})
                if not feature_importance and 'employee_data' in employee.get('analysis_result', {}):
                    # ì§ì› ë°ì´í„°ì—ì„œ ê¸°ë³¸ feature importance ìƒì„±
                    emp_data = employee['analysis_result']['employee_data']
                    feature_importance = {
                        'Age': min(abs(emp_data.get('Age', 30) - 35) / 35, 1.0) * 0.3,
                        'YearsAtCompany': min(emp_data.get('YearsAtCompany', 5) / 20, 1.0) * 0.25,
                        'JobSatisfaction': (5 - emp_data.get('JobSatisfaction', 3)) / 4 * 0.2,
                        'WorkLifeBalance': (5 - emp_data.get('WorkLifeBalance', 3)) / 4 * 0.15,
                        'MonthlyIncome': max(0, (50000 - emp_data.get('MonthlyIncome', 50000)) / 50000) * 0.1
                    }
                
                employee_result['agent_results']['structura'] = {
                    'attrition_probability': attrition_prob,
                    'predicted_attrition': prediction.get('predicted_attrition', 0),
                    'confidence': prediction.get('confidence', 0.8),
                    'feature_importance': feature_importance,
                    'xai_explanation': structura.get('xai_explanation', {
                        'top_factors': list(feature_importance.keys())[:3] if feature_importance else [],
                        'interpretation': f"ì´íƒˆ í™•ë¥  {attrition_prob:.1%}ë¡œ ì˜ˆì¸¡ë¨"
                    }),
                    'shap_values': structura.get('shap_values', {}),
                    'lime_explanation': structura.get('lime_explanation', {})
                }
            
            # Chronos ê²°ê³¼ (XAI í¬í•¨)
            if 'chronos_result' in analysis_result:
                chronos = analysis_result['chronos_result']
                
                # ê¸°ë³¸ ì˜ˆì¸¡ ì •ë³´
                prediction = chronos.get('prediction', {})
                risk_score = prediction.get('risk_score', 0)
                
                # XAI ì •ë³´ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì •ë³´ ìƒì„±
                attention_weights = chronos.get('attention_weights', {})
                if not attention_weights:
                    # ì‹œê°„ ì‹œí€€ìŠ¤ ê¸°ë°˜ ê¸°ë³¸ attention weights ìƒì„±
                    attention_weights = {
                        'recent_period': 0.4,  # ìµœê·¼ ê¸°ê°„ì— ë†’ì€ ê°€ì¤‘ì¹˜
                        'mid_period': 0.35,    # ì¤‘ê°„ ê¸°ê°„
                        'early_period': 0.25   # ì´ˆê¸° ê¸°ê°„
                    }
                
                trend_analysis = chronos.get('trend_analysis', {})
                if not trend_analysis:
                    trend_analysis = {
                        'trend_direction': 'increasing' if risk_score > 0.5 else 'stable',
                        'volatility': 'medium',
                        'seasonal_pattern': 'detected'
                    }
                
                employee_result['agent_results']['chronos'] = {
                    'risk_score': risk_score,
                    'anomaly_score': chronos.get('anomaly_detection', {}).get('anomaly_score', 0),
                    'trend_analysis': trend_analysis,
                    'xai_explanation': chronos.get('xai_explanation', {
                        'temporal_factors': ['recent_performance_decline', 'workload_increase'],
                        'interpretation': f"ì‹œê³„ì—´ ìœ„í—˜ë„ {risk_score:.1%}ë¡œ ë¶„ì„ë¨"
                    }),
                    'attention_weights': attention_weights,
                    'sequence_importance': chronos.get('sequence_importance', {
                        'last_3_months': 0.5,
                        'last_6_months': 0.3,
                        'last_12_months': 0.2
                    })
                }
            
            # Cognita ê²°ê³¼
            if 'cognita_result' in analysis_result:
                cognita = analysis_result['cognita_result']
                employee_result['agent_results']['cognita'] = {
                    'overall_risk_score': cognita.get('overall_risk_score', 0),
                    'network_centrality': cognita.get('network_centrality_score', 0),
                    'relationship_strength': cognita.get('network_stats', {}).get('avg_strength', 0),
                    'influence_score': cognita.get('influence_score', 0)
                }
            
            # Sentio ê²°ê³¼
            if 'sentio_result' in analysis_result:
                sentio = analysis_result['sentio_result']
                employee_result['agent_results']['sentio'] = {
                    'sentiment_score': sentio.get('sentiment_score', 0),
                    'risk_score': sentio.get('psychological_risk_score', 0),
                    'keyword_analysis': sentio.get('risk_keywords', {}),
                    'emotion_distribution': sentio.get('detailed_analysis', {})
                }
            
            # Agora ê²°ê³¼
            if 'agora_result' in analysis_result:
                agora = analysis_result['agora_result']
                employee_result['agent_results']['agora'] = {
                    'market_risk_score': agora.get('market_analysis', {}).get('risk_score', 0),
                    'industry_trend': agora.get('industry_analysis', {}),
                    'job_market_analysis': agora.get('job_market', {}),
                    'external_factors': agora.get('external_factors', {})
                }
            
            individual_results.append(employee_result)
            department_results[normalized_dept]['employees'].append(employee_result)
            
            # ê¸°ì¡´ êµ¬ì¡°ì— ë§ê²Œ ê°œë³„ ì§ì› íŒŒì¼ ì €ì¥
            employee_id = str(employee.get('employee_number', 'Unknown'))
            
            try:
                employee_dir = os.path.join(results_base_dir, normalized_dept, f'employee_{employee_id}')
                os.makedirs(employee_dir, exist_ok=True)
                
                # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„± (íŠ¹ìˆ˜ë¬¸ì ì œê±°)
                safe_timestamp = analysis_timestamp.replace(":", "-").replace(".", "-").replace("T", "_")
                batch_result_file = os.path.join(employee_dir, f'batch_analysis_{safe_timestamp}.json')
                
                # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ë°ì´í„°ë§Œ ì €ì¥
                safe_employee_result = {
                    'employee_id': employee_result.get('employee_id', employee_id),
                    'department': employee_result.get('department', dept),
                    'risk_score': float(employee_result.get('risk_score', 0)) if employee_result.get('risk_score') is not None else 0,
                    'risk_level': str(employee_result.get('risk_level', 'unknown')),
                    'analysis_timestamp': analysis_timestamp,
                    'agent_results': employee_result.get('agent_results', {})
                }
                
                with open(batch_result_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        'analysis_type': 'batch_analysis',
                        'timestamp': analysis_timestamp,
                        'employee_result': safe_employee_result,
                        'applied_settings': data.get('applied_settings', {}),
                        'xai_included': True,
                        'visualizations_generated': False  # ì´ˆê¸°ê°’
                    }, f, indent=2, ensure_ascii=False)
                
                # XAI ì‹œê°í™” ìƒì„± ë° ì €ì¥ (ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ê³„ì† ì§„í–‰)
                visualization_success = False
                try:
                    visualization_success = create_xai_visualizations(employee_result, employee_dir, employee_id)
                except Exception as viz_error:
                    print(f"âš ï¸ ì§ì› {employee_id} XAI ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {str(viz_error)}")
                    visualization_success = False
                
                print(f"âœ… ì§ì› {employee_id} ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥: {batch_result_file}")
                if visualization_success:
                    print(f"âœ… ì§ì› {employee_id} XAI ì‹œê°í™” ìƒì„± ì™„ë£Œ")
                else:
                    print(f"âš ï¸ ì§ì› {employee_id} XAI ì‹œê°í™” ìƒì„± ë¶€ë¶„ ì‹¤íŒ¨")
                    
            except Exception as emp_error:
                print(f"âš ï¸ ì§ì› {employee_id} ê°œë³„ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(emp_error)}")
                # ê°œë³„ ì§ì› íŒŒì¼ ì €ì¥ ì‹¤íŒ¨í•´ë„ ì „ì²´ í”„ë¡œì„¸ìŠ¤ëŠ” ê³„ì† ì§„í–‰
        
        # ë¶€ì„œë³„ ê²°ê³¼ ì €ì¥ (ë°°ì¹˜ ë¶„ì„ ìš”ì•½ ë””ë ‰í† ë¦¬ì—)
        safe_timestamp = analysis_timestamp.replace(":", "-").replace(".", "-").replace("T", "_")
        dept_summary_file = os.path.join(batch_summary_dir, f'department_summary_{safe_timestamp}.json')
        
        try:
            with open(dept_summary_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_timestamp': analysis_timestamp,
                    'total_employees': len(results),
                    'total_departments': len(department_results),
                    'department_results': department_results,
                    'applied_settings': data.get('applied_settings', {}),
                    'summary_statistics': {
                        'overall_risk_distribution': {
                            'ì•ˆì „êµ°': sum(dept['risk_distribution']['ì•ˆì „êµ°'] for dept in department_results.values()),
                            'ì£¼ì˜êµ°': sum(dept['risk_distribution']['ì£¼ì˜êµ°'] for dept in department_results.values()),
                            'ê³ ìœ„í—˜êµ°': sum(dept['risk_distribution']['ê³ ìœ„í—˜êµ°'] for dept in department_results.values())
                        }
                    }
                }, f, indent=2, ensure_ascii=False)
        except Exception as dept_error:
            print(f"âš ï¸ ë¶€ì„œë³„ ìš”ì•½ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(dept_error)}")
        
        # ê°œë³„ ì§ì› ìƒì„¸ ê²°ê³¼ ì €ì¥ (XAI í¬í•¨) - ë°°ì¹˜ ë¶„ì„ ìš”ì•½ ë””ë ‰í† ë¦¬ì—
        individual_file = os.path.join(batch_summary_dir, f'individual_results_{safe_timestamp}.json')
        
        try:
            # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ê°œë³„ ê²°ê³¼ë§Œ ì €ì¥
            safe_individual_results = []
            for result in individual_results:
                safe_result = {
                    'employee_id': str(result.get('employee_id', 'Unknown')),
                    'department': str(result.get('department', 'ë¯¸ë¶„ë¥˜')),
                    'risk_score': float(result.get('risk_score', 0)) if result.get('risk_score') is not None else 0,
                    'risk_level': str(result.get('risk_level', 'unknown')),
                    'analysis_timestamp': analysis_timestamp,
                    'agent_results': result.get('agent_results', {})
                }
                safe_individual_results.append(safe_result)
            
            with open(individual_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_timestamp': analysis_timestamp,
                    'total_employees': len(safe_individual_results),
                    'individual_results': safe_individual_results,
                    'applied_settings': data.get('applied_settings', {}),
                    'xai_included': True,
                    'agents_analyzed': ['structura', 'chronos', 'cognita', 'sentio', 'agora']
                }, f, indent=2, ensure_ascii=False)
        except Exception as ind_error:
            print(f"âš ï¸ ê°œë³„ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(ind_error)}")
        
        # ë¯¸ë¶„ë¥˜ í´ë” ì •ë¦¬ (ì¤‘ë³µ ì œê±°)
        cleanup_success = cleanup_misclassified_folders(individual_results)
        
        print(f"âœ… ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"   ë¶€ì„œë³„ ìš”ì•½: {dept_summary_file}")
        print(f"   ê°œë³„ ìƒì„¸: {individual_file}")
        print(f"   ì´ {len(department_results)}ê°œ ë¶€ì„œ, {len(individual_results)}ëª… ì§ì›")
        if cleanup_success:
            print(f"âœ… ë¯¸ë¶„ë¥˜ í´ë” ì •ë¦¬ ì™„ë£Œ")
        
        return jsonify({
            'success': True,
            'message': f'ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.',
            'files_saved': {
                'department_summary': dept_summary_file,
                'individual_results': individual_file
            },
            'statistics': {
                'total_employees': len(individual_results),
                'total_departments': len(department_results),
                'department_breakdown': {dept: info['total_employees'] for dept, info in department_results.items()}
            }
        })
        
    except Exception as e:
        print(f"âŒ ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500

def _sanitize_folder_name(name: str) -> str:
    """í´ë”ëª…ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë¬¸ìì—´ ì •ë¦¬"""
    if not name or name in ['Unknown', 'N/A', '', None]:
        return 'Unknown'
    
    # íŠ¹ìˆ˜ë¬¸ìë¥¼ ì•ˆì „í•œ ë¬¸ìë¡œ ë³€í™˜
    safe_name = str(name).strip()
    safe_name = safe_name.replace(' ', '_')
    safe_name = safe_name.replace('&', 'and')
    safe_name = safe_name.replace('/', '_')
    safe_name = safe_name.replace('\\', '_')
    safe_name = safe_name.replace(':', '_')
    safe_name = safe_name.replace('*', '_')
    safe_name = safe_name.replace('?', '_')
    safe_name = safe_name.replace('"', '_')
    safe_name = safe_name.replace('<', '_')
    safe_name = safe_name.replace('>', '_')
    safe_name = safe_name.replace('|', '_')
    
    # ì—°ì†ëœ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
    while '__' in safe_name:
        safe_name = safe_name.replace('__', '_')
    
    # ì•ë’¤ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
    safe_name = safe_name.strip('_')
    
    return safe_name if safe_name else 'Unknown'

# _create_hierarchical_path í•¨ìˆ˜ ì œê±°ë¨ - Supervisorì—ì„œ ê³„ì¸µì  ì €ì¥ì„ ë‹´ë‹¹

@app.route('/api/batch-analysis/save-hierarchical-results', methods=['POST'])
def save_hierarchical_batch_results():
    """ê³„ì¸µì  êµ¬ì¡° ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ í™•ì¸ - Supervisorì—ì„œ ì‹¤ì œ ì €ì¥ ì²˜ë¦¬"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({
                'success': False,
                'error': 'ê³„ì¸µì  ê²°ê³¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 400
        
        print(f"ğŸ“Š ê³„ì¸µì  ë°°ì¹˜ ê²°ê³¼ ë°ì´í„° í™•ì¸ ì¤‘...")
        print(f"ğŸ“‹ ë°›ì€ ë°ì´í„° í‚¤: {list(data.keys())}")
        
        # ë°ì´í„° í¬ê¸° í™•ì¸
        import sys
        data_size = sys.getsizeof(str(data))
        print(f"ğŸ“ ë°ì´í„° í¬ê¸°: {data_size:,} bytes ({data_size/1024/1024:.2f} MB)")
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # hierarchical_results ë°ì´í„° ì²˜ë¦¬
        hierarchical_results = data.get('hierarchical_results', {})
        chunk_info = data.get('chunk_info', {})
        
        if not hierarchical_results:
            return jsonify({
                'success': False,
                'error': 'ê³„ì¸µì  ê²°ê³¼ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.'
            }), 400
        
        # ì²­í¬ ë°ì´í„° ì²˜ë¦¬ ë¡œê·¸
        if chunk_info.get('is_chunk'):
            print(f"ğŸ“¦ ì²­í¬ ë°ì´í„° ìˆ˜ì‹ : {chunk_info.get('chunk_index', 'N/A')}/{chunk_info.get('total_chunks', 'N/A')}")
            print(f"ğŸ¢ ì²­í¬ ë¶€ì„œ: {chunk_info.get('department', 'Unknown')}")
        else:
            print(f"ğŸ“Š ì¼ë°˜ ê³„ì¸µì  ë°ì´í„° ìˆ˜ì‹ ")
            
        print(f"ğŸ¢ ë°œê²¬ëœ ë¶€ì„œ ìˆ˜: {len(hierarchical_results)}")
        print(f"ğŸ“‹ ë¶€ì„œ ëª©ë¡: {list(hierarchical_results.keys())}")
        
        # Integrationì—ì„œëŠ” í†µê³„ë§Œ ê³„ì‚° (ì‹¤ì œ ì €ì¥ì€ Supervisorê°€ ë‹´ë‹¹)
        total_employees = 0
        departments_processed = []
        
        # ë°›ì€ ë°ì´í„° í†µê³„ ê³„ì‚°
        for department, dept_data in hierarchical_results.items():
            dept_employees = 0
            departments_processed.append(department)
            
            # ë¶€ì„œë³„ ì§ì› ìˆ˜ ê³„ì‚°
            for job_role, role_data in dept_data.items():
                for job_level, level_data in role_data.items():
                    dept_employees += len(level_data)
            
            total_employees += dept_employees
            print(f"   ğŸ‘¥ {department} ë¶€ì„œ ì§ì› ìˆ˜: {dept_employees}ëª…")
        
        print(f"ğŸ“Š ì´ {total_employees}ëª…ì˜ ì§ì› ë°ì´í„°ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ¢ ì²˜ë¦¬ëœ ë¶€ì„œ: {departments_processed}")
        print(f"â„¹ï¸  ì‹¤ì œ ê³„ì¸µì  ì €ì¥ì€ Supervisor(5006)ì—ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
        # ì‘ë‹µ ë©”ì‹œì§€ êµ¬ì„±
        if chunk_info.get('is_chunk'):
            message = f'ì²­í¬ {chunk_info.get("chunk_index")}/{chunk_info.get("total_chunks")} ë°ì´í„° í™•ì¸ ì™„ë£Œ ({chunk_info.get("department")} ë¶€ì„œ)'
        else:
            message = f'ê³„ì¸µì  ê²°ê³¼ ë°ì´í„°ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. (ì‹¤ì œ ì €ì¥ì€ Supervisorì—ì„œ ì²˜ë¦¬ë¨)'
        
        return jsonify({
            'success': True,
            'message': message,
            'statistics': {
                'total_departments': len(hierarchical_results),
                'total_employees': total_employees,
                'structure': 'Department > JobRole > JobLevel > Employee',
                'note': 'Supervisorì—ì„œ ê³„ì¸µì  ì €ì¥ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.',
                'chunk_info': chunk_info if chunk_info.get('is_chunk') else None
            },
            'departments': departments_processed,
            'analysis_timestamp': timestamp
        })
            
    except Exception as e:
        print(f"âŒ ê³„ì¸µì  ê²°ê³¼ ë°ì´í„° í™•ì¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        print(f"ğŸ“‹ ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        return jsonify({
            'success': False,
            'error': f'ê³„ì¸µì  ê²°ê³¼ ë°ì´í„° í™•ì¸ ì‹¤íŒ¨: {str(e)}'
        }), 500

@app.route('/api/batch-analysis/load-results', methods=['GET'])
def load_batch_analysis_results():
    """ì €ì¥ëœ ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ìœ¼ë¡œ ë¡œë“œ"""
    try:
        timestamp = request.args.get('timestamp')
        if not timestamp:
            return jsonify({
                'success': False,
                'error': 'íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        batch_summary_dir = os.path.join(project_root, 'app/results/batch_analysis')
        
        print(f"ğŸ“Š ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì‹œë„: timestamp={timestamp}")
        print(f"ğŸ“ ê²€ìƒ‰ ë””ë ‰í† ë¦¬: {batch_summary_dir}")
        
        if not os.path.exists(batch_summary_dir):
            return jsonify({
                'success': False,
                'error': 'ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 404
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ì™€ ì¼ì¹˜í•˜ëŠ” íŒŒì¼ ì°¾ê¸°
        target_files = []
        for filename in os.listdir(batch_summary_dir):
            if timestamp.replace(':', '-').replace('.', '-') in filename:
                target_files.append(filename)
        
        if not target_files:
            # ì •í™•í•œ ë§¤ì¹˜ê°€ ì—†ìœ¼ë©´ ê°€ì¥ ìµœê·¼ íŒŒì¼ ì‚¬ìš©
            all_files = [f for f in os.listdir(batch_summary_dir) if f.startswith('individual_results_')]
            if all_files:
                target_files = [sorted(all_files)[-1]]  # ê°€ì¥ ìµœì‹  íŒŒì¼
            else:
                return jsonify({
                    'success': False,
                    'error': 'í•´ë‹¹ íƒ€ì„ìŠ¤íƒ¬í”„ì˜ ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
                }), 404
        
        # individual_results íŒŒì¼ ìš°ì„  ë¡œë“œ
        individual_file = None
        for filename in target_files:
            if filename.startswith('individual_results_'):
                individual_file = filename
                break
        
        if not individual_file:
            return jsonify({
                'success': False,
                'error': 'ê°œë³„ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }), 404
        
        # íŒŒì¼ ë¡œë“œ
        individual_file_path = os.path.join(batch_summary_dir, individual_file)
        with open(individual_file_path, 'r', encoding='utf-8') as f:
            individual_data = json.load(f)
        
        print(f"âœ… ê°œë³„ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì„±ê³µ: {individual_file}")
        print(f"ğŸ“Š ë¡œë“œëœ ì§ì› ìˆ˜: {len(individual_data.get('results', []))}")
        
        # ë¶€ì„œë³„ ìš”ì•½ íŒŒì¼ë„ ë¡œë“œ ì‹œë„
        summary_file = individual_file.replace('individual_results_', 'department_summary_')
        summary_file_path = os.path.join(batch_summary_dir, summary_file)
        summary_data = {}
        
        if os.path.exists(summary_file_path):
            with open(summary_file_path, 'r', encoding='utf-8') as f:
                summary_data = json.load(f)
            print(f"âœ… ë¶€ì„œë³„ ìš”ì•½ íŒŒì¼ë„ ë¡œë“œ: {summary_file}")
        
        # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
        response_data = {
            'success': True,
            'results': individual_data.get('results', []),
            'total_employees': individual_data.get('total_employees', len(individual_data.get('results', []))),
            'completed_employees': individual_data.get('completed_employees', len(individual_data.get('results', []))),
            'analysis_timestamp': individual_data.get('analysis_timestamp', timestamp),
            'summary': {
                'total_employees': len(individual_data.get('results', [])),
                'successful_analyses': len(individual_data.get('results', [])),
                'failed_analyses': 0,
                'success_rate': 1.0
            },
            'department_summary': summary_data.get('department_results', {}),
            'loaded_from': 'saved_files'
        }
        
        return jsonify(response_data)
        
    except Exception as e:
        print(f"âŒ ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
        }), 500

@app.route('/api/batch-analysis/list-saved-files', methods=['GET'])
def list_saved_batch_analysis_files():
    """ì €ì¥ëœ ë°°ì¹˜ ë¶„ì„ íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
    try:
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        batch_summary_dir = os.path.join(project_root, 'app/results/batch_analysis')
        
        if not os.path.exists(batch_summary_dir):
            return jsonify({
                'success': True,
                'files': []
            })
        
        # individual_results íŒŒì¼ë“¤ ì°¾ê¸°
        individual_files = []
        for filename in os.listdir(batch_summary_dir):
            if filename.startswith('individual_results_') and filename.endswith('.json'):
                file_path = os.path.join(batch_summary_dir, filename)
                try:
                    # íŒŒì¼ ì •ë³´ ì¶”ì¶œ
                    stat = os.stat(file_path)
                    file_size = stat.st_size
                    modified_time = datetime.fromtimestamp(stat.st_mtime).isoformat()
                    
                    # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ (íŒŒì¼ëª…ì—ì„œ)
                    timestamp_part = filename.replace('individual_results_', '').replace('.json', '')
                    
                    # íŒŒì¼ ë‚´ìš©ì—ì„œ ì§ì› ìˆ˜ í™•ì¸
                    employee_count = 0
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            if 'individual_results' in data:
                                employee_count = len(data['individual_results'])
                            elif 'results' in data:
                                employee_count = len(data['results'])
                            else:
                                employee_count = data.get('total_employees', 0)
                    except:
                        pass
                    
                    individual_files.append({
                        'filename': filename,
                        'timestamp': timestamp_part,
                        'file_size': file_size,
                        'modified_time': modified_time,
                        'employee_count': employee_count,
                        'display_name': f"ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ({employee_count}ëª…) - {modified_time[:19].replace('T', ' ')}"
                    })
                except Exception as e:
                    print(f"íŒŒì¼ {filename} ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
        
        # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹  ìˆœ)
        individual_files.sort(key=lambda x: x['modified_time'], reverse=True)
        
        return jsonify({
            'success': True,
            'files': individual_files
        })
        
    except Exception as e:
        print(f"âŒ ì €ì¥ëœ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
        }), 500

@app.route('/api/batch-analysis/delete-saved-file', methods=['DELETE'])
def delete_saved_batch_analysis_file():
    """ì €ì¥ëœ ë°°ì¹˜ ë¶„ì„ íŒŒì¼ ì‚­ì œ"""
    try:
        data = request.get_json()
        filename = data.get('filename')
        
        if not filename:
            return jsonify({
                'success': False,
                'error': 'íŒŒì¼ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        batch_summary_dir = os.path.join(project_root, 'app/results/batch_analysis')
        
        # individual_results íŒŒì¼ ì‚­ì œ
        individual_file_path = os.path.join(batch_summary_dir, filename)
        deleted_files = []
        
        if os.path.exists(individual_file_path):
            os.remove(individual_file_path)
            deleted_files.append(filename)
            print(f"âœ… ì‚­ì œë¨: {filename}")
        
        # í•´ë‹¹í•˜ëŠ” department_summary íŒŒì¼ë„ ì‚­ì œ
        summary_filename = filename.replace('individual_results_', 'department_summary_')
        summary_file_path = os.path.join(batch_summary_dir, summary_filename)
        
        if os.path.exists(summary_file_path):
            os.remove(summary_file_path)
            deleted_files.append(summary_filename)
            print(f"âœ… ì‚­ì œë¨: {summary_filename}")
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œí•´ì„œ ê´€ë ¨ëœ ê³„ì¸µì  íŒŒì¼ë“¤ë„ ì‚­ì œ
        timestamp_part = filename.replace('individual_results_', '').replace('.json', '')
        hierarchical_dir = os.path.join(project_root, 'app/results/hierarchical_analysis')
        
        if os.path.exists(hierarchical_dir):
            for hierarchical_file in os.listdir(hierarchical_dir):
                if timestamp_part.replace('-', '').replace('_', '') in hierarchical_file.replace('-', '').replace('_', ''):
                    hierarchical_file_path = os.path.join(hierarchical_dir, hierarchical_file)
                    try:
                        os.remove(hierarchical_file_path)
                        deleted_files.append(f"hierarchical_analysis/{hierarchical_file}")
                        print(f"âœ… ê³„ì¸µì  íŒŒì¼ ì‚­ì œë¨: {hierarchical_file}")
                    except Exception as e:
                        print(f"âš ï¸ ê³„ì¸µì  íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {hierarchical_file} - {e}")
        
        if not deleted_files:
            return jsonify({
                'success': False,
                'error': 'ì‚­ì œí•  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }), 404
        
        return jsonify({
            'success': True,
            'deleted_files': deleted_files,
            'message': f'{len(deleted_files)}ê°œ íŒŒì¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.'
        })
        
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
        }), 500

@app.route('/api/batch-analysis/load-file/<filename>', methods=['GET'])
def load_batch_analysis_file(filename):
    """ì €ì¥ëœ ë°°ì¹˜ ë¶„ì„ íŒŒì¼ ë¡œë“œ"""
    try:
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        batch_summary_dir = os.path.join(project_root, 'app/results/batch_analysis')
        
        file_path = os.path.join(batch_summary_dir, filename)
        
        if not os.path.exists(file_path):
            return jsonify({
                'success': False,
                'error': 'íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }), 404
        
        # íŒŒì¼ ì½ê¸°
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"âœ… íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {filename}")
        print(f"ğŸ“Š ë°ì´í„° êµ¬ì¡°: {list(data.keys())}")
        
        return jsonify({
            'success': True,
            'data': data,
            'filename': filename
        })
        
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
        }), 500

@app.route('/api/batch-analysis/cleanup-misclassified', methods=['POST'])
def cleanup_misclassified_manual():
    """ìˆ˜ë™ìœ¼ë¡œ ë¯¸ë¶„ë¥˜ í´ë” ì •ë¦¬"""
    try:
        # ìµœê·¼ ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ íŒŒì¼ì—ì„œ ë¶€ì„œ ì •ë³´ ë¡œë“œ
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        batch_summary_dir = os.path.join(project_root, 'app/results/batch_analysis')
        
        if not os.path.exists(batch_summary_dir):
            return jsonify({
                'success': False,
                'error': 'ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }), 404
        
        # ê°€ì¥ ìµœê·¼ individual_results íŒŒì¼ ì°¾ê¸°
        individual_files = [f for f in os.listdir(batch_summary_dir) 
                          if f.startswith('individual_results_') and f.endswith('.json')]
        
        if not individual_files:
            return jsonify({
                'success': False,
                'error': 'ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }), 404
        
        # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì„ íƒ
        latest_file = sorted(individual_files)[-1]
        file_path = os.path.join(batch_summary_dir, latest_file)
        
        # íŒŒì¼ì—ì„œ ë¶€ì„œ ì •ë³´ ë¡œë“œ
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        individual_results = data.get('individual_results', [])
        
        if not individual_results:
            return jsonify({
                'success': False,
                'error': 'ê°œë³„ ì§ì› ê²°ê³¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 400
        
        # ë¯¸ë¶„ë¥˜ í´ë” ì •ë¦¬ ì‹¤í–‰
        cleanup_success = cleanup_misclassified_folders(individual_results)
        
        if cleanup_success:
            return jsonify({
                'success': True,
                'message': 'ë¯¸ë¶„ë¥˜ í´ë” ì •ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
                'source_file': latest_file,
                'processed_employees': len(individual_results)
            })
        else:
            return jsonify({
                'success': False,
                'error': 'ë¯¸ë¶„ë¥˜ í´ë” ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
            }), 500
        
    except Exception as e:
        print(f"âŒ ìˆ˜ë™ ë¯¸ë¶„ë¥˜ í´ë” ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'ë¯¸ë¶„ë¥˜ í´ë” ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
        }), 500

@app.route('/api/statistics/load-from-files', methods=['GET'])
def load_statistics_from_files():
    """ì €ì¥ëœ ë¶€ì„œë³„ íŒŒì¼ë“¤ì—ì„œ í†µê³„ ë°ì´í„°ë¥¼ ë¡œë“œ"""
    try:
        group_by = request.args.get('group_by', 'department')
        department_filter = request.args.get('department', None)
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        results_base_dir = os.path.join(project_root, 'app/results')
        
        print(f"ğŸ“Š ì €ì¥ëœ íŒŒì¼ì—ì„œ í†µê³„ ë¡œë“œ: group_by={group_by}, department_filter={department_filter}")
        print(f"ğŸ“ ê²°ê³¼ ë””ë ‰í† ë¦¬: {results_base_dir}")
        
        statistics = {}
        
        # ë¶€ì„œë³„ ë””ë ‰í† ë¦¬ ìŠ¤ìº”
        department_dirs = [d for d in os.listdir(results_base_dir) 
                          if os.path.isdir(os.path.join(results_base_dir, d)) 
                          and d not in ['batch_analysis', 'global_reports', 'hierarchical_analysis', 'models', 'temp', 'departments', 'ë¯¸ë¶„ë¥˜']]
        
        print(f"ğŸ¢ ë°œê²¬ëœ ë¶€ì„œ ë””ë ‰í† ë¦¬: {department_dirs}")
        
        for dept_dir in department_dirs:
            dept_path = os.path.join(results_base_dir, dept_dir)
            dept_index_file = os.path.join(dept_path, 'department_index.json')
            
            if not os.path.exists(dept_index_file):
                print(f"âš ï¸ {dept_dir} ë¶€ì„œì˜ ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # ë¶€ì„œ ì¸ë±ìŠ¤ íŒŒì¼ ë¡œë“œ
            with open(dept_index_file, 'r', encoding='utf-8') as f:
                dept_data = json.load(f)
            
            department_name = dept_data.get('department', dept_dir.replace('_', ' '))
            
            # ë¶€ì„œ í•„í„°ë§
            if department_filter and department_name != department_filter:
                continue
            
            # ê·¸ë£¹í™” ë°©ì‹ì— ë”°ë¥¸ í†µê³„ ìƒì„±
            if group_by == 'department':
                # ë¶€ì„œë³„ í†µê³„
                dept_stats = dept_data.get('statistics', {})
                total_employees = dept_stats.get('total_employees', 0)
                
                # ìœ„í—˜ë„ ë¶„í¬ëŠ” ê°œë³„ ì§ì› íŒŒì¼ì—ì„œ ê³„ì‚°í•´ì•¼ í•¨
                risk_distribution = calculate_department_risk_distribution(dept_path, dept_data)
                
                statistics[department_name] = {
                    'total_employees': total_employees,
                    'high_risk': risk_distribution.get('high_risk', 0),
                    'medium_risk': risk_distribution.get('medium_risk', 0),
                    'low_risk': risk_distribution.get('low_risk', 0),
                    'avg_risk_score': risk_distribution.get('avg_risk_score', 0),
                    'common_risk_factors': risk_distribution.get('common_risk_factors', {})
                }
                
            elif group_by == 'job_role':
                # ì§ë¬´ë³„ í†µê³„
                job_roles = dept_data.get('job_roles', {})
                for job_role, levels in job_roles.items():
                    if department_filter and department_name != department_filter:
                        continue
                    
                    role_stats = calculate_job_role_risk_distribution(dept_path, job_role, levels)
                    statistics[job_role] = role_stats
                    
            elif group_by == 'job_level':
                # ì§ê¸‰ë³„ í†µê³„
                position_counts = dept_data.get('statistics', {}).get('position_count', {})
                for level, count in position_counts.items():
                    level_name = f"Level {level}"
                    if level_name not in statistics:
                        statistics[level_name] = {
                            'total_employees': 0,
                            'high_risk': 0,
                            'medium_risk': 0,
                            'low_risk': 0,
                            'avg_risk_score': 0,
                            'common_risk_factors': {}
                        }
                    
                    level_stats = calculate_job_level_risk_distribution(dept_path, level)
                    statistics[level_name]['total_employees'] += level_stats.get('total_employees', 0)
                    statistics[level_name]['high_risk'] += level_stats.get('high_risk', 0)
                    statistics[level_name]['medium_risk'] += level_stats.get('medium_risk', 0)
                    statistics[level_name]['low_risk'] += level_stats.get('low_risk', 0)
        
        # í‰ê·  ìœ„í—˜ë„ ì¬ê³„ì‚° (ì§ê¸‰ë³„ì¸ ê²½ìš°)
        if group_by == 'job_level':
            for level_name in statistics:
                total = statistics[level_name]['total_employees']
                if total > 0:
                    # ê°€ì¤‘í‰ê· ìœ¼ë¡œ ê³„ì‚° (ì‹¤ì œë¡œëŠ” ê°œë³„ íŒŒì¼ì—ì„œ ì½ì–´ì™€ì•¼ í•¨)
                    statistics[level_name]['avg_risk_score'] = 0.5  # ì„ì‹œê°’
        
        print(f"ğŸ“Š í†µê³„ ìƒì„± ì™„ë£Œ: {len(statistics)}ê°œ ê·¸ë£¹")
        
        return jsonify({
            'success': True,
            'group_by': group_by,
            'department_filter': department_filter,
            'statistics': statistics,
            'generated_at': datetime.now().isoformat(),
            'data_source': 'saved_files'
        })
        
    except Exception as e:
        print(f"âŒ íŒŒì¼ ê¸°ë°˜ í†µê³„ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        import traceback
        print(f"ğŸ“‹ ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        return jsonify({
            'success': False,
            'error': f'íŒŒì¼ ê¸°ë°˜ í†µê³„ ë¡œë“œ ì‹¤íŒ¨: {str(e)}'
        }), 500

def calculate_department_risk_distribution(dept_path, dept_data):
    """ë¶€ì„œë³„ ìœ„í—˜ë„ ë¶„í¬ ê³„ì‚°"""
    try:
        risk_distribution = {
            'high_risk': 0,
            'medium_risk': 0,
            'low_risk': 0,
            'avg_risk_score': 0,
            'common_risk_factors': {}
        }
        
        total_risk_score = 0
        total_employees = 0
        
        # ê° ì§ë¬´ë³„ë¡œ ì§ì› íŒŒì¼ë“¤ì„ ìŠ¤ìº”
        job_roles = dept_data.get('job_roles', {})
        for job_role, levels in job_roles.items():
            job_role_path = os.path.join(dept_path, job_role)
            if not os.path.exists(job_role_path):
                continue
                
            for level, employee_ids in levels.items():
                level_path = os.path.join(job_role_path, level)
                if not os.path.exists(level_path):
                    continue
                
                for employee_id in employee_ids:
                    employee_dir = os.path.join(level_path, f'employee_{employee_id}')
                    if not os.path.exists(employee_dir):
                        continue
                    
                    # ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
                    batch_files = [f for f in os.listdir(employee_dir) 
                                 if f.startswith('batch_analysis_') and f.endswith('.json')]
                    
                    if batch_files:
                        latest_batch_file = sorted(batch_files)[-1]
                        batch_file_path = os.path.join(employee_dir, latest_batch_file)
                        
                        try:
                            with open(batch_file_path, 'r', encoding='utf-8') as f:
                                batch_data = json.load(f)
                            
                            risk_score = batch_data.get('risk_score', 0)
                            total_risk_score += risk_score
                            total_employees += 1
                            
                            # ìœ„í—˜ë„ ë¶„ë¥˜
                            if risk_score >= 0.7:
                                risk_distribution['high_risk'] += 1
                            elif risk_score >= 0.3:
                                risk_distribution['medium_risk'] += 1
                            else:
                                risk_distribution['low_risk'] += 1
                                
                        except Exception as e:
                            print(f"âš ï¸ ì§ì› {employee_id} ë°°ì¹˜ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        
        # í‰ê·  ìœ„í—˜ë„ ê³„ì‚°
        if total_employees > 0:
            risk_distribution['avg_risk_score'] = total_risk_score / total_employees
        
        return risk_distribution
        
    except Exception as e:
        print(f"âŒ ë¶€ì„œ ìœ„í—˜ë„ ë¶„í¬ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return {
            'high_risk': 0,
            'medium_risk': 0,
            'low_risk': 0,
            'avg_risk_score': 0,
            'common_risk_factors': {}
        }

def calculate_job_role_risk_distribution(dept_path, job_role, levels):
    """ì§ë¬´ë³„ ìœ„í—˜ë„ ë¶„í¬ ê³„ì‚°"""
    try:
        risk_distribution = {
            'total_employees': 0,
            'high_risk': 0,
            'medium_risk': 0,
            'low_risk': 0,
            'avg_risk_score': 0,
            'common_risk_factors': {}
        }
        
        total_risk_score = 0
        total_employees = 0
        
        job_role_path = os.path.join(dept_path, job_role)
        if not os.path.exists(job_role_path):
            return risk_distribution
        
        for level, employee_ids in levels.items():
            level_path = os.path.join(job_role_path, level)
            if not os.path.exists(level_path):
                continue
            
            for employee_id in employee_ids:
                employee_dir = os.path.join(level_path, f'employee_{employee_id}')
                if not os.path.exists(employee_dir):
                    continue
                
                # ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
                batch_files = [f for f in os.listdir(employee_dir) 
                             if f.startswith('batch_analysis_') and f.endswith('.json')]
                
                if batch_files:
                    latest_batch_file = sorted(batch_files)[-1]
                    batch_file_path = os.path.join(employee_dir, latest_batch_file)
                    
                    try:
                        with open(batch_file_path, 'r', encoding='utf-8') as f:
                            batch_data = json.load(f)
                        
                        risk_score = batch_data.get('risk_score', 0)
                        total_risk_score += risk_score
                        total_employees += 1
                        
                        # ìœ„í—˜ë„ ë¶„ë¥˜
                        if risk_score >= 0.7:
                            risk_distribution['high_risk'] += 1
                        elif risk_score >= 0.3:
                            risk_distribution['medium_risk'] += 1
                        else:
                            risk_distribution['low_risk'] += 1
                            
                    except Exception as e:
                        print(f"âš ï¸ ì§ì› {employee_id} ë°°ì¹˜ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        
        risk_distribution['total_employees'] = total_employees
        if total_employees > 0:
            risk_distribution['avg_risk_score'] = total_risk_score / total_employees
        
        return risk_distribution
        
    except Exception as e:
        print(f"âŒ ì§ë¬´ ìœ„í—˜ë„ ë¶„í¬ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return {
            'total_employees': 0,
            'high_risk': 0,
            'medium_risk': 0,
            'low_risk': 0,
            'avg_risk_score': 0,
            'common_risk_factors': {}
        }

def calculate_job_level_risk_distribution(dept_path, level):
    """ì§ê¸‰ë³„ ìœ„í—˜ë„ ë¶„í¬ ê³„ì‚°"""
    try:
        risk_distribution = {
            'total_employees': 0,
            'high_risk': 0,
            'medium_risk': 0,
            'low_risk': 0,
            'avg_risk_score': 0,
            'common_risk_factors': {}
        }
        
        total_risk_score = 0
        total_employees = 0
        
        # ëª¨ë“  ì§ë¬´ì—ì„œ í•´ë‹¹ ë ˆë²¨ì˜ ì§ì›ë“¤ì„ ì°¾ê¸°
        for item in os.listdir(dept_path):
            item_path = os.path.join(dept_path, item)
            if not os.path.isdir(item_path) or item == 'department_index.json':
                continue
            
            level_path = os.path.join(item_path, str(level))
            if not os.path.exists(level_path):
                continue
            
            for employee_dir_name in os.listdir(level_path):
                if not employee_dir_name.startswith('employee_'):
                    continue
                
                employee_dir = os.path.join(level_path, employee_dir_name)
                if not os.path.isdir(employee_dir):
                    continue
                
                # ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
                batch_files = [f for f in os.listdir(employee_dir) 
                             if f.startswith('batch_analysis_') and f.endswith('.json')]
                
                if batch_files:
                    latest_batch_file = sorted(batch_files)[-1]
                    batch_file_path = os.path.join(employee_dir, latest_batch_file)
                    
                    try:
                        with open(batch_file_path, 'r', encoding='utf-8') as f:
                            batch_data = json.load(f)
                        
                        risk_score = batch_data.get('risk_score', 0)
                        total_risk_score += risk_score
                        total_employees += 1
                        
                        # ìœ„í—˜ë„ ë¶„ë¥˜
                        if risk_score >= 0.7:
                            risk_distribution['high_risk'] += 1
                        elif risk_score >= 0.3:
                            risk_distribution['medium_risk'] += 1
                        else:
                            risk_distribution['low_risk'] += 1
                            
                    except Exception as e:
                        print(f"âš ï¸ ì§ì› ë°°ì¹˜ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        
        risk_distribution['total_employees'] = total_employees
        if total_employees > 0:
            risk_distribution['avg_risk_score'] = total_risk_score / total_employees
        
        return risk_distribution
        
    except Exception as e:
        print(f"âŒ ì§ê¸‰ ìœ„í—˜ë„ ë¶„í¬ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return {
            'total_employees': 0,
            'high_risk': 0,
            'medium_risk': 0,
            'low_risk': 0,
            'avg_risk_score': 0,
            'common_risk_factors': {}
        }

@app.route('/api/results/list-all-employees', methods=['GET'])
def list_all_employees_from_results():
    """results í´ë”ì—ì„œ ëª¨ë“  ì§ì› ë¶„ì„ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ"""
    try:
        import os
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        results_dir = os.path.join(project_root, 'app/results')
        
        if not os.path.exists(results_dir):
            return jsonify({
                'success': False,
                'error': 'results í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }), 404
        
        print(f"ğŸ“‚ results í´ë” ìŠ¤ìº” ì‹œì‘: {results_dir}")
        
        employees = []
        
        # results í´ë” ì „ì²´ íƒìƒ‰
        for root, dirs, files in os.walk(results_dir):
            # employee_ í´ë” ì°¾ê¸°
            if os.path.basename(root).startswith('employee_'):
                employee_id = os.path.basename(root).replace('employee_', '')
                
                # employee_info.json íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                employee_info_file = os.path.join(root, 'employee_info.json')
                comprehensive_report_file = os.path.join(root, 'comprehensive_report.json')
                
                if os.path.exists(employee_info_file):
                    try:
                        with open(employee_info_file, 'r', encoding='utf-8') as f:
                            emp_info = json.load(f)
                        
                        emp_data = emp_info.get('employee_data', {})
                        
                        # comprehensive_reportì—ì„œ ì „ì²´ ìœ„í—˜ë„ ê°€ì ¸ì˜¤ê¸°
                        risk_score = 0
                        risk_level = 'UNKNOWN'
                        
                        if os.path.exists(comprehensive_report_file):
                            with open(comprehensive_report_file, 'r', encoding='utf-8') as f:
                                comp_report = json.load(f)
                                overall_assessment = comp_report.get('comprehensive_assessment', {})
                                risk_score = overall_assessment.get('overall_risk_score', 0)
                                risk_level = overall_assessment.get('overall_risk_level', 'UNKNOWN')
                        
                        # ê° ì—ì´ì „íŠ¸ ê²°ê³¼ íŒŒì¼ì—ì„œ ì§ì ‘ ì ìˆ˜ ì¶”ì¶œ
                        structura_score = 0
                        chronos_score = 0
                        cognita_score = 0
                        sentio_score = 0
                        agora_score = 0
                        
                        # Structura ì ìˆ˜
                        structura_file = os.path.join(root, 'structura_result.json')
                        if os.path.exists(structura_file):
                            try:
                                with open(structura_file, 'r', encoding='utf-8') as f:
                                    structura_data = json.load(f)
                                    structura_score = structura_data.get('prediction', {}).get('attrition_probability', 0)
                            except:
                                pass
                        
                        # Chronos ì ìˆ˜
                        chronos_file = os.path.join(root, 'chronos_result.json')
                        if os.path.exists(chronos_file):
                            try:
                                with open(chronos_file, 'r', encoding='utf-8') as f:
                                    chronos_data = json.load(f)
                                    chronos_score = chronos_data.get('prediction', {}).get('risk_score', 0)
                            except:
                                pass
                        
                        # Cognita ì ìˆ˜
                        cognita_file = os.path.join(root, 'cognita_result.json')
                        if os.path.exists(cognita_file):
                            try:
                                with open(cognita_file, 'r', encoding='utf-8') as f:
                                    cognita_data = json.load(f)
                                    cognita_score = cognita_data.get('risk_analysis', {}).get('overall_risk_score', 0)
                            except:
                                pass
                        
                        # Sentio ì ìˆ˜
                        sentio_file = os.path.join(root, 'sentio_result.json')
                        if os.path.exists(sentio_file):
                            try:
                                with open(sentio_file, 'r', encoding='utf-8') as f:
                                    sentio_data = json.load(f)
                                    sentio_score = sentio_data.get('sentiment_analysis', {}).get('risk_score', 0)
                                    if sentio_score == 0:
                                        sentio_score = sentio_data.get('psychological_risk_score', 0)
                            except:
                                pass
                        
                        # Agora ì ìˆ˜
                        agora_file = os.path.join(root, 'agora_result.json')
                        if os.path.exists(agora_file):
                            try:
                                with open(agora_file, 'r', encoding='utf-8') as f:
                                    agora_data = json.load(f)
                                    agora_score = agora_data.get('market_analysis', {}).get('risk_score', 0)
                                    if agora_score == 0:
                                        agora_score = agora_data.get('risk_score', 0)
                            except:
                                pass
                        
                        # ê²½ë¡œì—ì„œ ë¶€ì„œ/ì§ë¬´/ì§ê¸‰ ì •ë³´ ì¶”ì¶œ
                        path_parts = root.replace(results_dir, '').strip(os.sep).split(os.sep)
                        
                        employee_entry = {
                            'employee_id': employee_id,
                            'employee_number': emp_data.get('EmployeeNumber', employee_id),
                            'name': f"ì§ì› {employee_id}",  # ì‹¤ì œ ì´ë¦„ì´ ìˆë‹¤ë©´ ì‚¬ìš©
                            'department': emp_data.get('Department', path_parts[0] if path_parts else 'ë¯¸ë¶„ë¥˜'),
                            'job_role': emp_data.get('JobRole', path_parts[1] if len(path_parts) > 1 else 'ë¯¸ë¶„ë¥˜'),
                            'position': emp_data.get('JobLevel', path_parts[2] if len(path_parts) > 2 else None),
                            'age': emp_data.get('Age'),
                            'years_at_company': emp_data.get('YearsAtCompany'),
                            'job_satisfaction': emp_data.get('JobSatisfaction'),
                            'work_life_balance': emp_data.get('WorkLifeBalance'),
                            'risk_score': risk_score,
                            'risk_level': risk_level,
                            'structura_score': structura_score,
                            'chronos_score': chronos_score,
                            'cognita_score': cognita_score,
                            'sentio_score': sentio_score,
                            'agora_score': agora_score,
                            'has_comprehensive_report': os.path.exists(comprehensive_report_file),
                            'folder_path': root
                        }
                        
                        employees.append(employee_entry)
                        
                    except Exception as e:
                        print(f"âš ï¸ employee {employee_id} ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
                        continue
        
        print(f"âœ… ì´ {len(employees)}ëª…ì˜ ì§ì› ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # ìœ„í—˜ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        employees.sort(key=lambda x: x['risk_score'], reverse=True)
        
        return jsonify({
            'success': True,
            'results': employees,
            'total_employees': len(employees),
            'completed_employees': len(employees),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        import traceback
        print(f"âŒ ì§ì› ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        print(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': f'ì§ì› ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
        }), 500


@app.route('/api/generate-employee-report', methods=['POST'])
def generate_employee_report():
    """ê°œë³„ ì§ì› ë³´ê³ ì„œ ìƒì„± (ì €ì¥ëœ íŒŒì¼ ê¸°ë°˜)"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({
                'success': False,
                'error': 'ìš”ì²­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 400
        
        employee_id = data.get('employee_id')
        department = data.get('department', 'ë¯¸ë¶„ë¥˜')
        job_role = data.get('job_role')
        position = data.get('position')
        risk_level = data.get('risk_level', 'unknown')
        risk_score = data.get('risk_score', 0)
        agent_scores = data.get('agent_scores', {})
        
        if not employee_id:
            return jsonify({
                'success': False,
                'error': 'ì§ì› IDê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        print(f"ğŸ“ ì§ì› {employee_id} ë³´ê³ ì„œ ìƒì„± ì‹œì‘")
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        import os
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        
        # ë¶€ì„œëª… ì •ê·œí™”
        dept_mapping = {
            'Human Resources': 'Human_Resources',
            'Research & Development': 'Research_&_Development', 
            'Research and Development': 'Research_&_Development',
            'R&D': 'Research_&_Development',
            'Sales': 'Sales',
            'HR': 'Human_Resources'
        }
        normalized_dept = dept_mapping.get(department, department.replace(' ', '_').replace('&', '_&_'))
        
        # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ê²½ë¡œ íƒìƒ‰
        possible_paths = []
        
        # 1. ê³„ì¸µ êµ¬ì¡° ê²½ë¡œ (department/job_role/position/employee_id)
        if job_role and position:
            normalized_job_role = job_role.replace(' ', '_').replace('&', '_&_')
            hierarchical_path = os.path.join(project_root, 'app/results', normalized_dept, normalized_job_role, str(position), f'employee_{employee_id}')
            possible_paths.append(hierarchical_path)
        
        # 2. ê°„ì†Œí™”ëœ ê²½ë¡œ (department/employee_id)
        simplified_path = os.path.join(project_root, 'app/results', normalized_dept, f'employee_{employee_id}')
        possible_paths.append(simplified_path)
        
        # 3. ì›ë³¸ ë¶€ì„œëª…ìœ¼ë¡œ ì‹œë„
        original_dept_path = os.path.join(project_root, 'app/results', department.replace(' ', '_'), f'employee_{employee_id}')
        possible_paths.append(original_dept_path)
        
        # ê²½ë¡œ íƒìƒ‰
        employee_dir = None
        for path in possible_paths:
            if os.path.exists(path):
                employee_dir = path
                print(f"âœ… ì§ì› ë°ì´í„° ê²½ë¡œ ë°œê²¬: {path}")
                break
        
        if not employee_dir:
            # ì „ì²´ results í´ë”ì—ì„œ ê²€ìƒ‰
            results_dir = os.path.join(project_root, 'app/results')
            found = False
            for root, dirs, files in os.walk(results_dir):
                if f'employee_{employee_id}' in root:
                    employee_dir = root
                    found = True
                    print(f"âœ… ì§ì› ë°ì´í„° ê²½ë¡œ ê²€ìƒ‰ìœ¼ë¡œ ë°œê²¬: {employee_dir}")
                    break
            
            if not found:
                return jsonify({
                    'success': False,
                    'error': f'ì§ì› {employee_id}ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ëœ ê²½ë¡œ: {", ".join(possible_paths[:2])}'
                }), 404
        
        # ì¢…í•© ë³´ê³ ì„œ ë¡œë“œ
        comprehensive_report_file = os.path.join(employee_dir, 'comprehensive_report.json')
        comprehensive_report = {}
        if os.path.exists(comprehensive_report_file):
            with open(comprehensive_report_file, 'r', encoding='utf-8') as f:
                comprehensive_report = json.load(f)
                print(f"âœ… ì¢…í•© ë³´ê³ ì„œ ë¡œë“œ ì™„ë£Œ")
        
        # ê° ì—ì´ì „íŠ¸ ê²°ê³¼ íŒŒì¼ ë¡œë“œ
        agent_data = {}
        
        # Structura ê²°ê³¼ ë¡œë“œ
        structura_file = os.path.join(employee_dir, 'structura_result.json')
        if os.path.exists(structura_file):
            with open(structura_file, 'r', encoding='utf-8') as f:
                agent_data['structura'] = json.load(f)
        
        # Chronos ê²°ê³¼ ë¡œë“œ
        chronos_file = os.path.join(employee_dir, 'chronos_result.json')
        if os.path.exists(chronos_file):
            with open(chronos_file, 'r', encoding='utf-8') as f:
                agent_data['chronos'] = json.load(f)
        
        # Cognita ê²°ê³¼ ë¡œë“œ
        cognita_file = os.path.join(employee_dir, 'cognita_result.json')
        if os.path.exists(cognita_file):
            with open(cognita_file, 'r', encoding='utf-8') as f:
                agent_data['cognita'] = json.load(f)
        
        # Sentio ê²°ê³¼ ë¡œë“œ
        sentio_file = os.path.join(employee_dir, 'sentio_result.json')
        if os.path.exists(sentio_file):
            with open(sentio_file, 'r', encoding='utf-8') as f:
                agent_data['sentio'] = json.load(f)
        
        # Agora ê²°ê³¼ ë¡œë“œ
        agora_file = os.path.join(employee_dir, 'agora_result.json')
        if os.path.exists(agora_file):
            with open(agora_file, 'r', encoding='utf-8') as f:
                agent_data['agora'] = json.load(f)
        
        # ì§ì› ì •ë³´ ë¡œë“œ
        employee_info_file = os.path.join(employee_dir, 'employee_info.json')
        employee_info = {}
        if os.path.exists(employee_info_file):
            with open(employee_info_file, 'r', encoding='utf-8') as f:
                employee_info = json.load(f)
        
        # ë¶„ì„ ìš”ì•½ CSV ë¡œë“œ
        analysis_summary_file = os.path.join(employee_dir, 'analysis_summary.csv')
        analysis_summary = None
        if os.path.exists(analysis_summary_file):
            import pandas as pd
            try:
                analysis_summary = pd.read_csv(analysis_summary_file).to_dict('records')[0]
                print(f"âœ… ë¶„ì„ ìš”ì•½ CSV ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ì‹œê°í™” íŒŒì¼ ëª©ë¡
        visualizations_dir = os.path.join(employee_dir, 'visualizations')
        visualization_files = []
        if os.path.exists(visualizations_dir):
            visualization_files = [f for f in os.listdir(visualizations_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]
            print(f"âœ… ì‹œê°í™” íŒŒì¼ {len(visualization_files)}ê°œ ë°œê²¬")
        
        # ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ë¡œë“œ (ê°€ì¥ ìµœì‹ )
        batch_files = [f for f in os.listdir(employee_dir) if f.startswith('batch_analysis_') and f.endswith('.json')]
        batch_data = {}
        if batch_files:
            latest_batch_file = sorted(batch_files)[-1]  # ê°€ì¥ ìµœì‹  íŒŒì¼
            with open(os.path.join(employee_dir, latest_batch_file), 'r', encoding='utf-8') as f:
                batch_data = json.load(f)
        
        # comprehensive_reportê°€ ìˆìœ¼ë©´ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³´ê³ ì„œ ìƒì„±, ì—†ìœ¼ë©´ LLM ì‚¬ìš©
        if comprehensive_report and 'rule_based_interpretation' in comprehensive_report:
            # ì €ì¥ëœ ë³´ê³ ì„œ ì‚¬ìš© (ë” ë¹ ë¥´ê³  ì¼ê´€ì„± ìˆìŒ)
            report = generate_report_from_saved_data(
                employee_id=employee_id,
                comprehensive_report=comprehensive_report,
                agent_data=agent_data,
                employee_info=employee_info,
                analysis_summary=analysis_summary,
                visualization_files=visualization_files
            )
        else:
            # LLMìœ¼ë¡œ ìƒˆë¡œ ìƒì„±
            report = generate_llm_report(
                employee_id=employee_id,
                department=department,
                risk_level=risk_level,
                risk_score=risk_score,
                agent_scores=agent_scores,
                agent_data=agent_data,
                employee_info=employee_info,
                batch_data=batch_data
            )
        
        print(f"âœ… ì§ì› {employee_id} ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
        
        return jsonify({
            'success': True,
            'report': report,
            'employee_id': employee_id,
            'department': department,
            'risk_level': risk_level,
            'visualization_files': visualization_files,
            'has_comprehensive_report': bool(comprehensive_report)
        })
        
    except Exception as e:
        print(f"âŒ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500

def generate_report_from_saved_data(employee_id, comprehensive_report, agent_data, employee_info, analysis_summary, visualization_files):
    """ì €ì¥ëœ íŒŒì¼ ë°ì´í„°ë¡œë¶€í„° ë³´ê³ ì„œ ìƒì„± (ReportGenerator ì‚¬ìš©)"""
    try:
        from report_generator import ReportGenerator
        
        # ReportGenerator ì´ˆê¸°í™”
        report_gen = ReportGenerator()
        
        # ë³´ê³ ì„œ ìƒì„± (ReportGeneratorì— ìœ„ì„)
        report = report_gen.generate_comprehensive_report(
            employee_id=employee_id,
            comprehensive_report=comprehensive_report,
            agent_data=agent_data,
            employee_info=employee_info,
            analysis_summary=analysis_summary,
            visualization_files=visualization_files
        )
        
        return report
    except Exception as e:
        import traceback
        print(f"âŒ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        print(traceback.format_exc())
        return f"ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


# ì•„ë˜ ì½”ë“œëŠ” ì´ì œ report_generator.pyë¡œ ì´ë™ë¨ (ë ˆê±°ì‹œ ì½”ë“œ ì œê±° ì™„ë£Œ)
# - analyze_xai_results()
# - perform_root_cause_analysis()
# - ì—ì´ì „íŠ¸ ì ìˆ˜ ì¶”ì¶œ ë¡œì§


def generate_llm_report(employee_id, department, risk_level, risk_score, agent_scores, agent_data, employee_info, batch_data):
    """LLMì„ ì‚¬ìš©í•œ ê°œë³„ ì§ì› ë³´ê³ ì„œ ìƒì„± (ë ˆê±°ì‹œ - ReportGeneratorë¡œ ëŒ€ì²´ ì˜ˆì •)"""
    # ì´ í•¨ìˆ˜ëŠ” í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ë˜ì§€ë§Œ, ìƒˆë¡œìš´ êµ¬í˜„ì€ ReportGeneratorë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
    try:
        from report_generator import ReportGenerator
        report_gen = ReportGenerator()
        
        # ReportGeneratorë¥¼ í†µí•œ LLM ë³´ê³ ì„œ ìƒì„±
        return report_gen.generate_llm_based_report(
            employee_id=employee_id,
            department=department,
            risk_level=risk_level,
            risk_score=risk_score,
            agent_scores=agent_scores,
            agent_data=agent_data,
            employee_info=employee_info
        )
    except Exception as e:
        import logging
        logging.error(f"LLM ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
        return f"ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def generate_llm_report(employee_id, department, risk_level, risk_score, agent_scores, agent_data, employee_info, batch_data):
    """LLMì„ ì‚¬ìš©í•œ ê°œë³„ ì§ì› ë³´ê³ ì„œ ìƒì„±"""
    try:
        # ìœ„í—˜ë„ ë¶„ë¥˜ í•œê¸€ ë³€í™˜
        risk_level_kr = {
            'high': 'ê³ ìœ„í—˜êµ°',
            'medium': 'ì£¼ì˜êµ°', 
            'low': 'ì•ˆì „êµ°'
        }.get(risk_level, 'ë¯¸ë¶„ë¥˜')
        
        # Structura ì£¼ìš” ìœ„í—˜ ìš”ì¸ ì¶”ì¶œ
        structura_risks = []
        if 'structura' in agent_data and 'explanation' in agent_data['structura']:
            explanation = agent_data['structura']['explanation']
            if 'individual_explanation' in explanation:
                top_risks = explanation['individual_explanation'].get('top_risk_factors', [])
                structura_risks = [factor.get('factor', '') for factor in top_risks[:3]]
        
        # Structura ë³´í˜¸ ìš”ì¸ ì¶”ì¶œ
        structura_protections = []
        if 'structura' in agent_data and 'explanation' in agent_data['structura']:
            explanation = agent_data['structura']['explanation']
            if 'individual_explanation' in explanation:
                top_protections = explanation['individual_explanation'].get('top_protective_factors', [])
                structura_protections = [factor.get('factor', '') for factor in top_protections[:3]]
        
        # ë³´ê³ ì„œ í…œí”Œë¦¿
        report_template = f"""
# ì§ì› ìœ„í—˜ë„ ë¶„ì„ ë³´ê³ ì„œ

## ğŸ“‹ ê¸°ë³¸ ì •ë³´
- **ì§ì› ID**: {employee_id}
- **ì†Œì† ë¶€ì„œ**: {department}
- **ìœ„í—˜ë„ ë¶„ë¥˜**: {risk_level_kr}
- **ì¢…í•© ìœ„í—˜ ì ìˆ˜**: {risk_score:.1%}

## ğŸ“Š ì—ì´ì „íŠ¸ë³„ ë¶„ì„ ê²°ê³¼

### ğŸ§  Structura (HR ë°ì´í„° ë¶„ì„)
- **ì´ì§ í™•ë¥ **: {agent_scores.get('structura', 0):.1%}
- **ì£¼ìš” ìœ„í—˜ ìš”ì¸**: {', '.join(structura_risks) if structura_risks else 'ë°ì´í„° ì—†ìŒ'}
- **ë³´í˜¸ ìš”ì¸**: {', '.join(structura_protections) if structura_protections else 'ë°ì´í„° ì—†ìŒ'}

### â° Chronos (ì‹œê³„ì—´ ë¶„ì„)
- **ì‹œê³„ì—´ ìœ„í—˜ë„**: {agent_scores.get('chronos', 0):.1%}
- **íŠ¸ë Œë“œ ë¶„ì„**: {"ìƒìŠ¹ ì¶”ì„¸" if agent_scores.get('chronos', 0) > 0.5 else "ì•ˆì •ì "}

### ğŸ”— Cognita (ê´€ê³„ ë¶„ì„)
- **ê´€ê³„ ìœ„í—˜ë„**: {agent_scores.get('cognita', 0):.1%}
- **ë„¤íŠ¸ì›Œí¬ ì˜í–¥ë ¥**: {"ë†’ìŒ" if agent_scores.get('cognita', 0) > 0.6 else "ë³´í†µ" if agent_scores.get('cognita', 0) > 0.3 else "ë‚®ìŒ"}

### ğŸ’­ Sentio (ê°ì • ë¶„ì„)
- **ê°ì • ìœ„í—˜ë„**: {agent_scores.get('sentio', 0):.1%}
- **ê°ì • ìƒíƒœ**: {"ë¶€ì •ì " if agent_scores.get('sentio', 0) > 0.5 else "ê¸ì •ì "}

### ğŸŒ Agora (ì‹œì¥ ë¶„ì„)
- **ì‹œì¥ ìœ„í—˜ë„**: {agent_scores.get('agora', 0):.1%}
- **ì™¸ë¶€ í™˜ê²½**: {"ë¶ˆë¦¬í•¨" if agent_scores.get('agora', 0) > 0.5 else "ìœ ë¦¬í•¨"}

## ğŸ¯ ì¢…í•© ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­

### ìœ„í—˜ë„ í‰ê°€
"""

        if risk_level == 'high':
            report_template += """
ì´ ì§ì›ì€ **ê³ ìœ„í—˜êµ°**ìœ¼ë¡œ ë¶„ë¥˜ë˜ì–´ ì¦‰ê°ì ì¸ ê´€ì‹¬ê³¼ ê°œì…ì´ í•„ìš”í•©ë‹ˆë‹¤.

**ì£¼ìš” ìš°ë ¤ì‚¬í•­:**
- ë†’ì€ ì´ì§ í™•ë¥ ë¡œ ì¸í•œ ì¸ì¬ ì†ì‹¤ ìœ„í—˜
- íŒ€ ë‚´ ë¶€ì •ì  ì˜í–¥ ì „íŒŒ ê°€ëŠ¥ì„±
- ì—…ë¬´ ì„±ê³¼ ë° ëª°ì…ë„ ì €í•˜ ìš°ë ¤

**ì¦‰ì‹œ ê¶Œì¥ì‚¬í•­:**
1. **1:1 ë©´ë‹´ ì‹¤ì‹œ**: ê´€ë¦¬ìì™€ì˜ ê°œë³„ ìƒë‹´ì„ í†µí•œ ë¬¸ì œì  íŒŒì•…
2. **ê·¼ë¬´ í™˜ê²½ ê°œì„ **: ì£¼ìš” ìœ„í—˜ ìš”ì¸ì— ëŒ€í•œ êµ¬ì²´ì  ê°œì„  ë°©ì•ˆ ìˆ˜ë¦½
3. **ì¸ì„¼í‹°ë¸Œ ì œê³µ**: ì„±ê³¼ ë³´ìƒ, ìŠ¹ì§„ ê¸°íšŒ, êµìœ¡ ì§€ì› ë“± ê³ ë ¤
4. **ì •ê¸°ì  ëª¨ë‹ˆí„°ë§**: ì›” 1íšŒ ì´ìƒ ìƒíƒœ ì ê²€ ë° í”¼ë“œë°±
"""
        elif risk_level == 'medium':
            report_template += """
ì´ ì§ì›ì€ **ì£¼ì˜êµ°**ìœ¼ë¡œ ë¶„ë¥˜ë˜ì–´ ì˜ˆë°©ì  ê´€ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**
- ì¤‘ê°„ ìˆ˜ì¤€ì˜ ì´ì§ ìœ„í—˜ë„
- ì ì ˆí•œ ê°œì…ì„ í†µí•œ ê°œì„  ê°€ëŠ¥ì„± ë†’ìŒ
- ì¡°ê¸° ëŒ€ì‘ì„ í†µí•œ ìœ„í—˜ë„ ê°ì†Œ ê¸°ëŒ€

**ê¶Œì¥ì‚¬í•­:**
1. **ì •ê¸°ì  í”¼ë“œë°±**: ë¶„ê¸°ë³„ ì„±ê³¼ ë©´ë‹´ ë° ì»¤ë¦¬ì–´ ìƒë‹´
2. **ì—…ë¬´ ë§Œì¡±ë„ í–¥ìƒ**: ì—…ë¬´ ë°°ì¹˜ ì¡°ì •, ì—­í•  ëª…í™•í™”
3. **êµìœ¡ ê¸°íšŒ ì œê³µ**: ì—­ëŸ‰ ê°œë°œ í”„ë¡œê·¸ë¨ ì°¸ì—¬ ì§€ì›
4. **íŒ€ ë‚´ ì†Œí†µ ê°•í™”**: ë™ë£Œ ë° ìƒì‚¬ì™€ì˜ ê´€ê³„ ê°œì„  ì§€ì›
"""
        else:
            report_template += """
ì´ ì§ì›ì€ **ì•ˆì „êµ°**ìœ¼ë¡œ ë¶„ë¥˜ë˜ì–´ í˜„ì¬ ì•ˆì •ì ì¸ ìƒíƒœì…ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**
- ë‚®ì€ ì´ì§ ìœ„í—˜ë„
- ë†’ì€ ì—…ë¬´ ë§Œì¡±ë„ ë° ì¡°ì§ ëª°ì…ë„
- íŒ€ ë‚´ ê¸ì •ì  ì˜í–¥ë ¥ ê¸°ëŒ€

**ìœ ì§€ ê´€ë¦¬ ë°©ì•ˆ:**
1. **í˜„ì¬ ìƒíƒœ ìœ ì§€**: ê¸°ì¡´ì˜ ê¸ì •ì  ìš”ì¸ë“¤ì„ ì§€ì†ì ìœ¼ë¡œ ì§€ì›
2. **ì„±ì¥ ê¸°íšŒ ì œê³µ**: ì¶”ê°€ì ì¸ ë„ì „ê³¼ ë°œì „ ê¸°íšŒ ì œê³µ
3. **ë©˜í†  ì—­í• **: ë‹¤ë¥¸ ì§ì›ë“¤ì˜ ë¡¤ëª¨ë¸ ë° ë©˜í†  ì—­í•  ë¶€ì—¬
4. **ì¥ê¸°ì  ê´€ì **: ê²½ë ¥ ê°œë°œ ê³„íš ìˆ˜ë¦½ ë° ì§€ì›
"""

        report_template += f"""

## ğŸ“ˆ ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸

### XAI (ì„¤ëª… ê°€ëŠ¥í•œ AI) ë¶„ì„
- **ê°€ì¥ ì¤‘ìš”í•œ ì˜ˆì¸¡ ìš”ì¸**: {structura_risks[0] if structura_risks else 'ë°ì´í„° ë¶„ì„ ì¤‘'}
- **ê°œì„  ê°€ëŠ¥í•œ ì˜ì—­**: {structura_protections[0] if structura_protections else 'ì¶”ê°€ ë¶„ì„ í•„ìš”'}

### ì‹œê°í™” ìë£Œ
- ìƒì„¸í•œ XAI ì‹œê°í™”ëŠ” ë‹¤ìŒ ê²½ë¡œì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤:
  `app/results/{department.replace(' ', '_').replace('&', '_&_')}/employee_{employee_id}/visualizations/`

## ğŸ“… í›„ì† ì¡°ì¹˜ ê³„íš

### ë‹¨ê¸° (1ê°œì›” ì´ë‚´)
- [ ] ì§ì† ìƒì‚¬ì™€ì˜ 1:1 ë©´ë‹´ ì‹¤ì‹œ
- [ ] ì£¼ìš” ìœ„í—˜ ìš”ì¸ì— ëŒ€í•œ êµ¬ì²´ì  ê°œì„  ë°©ì•ˆ ë…¼ì˜
- [ ] ì—…ë¬´ í™˜ê²½ ë° ì¡°ê±´ ì ê²€

### ì¤‘ê¸° (3ê°œì›” ì´ë‚´)
- [ ] ê°œì„  ë°©ì•ˆ ì‹¤í–‰ ë° íš¨ê³¼ ì¸¡ì •
- [ ] ì¶”ê°€ì ì¸ ì§€ì› ë°©ì•ˆ ê²€í† 
- [ ] ì •ê¸°ì  ëª¨ë‹ˆí„°ë§ ì²´ê³„ êµ¬ì¶•

### ì¥ê¸° (6ê°œì›” ì´í›„)
- [ ] ìœ„í—˜ë„ ì¬í‰ê°€ ì‹¤ì‹œ
- [ ] ì¥ê¸°ì  ê²½ë ¥ ê°œë°œ ê³„íš ìˆ˜ë¦½
- [ ] ì¡°ì§ ë‚´ ì—­í•  ë° ê¸°ì—¬ë„ ì¬ê²€í† 

---
*ë³¸ ë³´ê³ ì„œëŠ” AI ê¸°ë°˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìœ¼ë©°, ì‹¤ì œ ì¸ì‚¬ ê²°ì • ì‹œì—ëŠ” ì¶”ê°€ì ì¸ ì •ì„±ì  í‰ê°€ê°€ í•„ìš”í•©ë‹ˆë‹¤.*

**ë³´ê³ ì„œ ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}
"""

        return report_template.strip()
        
    except Exception as e:
        print(f"âŒ LLM ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return f"ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


@app.route('/get_employee_list', methods=['GET'])
def get_employee_list():
    """ë¡œë“œëœ ì§ì› ëª©ë¡ ì¡°íšŒ"""
    try:
        if report_generator.employee_data is None:
            return jsonify({
                'success': False,
                'error': 'ì§ì› ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'
            }), 400
        
        # ì§ì› ëª©ë¡ ì¶”ì¶œ
        if 'employee_id' in report_generator.employee_data.columns:
            employee_list = report_generator.employee_data['employee_id'].tolist()
        else:
            # employee_id ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ ì‚¬ìš©
            employee_list = report_generator.employee_data.index.tolist()
        
        return jsonify({
            'success': True,
            'total_employees': len(employee_list),
            'employee_ids': employee_list[:100],  # ì²˜ìŒ 100ëª…ë§Œ ë°˜í™˜
            'has_more': len(employee_list) > 100
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ì§ì› ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500

@app.route('/save_agent_models', methods=['POST'])
def save_agent_models():
    """ì—ì´ì „íŠ¸ ëª¨ë¸ì„ app/results/modelsì— ì €ì¥"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'ìš”ì²­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'
            }), 400
        
        models = data.get('models')
        save_path = data.get('save_path', 'app/results/models/agent_models.json')
        
        if not models:
            return jsonify({
                'success': False,
                'error': 'ì €ì¥í•  ëª¨ë¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'
            }), 400
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        import os
        # app/Integrationì—ì„œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™ (../../)
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        absolute_save_path = os.path.join(project_root, save_path)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(absolute_save_path), exist_ok=True)
        
        # íŒŒì¼ ì €ì¥
        import json
        with open(absolute_save_path, 'w', encoding='utf-8') as f:
            json.dump(models, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ì—ì´ì „íŠ¸ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
        
        return jsonify({
            'success': True,
            'message': 'ì—ì´ì „íŠ¸ ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤',
            'file_path': save_path,
            'agents_saved': len(models.get('saved_models', {})),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        print(f"âŒ ì—ì´ì „íŠ¸ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'ëª¨ë¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
        }), 500

@app.route('/save_optimized_models', methods=['POST'])
def save_optimized_models():
    """ìµœì í™”ëœ ëª¨ë¸ê³¼ ì„ê³„ê°’/ê°€ì¤‘ì¹˜ë¥¼ app/results/modelsì— ì €ì¥"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'ìš”ì²­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'
            }), 400
        
        complete_model = data.get('complete_model')
        save_path = data.get('save_path', 'app/results/models/optimized_models.json')
        
        if not complete_model:
            return jsonify({
                'success': False,
                'error': 'ì €ì¥í•  ëª¨ë¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'
            }), 400
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        import os
        # app/Integrationì—ì„œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™ (../../)
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        absolute_save_path = os.path.join(project_root, save_path)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(absolute_save_path), exist_ok=True)
        
        # ë°°ì¹˜ ë¶„ì„ì—ì„œ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬ì¡°í™”
        optimized_data = {
            'metadata': {
                'created_at': datetime.now().isoformat(),
                'stage': complete_model.get('training_metadata', {}).get('stage', 'optimization_completed'),
                'agents_used': complete_model.get('training_metadata', {}).get('agents_used', []),
                'training_data_size': complete_model.get('training_metadata', {}).get('training_data_size', 0)
            },
            'agent_models': complete_model.get('saved_models', {}),
            'agent_results': complete_model.get('agent_results', {}),
            'optimization_results': complete_model.get('optimization_results', {}),
            'ready_for_batch_analysis': True
        }
        
        # íŒŒì¼ ì €ì¥
        import json
        with open(absolute_save_path, 'w', encoding='utf-8') as f:
            json.dump(optimized_data, f, indent=2, ensure_ascii=False)
        
        # ë°°ì¹˜ ë¶„ì„ìš© ì‹¬ë³¼ë¦­ ë§í¬ ë˜ëŠ” ë³µì‚¬ë³¸ ìƒì„±
        batch_ready_path = 'app/results/models/batch_ready_models.json'
        absolute_batch_path = os.path.join(project_root, batch_ready_path)
        with open(absolute_batch_path, 'w', encoding='utf-8') as f:
            json.dump(optimized_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ìµœì í™”ëœ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
        print(f"âœ… ë°°ì¹˜ ë¶„ì„ìš© ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ: {batch_ready_path}")
        
        return jsonify({
            'success': True,
            'message': 'ìµœì í™”ëœ ëª¨ë¸ê³¼ ì„ê³„ê°’/ê°€ì¤‘ì¹˜ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤',
            'file_path': save_path,
            'batch_ready_path': batch_ready_path,
            'agents_count': len(optimized_data['agent_models']),
            'has_optimization': bool(optimized_data['optimization_results']),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        print(f"âŒ ìµœì í™”ëœ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'ëª¨ë¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
        }), 500


if __name__ == '__main__':
    print("ğŸš€ Integration Flask ì„œë²„ ì‹œì‘")
    print(f"ğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬: {DATA_DIR}")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_DIR}")
    print("ğŸŒ ì„œë²„ ì£¼ì†Œ: http://localhost:5007")
    
    # API í‚¤ ìƒíƒœ í™•ì¸ (Sentio/Agoraì™€ ë™ì¼)
    if report_generator.client:
        print("âœ… OpenAI API ì—°ê²° ì„±ê³µ - LLM ê¸°ë°˜ ë¶„ì„ ê°€ëŠ¥")
    else:
        print("âš ï¸  OpenAI API í‚¤ ì—†ìŒ - ê¸°ë³¸ ë¶„ì„ ëª¨ë“œë¡œ ë™ì‘")
        print("   ğŸ’¡ .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ /set_api_key ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
    print("\nì‚¬ìš© ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸:")
    print("  GET  /health - ì„œë²„ ìƒíƒœ í™•ì¸")
    print("  POST /set_api_key - OpenAI API í‚¤ ì„¤ì •")
    print("  POST /load_data - ë°ì´í„° ë¡œë“œ")
    print("  POST /calculate_thresholds - ì„ê³„ê°’ ê³„ì‚°")
    print("  POST /optimize_weights - ê°€ì¤‘ì¹˜ ìµœì í™”")
    print("  POST /predict_employee - ê°œë³„ ì§ì› ì˜ˆì¸¡")
    print("  GET  /get_results - í˜„ì¬ ê²°ê³¼ ì¡°íšŒ")
    print("  POST /compare_methods - ìµœì í™” ë°©ë²• ë¹„êµ")
    print("  POST /export_results - ê²°ê³¼ ë‚´ë³´ë‚´ê¸°")
    print("  POST /load_employee_data - ì§ì› ê¸°ë³¸ ë°ì´í„° ë¡œë“œ")
    print("  GET  /get_employee_list - ì§ì› ëª©ë¡ ì¡°íšŒ")
    print("  POST /generate_report - ê°œë³„ ì§ì› ë ˆí¬íŠ¸ ìƒì„± (LLM ì§€ì›)")
    print("  POST /generate_batch_reports - ì¼ê´„ ë ˆí¬íŠ¸ ìƒì„±")
    print("  POST /save_agent_models - ì—ì´ì „íŠ¸ ëª¨ë¸ ì €ì¥")
    print("  POST /save_optimized_models - ìµœì í™”ëœ ëª¨ë¸ ì €ì¥")
    
    app.run(host='0.0.0.0', port=5007, debug=True)
