"""
Integration Flask ë°±ì—”ë“œ
ì„ê³„ê°’ ì„¤ì •, ê°€ì¤‘ì¹˜ ìµœì í™” ë° LLM ê¸°ë°˜ ë ˆí¬íŠ¸ ìƒì„± API ì„œë²„
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
from werkzeug.utils import secure_filename
import pandas as pd
import numpy as np
import os
import json
from datetime import datetime
import traceback
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from matplotlib import font_manager
import warnings
warnings.filterwarnings('ignore')
from typing import Dict, List, Any
from dotenv import load_dotenv

from threshold_calculator import ThresholdCalculator, load_and_process_data
from weight_optimizer import WeightOptimizer
from report_generator import ReportGenerator

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (Sentio/Agoraì™€ ë™ì¼)
load_dotenv()

app = Flask(__name__)
CORS(app)

# ì „ì—­ ë³€ìˆ˜
threshold_calculator = ThresholdCalculator()
weight_optimizer = WeightOptimizer()
report_generator = ReportGenerator()  # í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ìœ¼ë¡œ API í‚¤ ë¡œë“œ ì‹œë„
current_data = None
current_results = {}

# ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'data')
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), 'outputs')

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(OUTPUT_DIR, exist_ok=True)


@app.route('/health', methods=['GET'])
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return jsonify({
        'status': 'healthy',
        'service': 'Integration',
        'timestamp': datetime.now().isoformat(),
        'version': '1.0.0',
        'llm_enabled': hasattr(report_generator, 'llm') and report_generator.llm is not None
    })


@app.route('/set_api_key', methods=['POST'])
def set_api_key():
    """OpenAI API í‚¤ ì„¤ì •"""
    try:
        data = request.get_json()
        api_key = data.get('api_key')
        
        if not api_key:
            return jsonify({
                'success': False,
                'error': 'API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        # ìƒˆë¡œìš´ ReportGenerator ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        global report_generator
        report_generator = ReportGenerator(api_key=api_key)
        
        return jsonify({
            'success': True,
            'message': 'API í‚¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.',
            'llm_enabled': hasattr(report_generator, 'llm') and report_generator.llm is not None
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'API í‚¤ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/load_data', methods=['POST'])
def load_data():
    """ë°ì´í„° ë¡œë“œ"""
    global current_data
    
    try:
        data = request.get_json()
        file_path = data.get('file_path', 'Total_score.csv')
        
        # ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        if not os.path.isabs(file_path):
            file_path = os.path.join(DATA_DIR, file_path)
        
        if not os.path.exists(file_path):
            return jsonify({
                'success': False,
                'error': f'íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}'
            }), 404
        
        # ë°ì´í„° ë¡œë“œ
        current_data, score_columns = load_and_process_data(file_path)
        
        # ê¸°ë³¸ í†µê³„ ê³„ì‚°
        stats = {
            'total_rows': len(current_data),
            'total_columns': len(current_data.columns),
            'score_columns': score_columns,
            'attrition_distribution': current_data['attrition'].value_counts().to_dict() if 'attrition' in current_data.columns else {},
            'missing_values': current_data.isnull().sum().to_dict()
        }
        
        return jsonify({
            'success': True,
            'message': 'ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'file_path': file_path,
            'statistics': stats
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/calculate_thresholds', methods=['POST'])
def calculate_thresholds():
    """ì„ê³„ê°’ ê³„ì‚°"""
    global current_data, current_results
    
    try:
        if current_data is None:
            return jsonify({
                'success': False,
                'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. /load_dataë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.'
            }), 400
        
        data = request.get_json()
        score_columns = data.get('score_columns', None)
        
        # Score ì»¬ëŸ¼ ìë™ ê°ì§€
        if score_columns is None:
            score_columns = [col for col in current_data.columns if col.endswith('_score')]
        
        if not score_columns:
            return jsonify({
                'success': False,
                'error': 'Score ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }), 400
        
        # ìµœì í™” ë°©ë²• ì„¤ì • (ê¸°ë³¸ê°’: Bayesian Optimization)
        optimization_method = data.get('method', 'bayesian')
        
        # ì„ê³„ê°’ ê³„ì‚°
        results = threshold_calculator.calculate_thresholds_for_scores(
            current_data, score_columns, method=optimization_method
        )
        
        # ìš”ì•½ í…Œì´ë¸” ìƒì„±
        summary_df = threshold_calculator.get_summary_table()
        thresholds_dict = threshold_calculator.get_thresholds_dict()
        
        # ì˜ˆì¸¡ ì»¬ëŸ¼ ì¶”ê°€
        data_with_predictions = threshold_calculator.apply_thresholds_to_data(current_data, score_columns)
        
        # ê²°ê³¼ ì €ì¥
        current_results['threshold_results'] = results
        current_results['threshold_summary'] = summary_df.to_dict('records')
        current_results['thresholds_dict'] = thresholds_dict
        current_results['data_with_predictions'] = data_with_predictions
        
        # íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ìš”ì•½ ê²°ê³¼ ì €ì¥
        summary_file = os.path.join(OUTPUT_DIR, f'threshold_summary_{timestamp}.csv')
        summary_df.to_csv(summary_file, index=False, encoding='utf-8')
        
        # ì˜ˆì¸¡ ë°ì´í„° ì €ì¥
        predictions_file = os.path.join(OUTPUT_DIR, f'data_with_predictions_{timestamp}.csv')
        data_with_predictions.to_csv(predictions_file, index=False, encoding='utf-8')
        
        return jsonify({
            'success': True,
            'message': 'ì„ê³„ê°’ ê³„ì‚°ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'results': {
                'summary': current_results['threshold_summary'],
                'thresholds': thresholds_dict,
                'best_score': summary_df.loc[summary_df['F1_Score'].idxmax()].to_dict(),
                'total_predictions': len(data_with_predictions)
            },
            'files': {
                'summary': summary_file,
                'predictions': predictions_file
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ì„ê³„ê°’ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/optimize_weights', methods=['POST'])
def optimize_weights():
    """ê°€ì¤‘ì¹˜ ìµœì í™”"""
    global current_data, current_results
    
    try:
        if current_data is None:
            return jsonify({
                'success': False,
                'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. /load_dataë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.'
            }), 400
        
        # ì˜ˆì¸¡ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        data_to_use = current_results.get('data_with_predictions', current_data)
        prediction_cols = [col for col in data_to_use.columns if col.endswith('_prediction')]
        
        if not prediction_cols:
            return jsonify({
                'success': False,
                'error': 'ì˜ˆì¸¡ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. /calculate_thresholdsë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.'
            }), 400
        
        data = request.get_json()
        method = data.get('method', 'bayesian')  # 'grid', 'bayesian', 'scipy'
        
        # ë°©ë²•ë³„ íŒŒë¼ë¯¸í„°
        method_params = {}
        if method == 'grid':
            method_params['n_points_per_dim'] = data.get('n_points_per_dim', 5)
        elif method == 'bayesian':
            method_params['n_calls'] = data.get('n_calls', 50)  # ë² ì´ì§€ì•ˆ ìµœì í™” 50íšŒë¡œ ì„¤ì •
        
        # ê°€ì¤‘ì¹˜ ìµœì í™” ì‹¤í–‰
        optimization_results = weight_optimizer.optimize_weights(
            data_to_use, method=method, **method_params
        )
        
        if not optimization_results.get('best_weights'):
            return jsonify({
                'success': False,
                'error': 'ê°€ì¤‘ì¹˜ ìµœì í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.',
                'details': optimization_results.get('error', 'Unknown error')
            }), 500
        
        # ìµœì  ê°€ì¤‘ì¹˜ ì ìš©
        data_with_weighted = weight_optimizer.apply_optimal_weights(data_to_use)
        data_with_risk = weight_optimizer.classify_risk_level(data_with_weighted)
        
        # ì„±ëŠ¥ ìš”ì•½
        performance_summary = weight_optimizer.get_performance_summary(data_with_risk)
        
        # ê²°ê³¼ ì €ì¥
        current_results['weight_optimization'] = optimization_results
        current_results['final_data'] = data_with_risk
        current_results['performance_summary'] = performance_summary
        
        # íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ìµœì¢… ë°ì´í„° ì €ì¥
        final_data_file = os.path.join(OUTPUT_DIR, f'final_weighted_predictions_{timestamp}.csv')
        data_with_risk.to_csv(final_data_file, index=False, encoding='utf-8')
        
        # ê°€ì¤‘ì¹˜ ì •ë³´ ì €ì¥
        weights_info = pd.DataFrame([
            {'Variable': col, 'Weight': weight} 
            for col, weight in optimization_results['best_weights'].items()
        ])
        weights_info['Method'] = method
        weights_info['F1_Score'] = optimization_results['best_f1']
        weights_info['Threshold'] = optimization_results['best_threshold']
        
        weights_file = os.path.join(OUTPUT_DIR, f'optimal_weights_{timestamp}.csv')
        weights_info.to_csv(weights_file, index=False, encoding='utf-8')
        
        # ìœ„í—˜ë„ ê¸°ì¤€ ì •ë³´ ì €ì¥
        risk_criteria = pd.DataFrame([
            {'Risk_Level': 'ì•ˆì „êµ°', 'Score_Range': '0.0 ~ 0.3', 'Numeric_Code': 1},
            {'Risk_Level': 'ì£¼ì˜êµ°', 'Score_Range': '0.3 ~ 0.7', 'Numeric_Code': 2},
            {'Risk_Level': 'ê³ ìœ„í—˜êµ°', 'Score_Range': '0.7 ~ 1.0', 'Numeric_Code': 3}
        ])
        
        risk_criteria_file = os.path.join(OUTPUT_DIR, f'risk_criteria_{timestamp}.csv')
        risk_criteria.to_csv(risk_criteria_file, index=False, encoding='utf-8')
        
        return jsonify({
            'success': True,
            'message': 'ê°€ì¤‘ì¹˜ ìµœì í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'results': {
                'method': method,
                'optimal_weights': optimization_results['best_weights'],
                'optimal_threshold': optimization_results['best_threshold'],
                'best_f1_score': optimization_results['best_f1'],
                'performance_metrics': performance_summary['performance_metrics'],
                'risk_statistics': performance_summary['risk_statistics'],
                'total_records': len(data_with_risk)
            },
            'files': {
                'final_data': final_data_file,
                'weights_info': weights_file,
                'risk_criteria': risk_criteria_file
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ê°€ì¤‘ì¹˜ ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/predict_employee', methods=['POST'])
def predict_employee():
    """ê°œë³„ ì§ì› ì˜ˆì¸¡"""
    try:
        data = request.get_json()
        employee_scores = data.get('scores', {})
        
        if not employee_scores:
            return jsonify({
                'success': False,
                'error': 'ì§ì› ì ìˆ˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        results = {}
        
        # ì„ê³„ê°’ ê¸°ë°˜ ì˜ˆì¸¡
        if threshold_calculator.optimal_thresholds:
            threshold_predictions = threshold_calculator.predict_attrition(employee_scores)
            results['threshold_predictions'] = threshold_predictions
        
        # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì˜ˆì¸¡
        if weight_optimizer.optimal_weights:
            # ì˜ˆì¸¡ ì»¬ëŸ¼ ìƒì„± (ì„ê³„ê°’ ê¸°ë°˜)
            prediction_data = {}
            for score_name, score_value in employee_scores.items():
                if score_name in threshold_calculator.optimal_thresholds:
                    threshold = threshold_calculator.optimal_thresholds[score_name]
                    prediction_col = f"{score_name}_prediction"
                    prediction_data[prediction_col] = 1 if score_value >= threshold else 0
            
            if prediction_data:
                # ê°€ì¤‘ ì ìˆ˜ ê³„ì‚°
                weighted_score = sum(
                    prediction_data[col] * weight 
                    for col, weight in weight_optimizer.optimal_weights.items()
                    if col in prediction_data
                )
                
                # ìµœì¢… ì˜ˆì¸¡
                final_prediction = 1 if weighted_score >= weight_optimizer.optimal_threshold else 0
                
                # ìœ„í—˜ë„ ë¶„ë¥˜
                if weighted_score < 0.3:
                    risk_level = 'ì•ˆì „êµ°'
                    risk_numeric = 1
                elif weighted_score < 0.7:
                    risk_level = 'ì£¼ì˜êµ°'
                    risk_numeric = 2
                else:
                    risk_level = 'ê³ ìœ„í—˜êµ°'
                    risk_numeric = 3
                
                results['weighted_prediction'] = {
                    'weighted_score': weighted_score,
                    'final_prediction': final_prediction,
                    'prediction_label': 'ìœ„í—˜' if final_prediction == 1 else 'ì•ˆì „',
                    'risk_level': risk_level,
                    'risk_numeric': risk_numeric,
                    'threshold_used': weight_optimizer.optimal_threshold
                }
        
        if not results:
            return jsonify({
                'success': False,
                'error': 'ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ì„ê³„ê°’ ê³„ì‚° ë° ê°€ì¤‘ì¹˜ ìµœì í™”ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”.'
            }), 400
        
        return jsonify({
            'success': True,
            'employee_scores': employee_scores,
            'predictions': results
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/get_results', methods=['GET'])
def get_results():
    """í˜„ì¬ ê²°ê³¼ ì¡°íšŒ"""
    try:
        if not current_results:
            return jsonify({
                'success': False,
                'error': 'ê³„ì‚°ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 404
        
        # ë¯¼ê°í•œ ë°ì´í„° ì œì™¸í•˜ê³  ìš”ì•½ ì •ë³´ë§Œ ë°˜í™˜
        summary = {
            'has_threshold_results': 'threshold_results' in current_results,
            'has_weight_optimization': 'weight_optimization' in current_results,
            'has_final_data': 'final_data' in current_results
        }
        
        if 'threshold_summary' in current_results:
            summary['threshold_summary'] = current_results['threshold_summary']
            summary['thresholds_dict'] = current_results['thresholds_dict']
        
        if 'performance_summary' in current_results:
            summary['performance_summary'] = current_results['performance_summary']
        
        return jsonify({
            'success': True,
            'results': summary
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/compare_methods', methods=['POST'])
def compare_methods():
    """ì—¬ëŸ¬ ìµœì í™” ë°©ë²• ë¹„êµ"""
    global current_data, current_results
    
    try:
        if current_data is None:
            return jsonify({
                'success': False,
                'error': 'ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'
            }), 400
        
        data_to_use = current_results.get('data_with_predictions', current_data)
        prediction_cols = [col for col in data_to_use.columns if col.endswith('_prediction')]
        
        if not prediction_cols:
            return jsonify({
                'success': False,
                'error': 'ì˜ˆì¸¡ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. /calculate_thresholdsë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.'
            }), 400
        
        data = request.get_json()
        methods = data.get('methods', ['grid', 'scipy'])  # bayesianì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ
        
        comparison_results = []
        
        for method in methods:
            try:
                print(f"=== {method} ë°©ë²• í…ŒìŠ¤íŠ¸ ì¤‘ ===")
                
                # ìƒˆë¡œìš´ optimizer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                temp_optimizer = WeightOptimizer()
                
                # ë°©ë²•ë³„ íŒŒë¼ë¯¸í„° ì„¤ì •
                method_params = {}
                if method == 'grid':
                    method_params['n_points_per_dim'] = 3  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¤„ì„
                elif method == 'bayesian':
                    method_params['n_calls'] = 50  # ë² ì´ì§€ì•ˆ ìµœì í™” 50íšŒ
                
                # ìµœì í™” ì‹¤í–‰
                result = temp_optimizer.optimize_weights(
                    data_to_use, method=method, **method_params
                )
                
                if result.get('best_weights'):
                    # ì„±ëŠ¥ í‰ê°€
                    data_temp = temp_optimizer.apply_optimal_weights(data_to_use)
                    performance = temp_optimizer.get_performance_summary(data_temp)
                    
                    comparison_results.append({
                        'method': method,
                        'best_weights': result['best_weights'],
                        'best_f1_score': result['best_f1'],
                        'best_threshold': result['best_threshold'],
                        'performance_metrics': performance['performance_metrics'],
                        'success': True
                    })
                else:
                    comparison_results.append({
                        'method': method,
                        'success': False,
                        'error': result.get('error', 'Unknown error')
                    })
                    
            except Exception as method_error:
                comparison_results.append({
                    'method': method,
                    'success': False,
                    'error': str(method_error)
                })
        
        # ìµœê³  ì„±ëŠ¥ ë°©ë²• ì°¾ê¸°
        successful_results = [r for r in comparison_results if r.get('success')]
        best_method = None
        
        if successful_results:
            best_method = max(successful_results, key=lambda x: x['best_f1_score'])
        
        return jsonify({
            'success': True,
            'message': 'ë°©ë²• ë¹„êµê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'comparison_results': comparison_results,
            'best_method': best_method,
            'total_methods_tested': len(methods),
            'successful_methods': len(successful_results)
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ë°©ë²• ë¹„êµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/export_results', methods=['POST'])
def export_results():
    """ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"""
    try:
        if not current_results:
            return jsonify({
                'success': False,
                'error': 'ë‚´ë³´ë‚¼ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 404
        
        data = request.get_json()
        export_format = data.get('format', 'csv')  # 'csv', 'json'
        include_data = data.get('include_data', True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        exported_files = []
        
        if export_format == 'csv':
            # ì„ê³„ê°’ ìš”ì•½
            if 'threshold_summary' in current_results:
                threshold_df = pd.DataFrame(current_results['threshold_summary'])
                threshold_file = os.path.join(OUTPUT_DIR, f'threshold_export_{timestamp}.csv')
                threshold_df.to_csv(threshold_file, index=False, encoding='utf-8')
                exported_files.append(threshold_file)
            
            # ìµœì¢… ë°ì´í„°
            if include_data and 'final_data' in current_results:
                final_data_file = os.path.join(OUTPUT_DIR, f'final_data_export_{timestamp}.csv')
                current_results['final_data'].to_csv(final_data_file, index=False, encoding='utf-8')
                exported_files.append(final_data_file)
        
        elif export_format == 'json':
            # JSON í˜•íƒœë¡œ ê²°ê³¼ ì €ì¥
            export_data = {
                'timestamp': timestamp,
                'threshold_results': current_results.get('threshold_summary', []),
                'thresholds_dict': current_results.get('thresholds_dict', {}),
                'performance_summary': current_results.get('performance_summary', {})
            }
            
            if include_data and 'final_data' in current_results:
                export_data['final_data'] = current_results['final_data'].to_dict('records')
            
            json_file = os.path.join(OUTPUT_DIR, f'results_export_{timestamp}.json')
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2, default=str)
            exported_files.append(json_file)
        
        return jsonify({
            'success': True,
            'message': 'ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ë‚´ë³´ë‚´ì¡ŒìŠµë‹ˆë‹¤.',
            'exported_files': exported_files,
            'format': export_format,
            'timestamp': timestamp
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/generate_report', methods=['POST'])
def generate_report():
    """ê°œë³„ ì§ì› ë ˆí¬íŠ¸ ìƒì„±"""
    try:
        data = request.get_json()
        employee_id = data.get('employee_id')
        agent_scores = data.get('agent_scores', {})
        format_type = data.get('format', 'json')  # 'json', 'text', 'both'
        save_file = data.get('save_file', False)
        use_llm = data.get('use_llm', True)  # LLM ì‚¬ìš© ì—¬ë¶€
        
        if not employee_id:
            return jsonify({
                'success': False,
                'error': 'ì§ì› IDê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        if not agent_scores:
            return jsonify({
                'success': False,
                'error': 'ì—ì´ì „íŠ¸ ì ìˆ˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        # ì ìˆ˜ ì„¤ì •
        report_generator.set_agent_scores(employee_id, agent_scores)
        
        # ë ˆí¬íŠ¸ ìƒì„±
        if format_type == 'text':
            report_content = report_generator.generate_text_report(employee_id, use_llm)
            result = {
                'success': True,
                'employee_id': employee_id,
                'format': 'text',
                'report': report_content,
                'llm_used': use_llm and hasattr(report_generator, 'llm') and report_generator.llm is not None
            }
        else:
            report_content = report_generator.generate_employee_report(employee_id, use_llm)
            result = {
                'success': True,
                'employee_id': employee_id,
                'format': 'json',
                'report': report_content,
                'llm_used': use_llm and hasattr(report_generator, 'llm') and report_generator.llm is not None
            }
        
        # íŒŒì¼ ì €ì¥ ì˜µì…˜
        if save_file:
            saved_files = report_generator.save_report(
                employee_id, 
                os.path.join(OUTPUT_DIR, 'reports'), 
                format_type
            )
            
            if 'error' in saved_files:
                result['file_save_error'] = saved_files['error']
            else:
                result['saved_files'] = saved_files
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ë ˆí¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/generate_batch_reports', methods=['POST'])
def generate_batch_reports():
    """ì—¬ëŸ¬ ì§ì›ì˜ ë ˆí¬íŠ¸ ì¼ê´„ ìƒì„±"""
    try:
        data = request.get_json()
        employees_data = data.get('employees', [])  # [{'employee_id': 'EMP001', 'agent_scores': {...}}, ...]
        
        if not employees_data:
            return jsonify({
                'success': False,
                'error': 'ì§ì› ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        # ê° ì§ì›ì˜ ì ìˆ˜ ì„¤ì •
        employee_ids = []
        for emp_data in employees_data:
            employee_id = emp_data.get('employee_id')
            agent_scores = emp_data.get('agent_scores', {})
            
            if employee_id and agent_scores:
                report_generator.set_agent_scores(employee_id, agent_scores)
                employee_ids.append(employee_id)
        
        if not employee_ids:
            return jsonify({
                'success': False,
                'error': 'ìœ íš¨í•œ ì§ì› ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 400
        
        # ì¼ê´„ ë ˆí¬íŠ¸ ìƒì„±
        batch_results = report_generator.generate_batch_reports(
            employee_ids, 
            os.path.join(OUTPUT_DIR, 'reports')
        )
        
        return jsonify({
            'success': True,
            'message': 'ì¼ê´„ ë ˆí¬íŠ¸ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'results': batch_results
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ì¼ê´„ ë ˆí¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/load_employee_data', methods=['POST'])
def load_employee_data():
    """ì§ì› ê¸°ë³¸ ë°ì´í„° ë¡œë“œ (ë ˆí¬íŠ¸ ìƒì„±ìš©)"""
    try:
        data = request.get_json()
        file_path = data.get('file_path')
        
        if not file_path:
            return jsonify({
                'success': False,
                'error': 'íŒŒì¼ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        # ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        if not os.path.isabs(file_path):
            file_path = os.path.join(DATA_DIR, file_path)
        
        # ë°ì´í„° ë¡œë“œ
        success = report_generator.load_employee_data(file_path)
        
        if success:
            return jsonify({
                'success': True,
                'message': 'ì§ì› ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.',
                'file_path': file_path,
                'total_employees': len(report_generator.employee_data) if report_generator.employee_data is not None else 0
            })
        else:
            return jsonify({
                'success': False,
                'error': 'ì§ì› ë°ì´í„° ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'
            }), 500
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ì§ì› ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500


@app.route('/upload/employee_data', methods=['POST'])
def upload_employee_data():
    """ì§ì› ë°ì´í„° CSV íŒŒì¼ ì—…ë¡œë“œ"""
    try:
        # íŒŒì¼ í™•ì¸
        if 'file' not in request.files:
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({
                "success": False,
                "error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 400
        
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        if not file.filename.lower().endswith('.csv'):
            return jsonify({
                "success": False,
                "error": "CSV íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            }), 400
        
        # íŒŒì¼ ì €ì¥
        filename = secure_filename(file.filename)
        upload_dir = os.path.join(os.path.dirname(__file__), '..', 'uploads', 'Integration')
        os.makedirs(upload_dir, exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•œ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥ (ê¸°ì¡´ íŒŒì¼ ë³´ì¡´)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = os.path.splitext(filename)[0]
        new_filename = f"{base_name}_{timestamp}.csv"
        file_path = os.path.join(upload_dir, new_filename)
        
        # ìµœì‹  íŒŒì¼ ë§í¬ë„ ìƒì„±
        latest_link = os.path.join(upload_dir, 'latest_employee_data.csv')
        file.save(file_path)
        
        # ìµœì‹  íŒŒì¼ ë§í¬ ìƒì„±
        try:
            if os.path.exists(latest_link):
                os.remove(latest_link)
            import shutil
            shutil.copy2(file_path, latest_link)
        except Exception as e:
            print(f"ìµœì‹  íŒŒì¼ ë§í¬ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # ë°ì´í„° ê²€ì¦ ë° ë¡œë“œ
        try:
            df = pd.read_csv(file_path)
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_columns = ['employee_id']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                return jsonify({
                    "success": False,
                    "error": f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing_columns)}",
                    "required_columns": required_columns,
                    "found_columns": list(df.columns)
                }), 400
            
            # ReportGeneratorì— ë°ì´í„° ë¡œë“œ
            success = report_generator.load_employee_data(file_path)
            
            if success:
                # ë°ì´í„° í†µê³„
                employee_stats = {
                    "total_employees": len(df),
                    "unique_employees": df['employee_id'].nunique(),
                    "columns": len(df.columns),
                    "column_names": list(df.columns)
                }
                
                # ë¶€ì„œë³„ í†µê³„ (ë¶€ì„œ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)
                if 'Department' in df.columns:
                    employee_stats["departments"] = df['Department'].value_counts().to_dict()
                
                return jsonify({
                    "success": True,
                    "message": "ì§ì› ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ê³  ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.",
                    "file_info": {
                        "original_filename": filename,
                        "saved_filename": new_filename,
                        "upload_path": upload_dir,
                        "file_path": file_path,
                        "latest_link": latest_link
                    },
                    "employee_stats": employee_stats,
                    "note": "ì´ì œ ì—ì´ì „íŠ¸ ì ìˆ˜ë¥¼ ì„¤ì •í•˜ê³  ë ˆí¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                })
            else:
                return jsonify({
                    "success": False,
                    "error": "ë°ì´í„° ì—…ë¡œë“œëŠ” ì„±ê³µí–ˆì§€ë§Œ ì‹œìŠ¤í…œ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                }), 500
                
        except Exception as e:
            return jsonify({
                "success": False,
                "error": f"ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"
            }), 400
            
    except Exception as e:
        return jsonify({
            "success": False,
            "error": f"íŒŒì¼ ì—…ë¡œë“œ ì˜¤ë¥˜: {str(e)}"
        }), 500


@app.route('/api/post-analysis/bayesian-optimization', methods=['POST'])
def bayesian_optimization():
    """ì‚¬í›„ ë¶„ì„ìš© ë² ì´ì§€ì•ˆ ìµœì í™” (PostAnalysis.js ì „ìš©)"""
    global current_data, current_results
    
    try:
        data = request.get_json()
        print(f"ğŸ”§ ë² ì´ì§€ì•ˆ ìµœì í™” ìš”ì²­ ë°›ìŒ")
        print(f"ğŸ“Š ìš”ì²­ ë°ì´í„° í‚¤: {list(data.keys()) if data else 'None'}")
        
        agent_results = data.get('agent_results', {})
        optimization_config = data.get('optimization_config', {})
        
        print(f"ğŸ“Š agent_results í‚¤: {list(agent_results.keys()) if agent_results else 'None'}")
        print(f"ğŸ“Š optimization_config: {optimization_config}")
        
        if not agent_results:
            return jsonify({
                'success': False,
                'error': 'ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        # ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ì—ì„œ ì§ì ‘ ë°ì´í„° ìƒì„± (Total_score.csv ë¶ˆí•„ìš”)
        print("âœ… ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ë² ì´ì§€ì•ˆ ìµœì í™” ìˆ˜í–‰")
        
        # ì„ê³„ê°’ì´ ê³„ì‚°ë˜ì§€ ì•Šì€ ê²½ìš° ìë™ ê³„ì‚°
        if 'threshold_results' not in current_results:
            try:
                # ê¸°ë³¸ ì„ê³„ê°’ ì„¤ì •
                current_results['threshold_results'] = {
                    'structura_threshold': 0.5,
                    'cognita_threshold': 0.5,
                    'chronos_threshold': 0.5,
                    'sentio_threshold': 0.5,
                    'agora_threshold': 0.5,
                    'high_risk_threshold': 0.7,
                    'medium_risk_threshold': 0.4
                }
                print("âœ… ê¸°ë³¸ ì„ê³„ê°’ ì„¤ì • ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ ì„ê³„ê°’ ì„¤ì • ì‹¤íŒ¨: {str(e)}")
                return jsonify({
                    'success': False,
                    'error': f'ì„ê³„ê°’ ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}'
                }), 500
        
        # ë² ì´ì§€ì•ˆ ìµœì í™” ì„¤ì •
        n_trials = optimization_config.get('n_trials', 50)  # ë² ì´ì§€ì•ˆ ìµœì í™” 50íšŒë¡œ ì„¤ì •
        optimization_target = optimization_config.get('optimization_target', 'f1_score')
        
        # ì—ì´ì „íŠ¸ ê²°ê³¼ì—ì„œ ì‹¤ì œ ì˜ˆì¸¡ ë°ì´í„° ì¶”ì¶œ
        print("ğŸ”§ ì‹¤ì œ ë² ì´ì§€ì•ˆ ìµœì í™” ì‹œì‘...")
        print(f"ğŸ“Š ì—ì´ì „íŠ¸ ê²°ê³¼ ë¶„ì„: {list(agent_results.keys())}")
        
        # 1. ì—ì´ì „íŠ¸ë³„ ì˜ˆì¸¡ ê²°ê³¼ ì¶”ì¶œ (0~1 ì‚¬ì´ ê°’)
        agent_predictions = {}
        actual_labels = []
        employee_ids = []
        
        for agent_name, result in agent_results.items():
            # ì˜ˆì¸¡ ê²°ê³¼ ì¶”ì¶œ (PostAnalysis.js êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
            predictions = None
            
            # PostAnalysis.jsì—ì„œ ìƒì„±í•˜ëŠ” êµ¬ì¡°: result.raw_result.data.predictions
            if result.get('raw_result', {}).get('data', {}).get('predictions'):
                predictions = result['raw_result']['data']['predictions']
                print(f"   - {agent_name}: raw_result.data.predictionsì—ì„œ ë°œê²¬")
            elif result.get('predictions'):
                predictions = result['predictions']
                print(f"   - {agent_name}: predictionsì—ì„œ ë°œê²¬")
            elif result.get('data', {}).get('predictions'):
                predictions = result['data']['predictions']
                print(f"   - {agent_name}: data.predictionsì—ì„œ ë°œê²¬")
            else:
                print(f"   - {agent_name}: ì˜ˆì¸¡ ë°ì´í„° êµ¬ì¡° í™•ì¸")
                print(f"     result í‚¤: {list(result.keys()) if isinstance(result, dict) else 'not dict'}")
                if result.get('raw_result'):
                    print(f"     raw_result í‚¤: {list(result['raw_result'].keys()) if isinstance(result['raw_result'], dict) else 'not dict'}")
                    if result['raw_result'].get('data'):
                        print(f"     raw_result.data í‚¤: {list(result['raw_result']['data'].keys()) if isinstance(result['raw_result']['data'], dict) else 'not dict'}")
            
            if predictions:
                print(f"   - {agent_name}: {len(predictions)}ê°œ ì˜ˆì¸¡ ê²°ê³¼")
                
                # ì²« ë²ˆì§¸ ì—ì´ì „íŠ¸ì—ì„œ employee_idì™€ actual_attrition ì¶”ì¶œ (ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš©)
                if not employee_ids:
                    employee_ids = [pred['employee_id'] for pred in predictions]
                    # actual_attritionì´ ìˆë‹¤ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë°˜í™˜
                    if predictions and len(predictions) > 0 and 'actual_attrition' in predictions[0]:
                        actual_labels = [pred['actual_attrition'] for pred in predictions]
                        print(f"âœ… ì‹¤ì œ ë¼ë²¨ ì‚¬ìš©: {sum(actual_labels)}/{len(actual_labels)} ì´íƒˆ")
                    else:
                        return jsonify({
                            'success': False,
                            'error': 'ì‹¤ì œ ì´íƒˆ ë¼ë²¨(actual_attrition)ì´ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.'
                        }), 400
                
                # ì—ì´ì „íŠ¸ë³„ ìœ„í—˜ë„ ì ìˆ˜ (0~1)
                agent_predictions[agent_name] = [pred['risk_score'] for pred in predictions]
            else:
                print(f"   âš ï¸ {agent_name}: ì˜ˆì¸¡ ê²°ê³¼ ì—†ìŒ")
        
        if not agent_predictions:
            return jsonify({
                'success': False,
                'error': 'ì—ì´ì „íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 1ë‹¨ê³„ ì—ì´ì „íŠ¸ ë¶„ì„ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”.'
            }), 400
        
        print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(employee_ids)}ëª…, {len(agent_predictions)}ê°œ ì—ì´ì „íŠ¸")
        
        # ì—ì´ì „íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ Total_score.csv í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        optimization_data = []
        for i, emp_id in enumerate(employee_ids):
            row = {'employee_id': emp_id}
            
            # Total_score.csv ì»¬ëŸ¼ëª…ì— ë§ê²Œ ë³€í™˜ (ëŒ€ë¬¸ì ì‹œì‘)
            agent_name_mapping = {
                'structura': 'Structura_score',
                'cognita': 'Cognita_score', 
                'chronos': 'Chronos_score',
                'sentio': 'Sentio_score',
                'agora': 'Agora_score'
            }
            
            # ê° ì—ì´ì „íŠ¸ì˜ ì˜ˆì¸¡ ì ìˆ˜ ì¶”ê°€ (Total_score.csv í˜•ì‹)
            for agent_name, predictions in agent_predictions.items():
                if i < len(predictions):
                    column_name = agent_name_mapping.get(agent_name.lower(), f'{agent_name}_score')
                    row[column_name] = predictions[i]
            
            # ëˆ„ë½ëœ ì—ì´ì „íŠ¸ ì ìˆ˜ëŠ” ê¸°ë³¸ê°’ 0.5ë¡œ ì„¤ì •
            for column_name in agent_name_mapping.values():
                if column_name not in row:
                    row[column_name] = 0.5
            
            # ì‹¤ì œ ë¼ë²¨ì„ Total_score.csv í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (Yes/No)
            if i < len(actual_labels):
                row['attrition'] = 'Yes' if actual_labels[i] == 1 else 'No'
            else:
                row['attrition'] = 'No'  # ê¸°ë³¸ê°’
            
            optimization_data.append(row)
        
        # DataFrameìœ¼ë¡œ ë³€í™˜
        current_data = pd.DataFrame(optimization_data)
        print(f"ğŸ“Š Total_score.csv í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(current_data)}í–‰, {len(current_data.columns)}ì—´")
        print(f"ğŸ“Š ì»¬ëŸ¼: {list(current_data.columns)}")
        print(f"ğŸ“Š ìƒ˜í”Œ ë°ì´í„°:")
        print(current_data.head(3).to_string())
        
        # 2. ë² ì´ì§€ì•ˆ ìµœì í™”ë¥¼ ìœ„í•œ ëª©ì  í•¨ìˆ˜ ì •ì˜
        from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score
        
        def objective_function(weights):
            """ê°€ì¤‘ì¹˜ ì¡°í•©ì˜ F1-Score ê³„ì‚°"""
            # ê°€ì¤‘ì¹˜ ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡)
            weights = np.array(weights)
            weights = weights / weights.sum()
            
            # ì•™ìƒë¸” ì˜ˆì¸¡ ê³„ì‚°
            ensemble_scores = np.zeros(len(employee_ids))
            agent_names = list(agent_predictions.keys())
            
            for i, agent_name in enumerate(agent_names):
                if i < len(weights):  # ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì—ì´ì „íŠ¸ë§Œ
                    ensemble_scores += np.array(agent_predictions[agent_name]) * weights[i]
            
            # ìµœì  ì„ê³„ê°’ ì°¾ê¸° (ROC ê³¡ì„  ê¸°ë°˜)
            from sklearn.metrics import roc_curve
            fpr, tpr, thresholds = roc_curve(actual_labels, ensemble_scores)
            
            # Youden's J statisticìœ¼ë¡œ ìµœì  ì„ê³„ê°’ ì°¾ê¸°
            j_scores = tpr - fpr
            best_threshold_idx = np.argmax(j_scores)
            optimal_threshold = thresholds[best_threshold_idx]
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            predictions = (ensemble_scores >= optimal_threshold).astype(int)
            
            # F1-Score ê³„ì‚° (ìµœì í™” ëª©í‘œ)
            f1 = f1_score(actual_labels, predictions)
            
            return -f1  # ìµœì†Œí™” ë¬¸ì œë¡œ ë³€í™˜ (ìŒìˆ˜)
        
        # 3. ë² ì´ì§€ì•ˆ ìµœì í™” ì‹¤í–‰
        try:
            from skopt import gp_minimize
            from skopt.space import Real
            
            # ê°€ì¤‘ì¹˜ ê³µê°„ ì •ì˜ (ê°ê° 0.1~0.9, í•©ì´ 1ì´ ë˜ë„ë¡ ì œì•½)
            n_agents = len(agent_predictions)
            dimensions = [Real(0.1, 0.9, name=f'weight_{i}') for i in range(n_agents)]
            
            print(f"ğŸ¯ ë² ì´ì§€ì•ˆ ìµœì í™” ì‹¤í–‰: {n_trials}íšŒ ì‹œë„, {n_agents}ê°œ ì—ì´ì „íŠ¸")
            
            # ë² ì´ì§€ì•ˆ ìµœì í™” ì‹¤í–‰
            result = gp_minimize(
                func=objective_function,
                dimensions=dimensions,
                n_calls=n_trials,
                n_initial_points=10,
                random_state=42,
                acq_func='EI'  # Expected Improvement
            )
            
            # ìµœì  ê°€ì¤‘ì¹˜ ì¶”ì¶œ ë° ì •ê·œí™”
            optimal_weights_raw = result.x
            optimal_weights_raw = np.array(optimal_weights_raw)
            optimal_weights_normalized = optimal_weights_raw / optimal_weights_raw.sum()
            
            # ì—ì´ì „íŠ¸ë³„ ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            agent_names = list(agent_predictions.keys())
            optimal_weights = {}
            for i, agent_name in enumerate(agent_names):
                optimal_weights[f'{agent_name}_weight'] = float(optimal_weights_normalized[i])
            
            print(f"âœ… ìµœì  ê°€ì¤‘ì¹˜: {optimal_weights}")
            
        except ImportError:
            print("âš ï¸ scikit-optimizeê°€ ì—†ì–´ ê°œì„ ëœ ëœë¤ ì„œì¹˜ë¡œ ëŒ€ì²´")
            # scikit-optimizeê°€ ì—†ëŠ” ê²½ìš° ê°œì„ ëœ ëœë¤ ì„œì¹˜
            best_f1 = -1
            optimal_weights = {}
            agent_names = list(agent_predictions.keys())
            n_agents = len(agent_names)
            
            # ëª¨ë“  ì‹œë„ ê¸°ë¡
            all_trials = []
            
            print(f"ğŸ” ê°œì„ ëœ ëœë¤ ì„œì¹˜ ì‹¤í–‰: {min(n_trials, 100)}íšŒ ì‹œë„")
            
            for trial in range(min(n_trials, 100)):  # ìµœëŒ€ 100íšŒ
                # ì œì•½ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê°€ì¤‘ì¹˜ ìƒì„±
                attempts = 0
                while attempts < 10:  # ìµœëŒ€ 10ë²ˆ ì‹œë„
                    # ë””ë¦¬í´ë ˆ ë¶„í¬ë¡œ í•©ì´ 1ì¸ ê°€ì¤‘ì¹˜ ìƒì„±
                    weights = np.random.dirichlet(np.ones(n_agents) * 2)  # alpha=2ë¡œ ë” ê· ë“±í•˜ê²Œ
                    
                    # ê²½ê³„ ì¡°ê±´ í™•ì¸ (0.1 ~ 0.9)
                    if np.all(weights >= 0.1) and np.all(weights <= 0.9):
                        break
                    
                    # ê²½ê³„ ì¡°ê±´ ìœ„ë°˜ ì‹œ í´ë¦¬í•‘ í›„ ì¬ì •ê·œí™”
                    weights = np.clip(weights, 0.1, 0.9)
                    weights = weights / weights.sum()
                    attempts += 1
                
                # F1-Score ê³„ì‚°
                f1 = -objective_function(weights)
                all_trials.append(f1)
                
                if f1 > best_f1:
                    best_f1 = f1
                    optimal_weights = {f'{agent_names[i]}_weight': float(weights[i]) for i in range(n_agents)}
                
                # ì§„í–‰ë¥  ì¶œë ¥
                if (trial + 1) % 20 == 0:
                    print(f"   ì§„í–‰ë¥ : {trial + 1}/{min(n_trials, 100)}, í˜„ì¬ ìµœê³  F1: {best_f1:.4f}")
            
            print(f"âœ… ëœë¤ ì„œì¹˜ ì™„ë£Œ: ìµœê³  F1-Score {best_f1:.4f}")
            
            # ê²°ê³¼ ê°ì²´ ìƒì„±
            result = type('Result', (), {
                'fun': -best_f1, 
                'func_vals': [-f1 for f1 in all_trials],
                'x': [optimal_weights[f'{agent_names[i]}_weight'] for i in range(n_agents)]
            })()
        
        # 4. ìµœì  ê°€ì¤‘ì¹˜ë¡œ ì•™ìƒë¸” ì˜ˆì¸¡ ë° ì„ê³„ê°’ ê³„ì‚°
        ensemble_scores = np.zeros(len(employee_ids))
        for agent_name, predictions in agent_predictions.items():
            weight_key = f'{agent_name}_weight'
            if weight_key in optimal_weights:
                ensemble_scores += np.array(predictions) * optimal_weights[weight_key]
        
        # ROC ê³¡ì„ ìœ¼ë¡œ ìµœì  ì„ê³„ê°’ ê³„ì‚°
        from sklearn.metrics import roc_curve
        fpr, tpr, thresholds = roc_curve(actual_labels, ensemble_scores)
        j_scores = tpr - fpr
        best_threshold_idx = np.argmax(j_scores)
        optimal_ensemble_threshold = float(thresholds[best_threshold_idx])
        
        # ì—ì´ì „íŠ¸ë³„ ê°œë³„ ì„ê³„ê°’ë„ ê³„ì‚°
        optimal_thresholds = {}
        for agent_name, predictions in agent_predictions.items():
            fpr_agent, tpr_agent, thresholds_agent = roc_curve(actual_labels, predictions)
            j_scores_agent = tpr_agent - fpr_agent
            best_idx_agent = np.argmax(j_scores_agent)
            optimal_thresholds[f'{agent_name}_threshold'] = float(thresholds_agent[best_idx_agent])
        
        # ìœ„í—˜ë„ ë¶„ë¥˜ ì„ê³„ê°’
        optimal_thresholds['high_risk_threshold'] = optimal_ensemble_threshold + 0.1
        optimal_thresholds['medium_risk_threshold'] = optimal_ensemble_threshold - 0.1
        
        # 5. ìµœì¢… ì„±ëŠ¥ ê³„ì‚°
        final_predictions = (ensemble_scores >= optimal_ensemble_threshold).astype(int)
        best_performance = {
            'f1_score': float(f1_score(actual_labels, final_predictions)),
            'precision': float(precision_score(actual_labels, final_predictions)),
            'recall': float(recall_score(actual_labels, final_predictions)),
            'accuracy': float(accuracy_score(actual_labels, final_predictions)),
            'auc': float(roc_auc_score(actual_labels, ensemble_scores))
        }
        
        # 6. ìµœì í™” íˆìŠ¤í† ë¦¬ ìƒì„±
        optimization_history = []
        if hasattr(result, 'func_vals'):
            for i, score in enumerate(result.func_vals[:20]):  # ìµœëŒ€ 20ê°œ
                optimization_history.append({
                    'trial': i + 1,
                    'score': float(-score),  # ë‹¤ì‹œ ì–‘ìˆ˜ë¡œ ë³€í™˜
                    'f1_score': float(-score)
                })
        
        optimization_history.sort(key=lambda x: x['score'], reverse=True)
        
        print(f"âœ… ë² ì´ì§€ì•ˆ ìµœì í™” ì™„ë£Œ!")
        print(f"   ìµœì  F1-Score: {best_performance['f1_score']:.4f}")
        print(f"   ìµœì  ì„ê³„ê°’: {optimal_ensemble_threshold:.4f}")
        print(f"   ê°€ì¤‘ì¹˜ í•©: {sum(optimal_weights.values()):.4f}")
        
        # ê²°ê³¼ ì €ì¥
        current_results['bayesian_optimization'] = {
            'optimal_weights': optimal_weights,
            'optimal_thresholds': optimal_thresholds,
            'best_performance': best_performance,
            'optimization_history': optimization_history
        }
        
        # ìµœì í™”ëœ ê²°ê³¼ë¥¼ Total_score.csv í˜•ì‹ìœ¼ë¡œ ì €ì¥
        try:
            # ìµœì  ê°€ì¤‘ì¹˜ë¡œ ì•™ìƒë¸” ì ìˆ˜ ê³„ì‚°
            final_ensemble_scores = np.zeros(len(employee_ids))
            agent_names = list(agent_predictions.keys())
            
            for i, agent_name in enumerate(agent_names):
                weight_key = f'{agent_name}_weight'
                if weight_key in optimal_weights:
                    final_ensemble_scores += np.array(agent_predictions[agent_name]) * optimal_weights[weight_key]
            
            # ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼
            final_predictions = (final_ensemble_scores >= optimal_ensemble_threshold).astype(int)
            
            # Total_score.csv í˜•ì‹ìœ¼ë¡œ ìµœì¢… ê²°ê³¼ DataFrame ìƒì„±
            final_results = []
            for i, emp_id in enumerate(employee_ids):
                row = {'employee_id': emp_id}
                
                # ê° ì—ì´ì „íŠ¸ ì ìˆ˜ (Total_score.csv ì»¬ëŸ¼ëª…)
                agent_name_mapping = {
                    'structura': 'Structura_score',
                    'cognita': 'Cognita_score', 
                    'chronos': 'Chronos_score',
                    'sentio': 'Sentio_score',
                    'agora': 'Agora_score'
                }
                
                for agent_name, predictions in agent_predictions.items():
                    if i < len(predictions):
                        column_name = agent_name_mapping.get(agent_name.lower(), f'{agent_name}_score')
                        row[column_name] = predictions[i]
                
                # ì•™ìƒë¸” ì ìˆ˜ ë° ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
                row['ensemble_score'] = final_ensemble_scores[i]
                row['ensemble_prediction'] = final_predictions[i]
                row['attrition'] = 'Yes' if actual_labels[i] == 1 else 'No'
                
                final_results.append(row)
            
            # DataFrameìœ¼ë¡œ ë³€í™˜
            final_df = pd.DataFrame(final_results)
            
            # íŒŒì¼ ì €ì¥
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = os.path.join('app/results', f'optimized_total_score_{timestamp}.csv')
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            final_df.to_csv(output_file, index=False, encoding='utf-8')
            
            print(f"âœ… ìµœì í™” ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
            print(f"   í˜•ì‹: Total_score.csv í˜¸í™˜")
            print(f"   í–‰ ìˆ˜: {len(final_df)}")
            print(f"   ì»¬ëŸ¼: {list(final_df.columns)}")
            
            # current_dataë¥¼ ìµœì¢… ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
            current_data = final_df
            
        except Exception as e:
            print(f"âš ï¸ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {str(e)}")
        
        # ìœ„í—˜ë„ ë¶„ë¥˜ í†µê³„ (ì‹¤ì œ ì ìˆ˜ ê¸°ë°˜)
        total_employees = len(current_data)
        
        # ì•™ìƒë¸” ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ìœ„í—˜ë„ ë¶„ë¥˜
        if 'ensemble_score' in current_data.columns:
            ensemble_scores = current_data['ensemble_score']
            high_risk_count = len(ensemble_scores[ensemble_scores >= 0.7])
            medium_risk_count = len(ensemble_scores[(ensemble_scores >= 0.3) & (ensemble_scores < 0.7)])
            low_risk_count = len(ensemble_scores[ensemble_scores < 0.3])
        else:
            # ì•™ìƒë¸” ì ìˆ˜ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¶„í¬ ì‚¬ìš©
            high_risk_count = int(total_employees * 0.15)
            medium_risk_count = int(total_employees * 0.25)
            low_risk_count = total_employees - high_risk_count - medium_risk_count
        
        risk_distribution = {
            'ì•ˆì „êµ°': low_risk_count,
            'ì£¼ì˜êµ°': medium_risk_count,
            'ê³ ìœ„í—˜êµ°': high_risk_count
        }
        
        # performance_summary ìƒì„± (ì„±ëŠ¥ ë¶„ì„ íƒ­ í™œì„±í™”ë¥¼ ìœ„í•´ í•„ìš”)
        performance_summary = {
            'performance_metrics': {
                'f1_score': best_performance['f1_score'],
                'precision': best_performance['precision'],
                'recall': best_performance['recall'],
                'accuracy': best_performance.get('accuracy', 0.85)
            },
            'risk_statistics': risk_distribution,
            'optimization_summary': {
                'total_trials': len(optimization_history),
                'best_trial': max(optimization_history, key=lambda x: x['f1_score']) if optimization_history else None,
                'convergence_achieved': True
            }
        }
        
        # current_resultsì— ì €ì¥
        current_results['performance_summary'] = performance_summary
        
        return jsonify({
            'success': True,
            'message': 'ë² ì´ì§€ì•ˆ ìµœì í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'optimal_thresholds': optimal_thresholds,
            'optimal_weights': optimal_weights,
            'best_performance': best_performance,
            'optimization_history': optimization_history,
            'performance_summary': performance_summary,
            'cv_results': {
                'mean_f1_score': best_performance['f1_score'],
                'std_f1_score': 0.02,
                'mean_precision': best_performance['precision'],
                'std_precision': 0.03,
                'mean_recall': best_performance['recall'],
                'std_recall': 0.025
            },
            'n_trials': len(optimization_history),
            'risk_distribution': risk_distribution,
            'total_employees': total_employees
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ë² ì´ì§€ì•ˆ ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500

@app.route('/api/post-analysis/update-risk-thresholds', methods=['POST'])
def update_risk_thresholds():
    """ìœ„í—˜ë„ ë¶„ë¥˜ ì„ê³„ê°’ ë° í‡´ì‚¬ ì˜ˆì¸¡ ê¸°ì¤€ ì—…ë°ì´íŠ¸"""
    global current_data, current_results
    
    try:
        data = request.get_json()
        new_thresholds = data.get('risk_thresholds', {})
        attrition_prediction_mode = data.get('attrition_prediction_mode', 'high_risk_only')  # 'high_risk_only' ë˜ëŠ” 'medium_high_risk'
        
        high_risk_threshold = new_thresholds.get('high_risk_threshold', 0.7)
        low_risk_threshold = new_thresholds.get('low_risk_threshold', 0.3)
        
        print(f"ğŸ¯ ìœ„í—˜ë„ ì„ê³„ê°’ ì—…ë°ì´íŠ¸: ì•ˆì „êµ° < {low_risk_threshold}, ì£¼ì˜êµ° {low_risk_threshold}-{high_risk_threshold}, ê³ ìœ„í—˜êµ° >= {high_risk_threshold}")
        print(f"ğŸ¯ í‡´ì‚¬ ì˜ˆì¸¡ ëª¨ë“œ: {attrition_prediction_mode}")
        
        if current_data is None or current_data.empty:
            return jsonify({
                'success': False,
                'error': 'ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Bayesian Optimizationì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.'
            }), 400
        
        # ìƒˆë¡œìš´ ì„ê³„ê°’ìœ¼ë¡œ ìœ„í—˜ë„ ì¬ë¶„ë¥˜
        total_employees = len(current_data)
        
        if 'ensemble_score' in current_data.columns:
            ensemble_scores = current_data['ensemble_score']
            high_risk_count = len(ensemble_scores[ensemble_scores >= high_risk_threshold])
            medium_risk_count = len(ensemble_scores[(ensemble_scores >= low_risk_threshold) & (ensemble_scores < high_risk_threshold)])
            low_risk_count = len(ensemble_scores[ensemble_scores < low_risk_threshold])
            
            # ìœ„í—˜ë„ ë ˆë²¨ ì»¬ëŸ¼ ì—…ë°ì´íŠ¸
            current_data['risk_level'] = current_data['ensemble_score'].apply(
                lambda x: 'ê³ ìœ„í—˜êµ°' if x >= high_risk_threshold 
                         else 'ì£¼ì˜êµ°' if x >= low_risk_threshold 
                         else 'ì•ˆì „êµ°'
            )
            
            # í‡´ì‚¬ ì˜ˆì¸¡ ì»¬ëŸ¼ ì—…ë°ì´íŠ¸ (ìƒˆë¡œìš´ ê¸°ì¤€ì— ë”°ë¼)
            if attrition_prediction_mode == 'high_risk_only':
                # ê³ ìœ„í—˜êµ°ë§Œ í‡´ì‚¬ ì˜ˆì¸¡
                current_data['predicted_attrition'] = (current_data['ensemble_score'] >= high_risk_threshold).astype(int)
            else:  # 'medium_high_risk'
                # ì£¼ì˜êµ° + ê³ ìœ„í—˜êµ° í‡´ì‚¬ ì˜ˆì¸¡
                current_data['predicted_attrition'] = (current_data['ensemble_score'] >= low_risk_threshold).astype(int)
                
        else:
            return jsonify({
                'success': False,
                'error': 'ì•™ìƒë¸” ì ìˆ˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 400
        
        # ìƒˆë¡œìš´ ìœ„í—˜ë„ ë¶„í¬
        new_risk_distribution = {
            'ì•ˆì „êµ°': low_risk_count,
            'ì£¼ì˜êµ°': medium_risk_count,
            'ê³ ìœ„í—˜êµ°': high_risk_count
        }
        
        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° (ì‹¤ì œ í‡´ì‚¬ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
        performance_metrics = {}
        confusion_matrix = {}
        
        if 'actual_attrition' in current_data.columns:
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix as cm
            
            y_true = current_data['actual_attrition'].astype(int)
            y_pred = current_data['predicted_attrition'].astype(int)
            
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            accuracy = accuracy_score(y_true, y_pred)
            precision = precision_score(y_true, y_pred, zero_division=0)
            recall = recall_score(y_true, y_pred, zero_division=0)
            f1 = f1_score(y_true, y_pred, zero_division=0)
            
            performance_metrics = {
                'accuracy': float(accuracy),
                'precision': float(precision),
                'recall': float(recall),
                'f1_score': float(f1)
            }
            
            # Confusion Matrix ê³„ì‚°
            cm_matrix = cm(y_true, y_pred)
            confusion_matrix = {
                'true_negative': int(cm_matrix[0, 0]),   # ì‹¤ì œ ì”ë¥˜, ì˜ˆì¸¡ ì”ë¥˜
                'false_positive': int(cm_matrix[0, 1]),  # ì‹¤ì œ ì”ë¥˜, ì˜ˆì¸¡ í‡´ì‚¬
                'false_negative': int(cm_matrix[1, 0]),  # ì‹¤ì œ í‡´ì‚¬, ì˜ˆì¸¡ ì”ë¥˜
                'true_positive': int(cm_matrix[1, 1])    # ì‹¤ì œ í‡´ì‚¬, ì˜ˆì¸¡ í‡´ì‚¬
            }
            
            print(f"ğŸ“Š ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì™„ë£Œ:")
            print(f"   ì •í™•ë„: {accuracy:.4f}")
            print(f"   ì •ë°€ë„: {precision:.4f}")
            print(f"   ì¬í˜„ìœ¨: {recall:.4f}")
            print(f"   F1-Score: {f1:.4f}")
            print(f"ğŸ“Š Confusion Matrix: TN={confusion_matrix['true_negative']}, FP={confusion_matrix['false_positive']}, FN={confusion_matrix['false_negative']}, TP={confusion_matrix['true_positive']}")
        else:
            print("âš ï¸ ì‹¤ì œ í‡´ì‚¬ ë°ì´í„°(actual_attrition)ê°€ ì—†ì–´ ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # performance_summary ì—…ë°ì´íŠ¸
        if 'performance_summary' in current_results:
            current_results['performance_summary']['risk_statistics'] = new_risk_distribution
            current_results['performance_summary']['risk_thresholds'] = {
                'high_risk_threshold': high_risk_threshold,
                'low_risk_threshold': low_risk_threshold
            }
            current_results['performance_summary']['attrition_prediction_mode'] = attrition_prediction_mode
            
            # ì„±ëŠ¥ ì§€í‘œê°€ ê³„ì‚°ëœ ê²½ìš° ì—…ë°ì´íŠ¸
            if performance_metrics:
                current_results['performance_summary']['performance_metrics'] = performance_metrics
                current_results['performance_summary']['confusion_matrix'] = confusion_matrix
        
        print(f"âœ… ìœ„í—˜ë„ ì¬ë¶„ë¥˜ ì™„ë£Œ: ì•ˆì „êµ° {low_risk_count}ëª…, ì£¼ì˜êµ° {medium_risk_count}ëª…, ê³ ìœ„í—˜êµ° {high_risk_count}ëª…")
        
        return jsonify({
            'success': True,
            'message': 'ìœ„í—˜ë„ ì„ê³„ê°’ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.',
            'risk_distribution': new_risk_distribution,
            'risk_thresholds': {
                'high_risk_threshold': high_risk_threshold,
                'low_risk_threshold': low_risk_threshold
            },
            'attrition_prediction_mode': attrition_prediction_mode,
            'performance_metrics': performance_metrics,
            'confusion_matrix': confusion_matrix,
            'total_employees': total_employees,
            'performance_summary': current_results.get('performance_summary', {})
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ìœ„í—˜ë„ ì„ê³„ê°’ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500

@app.route('/api/post-analysis/save-final-settings', methods=['POST'])
def save_final_settings():
    """ìµœì¢… ìœ„í—˜ë„ ë¶„ë¥˜ ì„¤ì •ì„ ë°°ì¹˜ ë¶„ì„ìš©ìœ¼ë¡œ ì €ì¥"""
    global current_results
    
    try:
        data = request.get_json()
        
        # ì‚¬ìš©ìê°€ ìµœì¢… ê²°ì •í•œ ì„¤ì • + ìµœì í™”ëœ ê°€ì¤‘ì¹˜
        final_settings = {
            'risk_thresholds': data.get('risk_thresholds', {}),
            'attrition_prediction_mode': data.get('attrition_prediction_mode', 'high_risk_only'),
            'performance_metrics': data.get('performance_metrics', {}),
            'confusion_matrix': data.get('confusion_matrix', {}),
            'risk_distribution': data.get('risk_distribution', {}),
            'integration_config': current_results.get('integration_config', {}) if current_results else {},
            'optimized_weights': current_results.get('optimized_weights', {}) if current_results else {},
            'timestamp': datetime.now().isoformat(),
            'total_employees': data.get('total_employees', 0),
            'source': 'post_analysis_optimization'
        }
        
        print(f"ğŸ’¾ ìµœì¢… ìœ„í—˜ë„ ë¶„ë¥˜ ì„¤ì • ì €ì¥:")
        print(f"   ì•ˆì „êµ° ì„ê³„ê°’: < {final_settings['risk_thresholds'].get('low_risk_threshold', 0.3)}")
        print(f"   ê³ ìœ„í—˜êµ° ì„ê³„ê°’: >= {final_settings['risk_thresholds'].get('high_risk_threshold', 0.7)}")
        print(f"   í‡´ì‚¬ ì˜ˆì¸¡ ëª¨ë“œ: {final_settings['attrition_prediction_mode']}")
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        import os
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        
        # ë°°ì¹˜ ë¶„ì„ìš© ì„¤ì • íŒŒì¼ ì €ì¥
        settings_file = os.path.join(project_root, 'app/results/models/final_risk_settings.json')
        os.makedirs(os.path.dirname(settings_file), exist_ok=True)
        
        import json
        with open(settings_file, 'w', encoding='utf-8') as f:
            json.dump(final_settings, f, indent=2, ensure_ascii=False)
        
        # current_resultsì—ë„ ì €ì¥
        current_results['final_risk_settings'] = final_settings
        
        print(f"âœ… ìµœì¢… ì„¤ì • ì €ì¥ ì™„ë£Œ: {settings_file}")
        
        return jsonify({
            'success': True,
            'message': 'ìµœì¢… ìœ„í—˜ë„ ë¶„ë¥˜ ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.',
            'settings_file': settings_file,
            'final_settings': final_settings
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ìµœì¢… ì„¤ì • ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500

def create_xai_visualizations(employee_result, employee_dir, employee_id):
    """XAI ì‹œê°í™” ìƒì„± ë° ì €ì¥"""
    try:
        # ì‹œê°í™” ë””ë ‰í† ë¦¬ ìƒì„±
        viz_dir = os.path.join(employee_dir, 'visualizations')
        os.makedirs(viz_dir, exist_ok=True)
        
        # í•œê¸€ í°íŠ¸ ì„¤ì •
        plt.rcParams['font.family'] = ['DejaVu Sans', 'Malgun Gothic', 'AppleGothic']
        plt.rcParams['axes.unicode_minus'] = False
        
        agent_results = employee_result.get('agent_results', {})
        
        # 1. Structura Feature Importance ì‹œê°í™”
        if 'structura' in agent_results:
            structura_data = agent_results['structura']
            feature_importance = structura_data.get('feature_importance', {})
            
            if feature_importance:
                plt.figure(figsize=(12, 8))
                features = list(feature_importance.keys())
                importances = list(feature_importance.values())
                
                # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
                sorted_idx = np.argsort(importances)[::-1]
                features = [features[i] for i in sorted_idx]
                importances = [importances[i] for i in sorted_idx]
                
                plt.barh(range(len(features)), importances, color='skyblue')
                plt.yticks(range(len(features)), features)
                plt.xlabel('Feature Importance')
                plt.title(f'Structura Feature Importance - Employee {employee_id}')
                plt.tight_layout()
                plt.savefig(os.path.join(viz_dir, 'structura_feature_importance.png'), dpi=300, bbox_inches='tight')
                plt.close()
        
        # 2. Structura SHAP Values ì‹œê°í™”
        if 'structura' in agent_results:
            shap_values = agent_results['structura'].get('shap_values', {})
            
            if shap_values:
                plt.figure(figsize=(12, 8))
                features = list(shap_values.keys())
                values = list(shap_values.values())
                
                # SHAP ê°’ì— ë”°ë¼ ìƒ‰ìƒ ê²°ì •
                colors = ['red' if v > 0 else 'blue' for v in values]
                
                plt.barh(range(len(features)), values, color=colors, alpha=0.7)
                plt.yticks(range(len(features)), features)
                plt.xlabel('SHAP Values')
                plt.title(f'Structura SHAP Values - Employee {employee_id}')
                plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)
                plt.tight_layout()
                plt.savefig(os.path.join(viz_dir, 'structura_shap_values.png'), dpi=300, bbox_inches='tight')
                plt.close()
        
        # 3. Chronos Attention Weights ì‹œê°í™”
        if 'chronos' in agent_results:
            chronos_data = agent_results['chronos']
            xai_explanation = chronos_data.get('xai_explanation', {})
            attention_weights = xai_explanation.get('attention_weights', {})
            
            if attention_weights:
                plt.figure(figsize=(12, 6))
                timesteps = list(attention_weights.keys())
                weights = list(attention_weights.values())
                
                plt.plot(range(len(timesteps)), weights, marker='o', linewidth=2, markersize=8)
                plt.xlabel('Time Steps')
                plt.ylabel('Attention Weights')
                plt.title(f'Chronos Attention Weights - Employee {employee_id}')
                plt.xticks(range(len(timesteps)), timesteps, rotation=45)
                plt.grid(True, alpha=0.3)
                plt.tight_layout()
                plt.savefig(os.path.join(viz_dir, 'chronos_attention_weights.png'), dpi=300, bbox_inches='tight')
                plt.close()
        
        # 4. Chronos Sequence Importance ì‹œê°í™”
        if 'chronos' in agent_results:
            sequence_importance = agent_results['chronos'].get('xai_explanation', {}).get('sequence_importance', {})
            
            if sequence_importance:
                plt.figure(figsize=(12, 6))
                timesteps = list(sequence_importance.keys())
                importance = list(sequence_importance.values())
                
                plt.bar(range(len(timesteps)), importance, color='lightcoral', alpha=0.7)
                plt.xlabel('Time Steps')
                plt.ylabel('Sequence Importance')
                plt.title(f'Chronos Sequence Importance - Employee {employee_id}')
                plt.xticks(range(len(timesteps)), timesteps, rotation=45)
                plt.tight_layout()
                plt.savefig(os.path.join(viz_dir, 'chronos_sequence_importance.png'), dpi=300, bbox_inches='tight')
                plt.close()
        
        # 5. ì—ì´ì „íŠ¸ë³„ ìœ„í—˜ë„ ì ìˆ˜ ë¹„êµ
        agent_scores = {}
        if 'structura' in agent_results:
            agent_scores['Structura'] = agent_results['structura'].get('attrition_probability', 0)
        if 'chronos' in agent_results:
            agent_scores['Chronos'] = agent_results['chronos'].get('risk_score', 0)
        if 'cognita' in agent_results:
            agent_scores['Cognita'] = agent_results['cognita'].get('overall_risk_score', 0)
        if 'sentio' in agent_results:
            agent_scores['Sentio'] = agent_results['sentio'].get('risk_score', 0)
        if 'agora' in agent_results:
            agent_scores['Agora'] = agent_results['agora'].get('market_risk_score', 0)
        
        if agent_scores:
            plt.figure(figsize=(10, 6))
            agents = list(agent_scores.keys())
            scores = list(agent_scores.values())
            
            bars = plt.bar(agents, scores, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])
            plt.ylabel('Risk Score')
            plt.title(f'Agent Risk Scores Comparison - Employee {employee_id}')
            plt.ylim(0, 1)
            
            # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
            for bar, score in zip(bars, scores):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                        f'{score:.3f}', ha='center', va='bottom')
            
            plt.tight_layout()
            plt.savefig(os.path.join(viz_dir, 'agent_scores_comparison.png'), dpi=300, bbox_inches='tight')
            plt.close()
        
        # 6. Sentio ê°ì • ë¶„í¬ ì‹œê°í™”
        if 'sentio' in agent_results:
            emotion_dist = agent_results['sentio'].get('emotion_distribution', {})
            
            if emotion_dist:
                plt.figure(figsize=(8, 8))
                emotions = list(emotion_dist.keys())
                values = list(emotion_dist.values())
                
                colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']
                plt.pie(values, labels=emotions, autopct='%1.1f%%', colors=colors[:len(emotions)])
                plt.title(f'Sentio Emotion Distribution - Employee {employee_id}')
                plt.tight_layout()
                plt.savefig(os.path.join(viz_dir, 'sentio_emotion_distribution.png'), dpi=300, bbox_inches='tight')
                plt.close()
        
        print(f"âœ… XAI ì‹œê°í™” ìƒì„± ì™„ë£Œ: {viz_dir}")
        return True
        
    except Exception as e:
        print(f"âŒ XAI ì‹œê°í™” ìƒì„± ì‹¤íŒ¨ (ì§ì› {employee_id}): {str(e)}")
        return False

@app.route('/api/batch-analysis/save-results', methods=['POST'])
def save_batch_analysis_results():
    """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë¶€ì„œë³„ë¡œ ì •ë¦¬í•˜ì—¬ ì €ì¥"""
    try:
        data = request.get_json()
        
        if not data or 'results' not in data:
            return jsonify({
                'success': False,
                'error': 'ë¶„ì„ ê²°ê³¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 400
        
        results = data['results']
        analysis_timestamp = datetime.now().isoformat()
        
        print(f"ğŸ’¾ ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹œì‘: {len(results)}ëª…")
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„± (ê¸°ì¡´ êµ¬ì¡° í™œìš©)
        import os
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        results_base_dir = os.path.join(project_root, 'app/results')
        
        # ë°°ì¹˜ ë¶„ì„ ìš”ì•½ìš© ë””ë ‰í† ë¦¬
        batch_summary_dir = os.path.join(results_base_dir, 'batch_analysis')
        os.makedirs(batch_summary_dir, exist_ok=True)
        
        # ë¶€ì„œë³„ ê²°ê³¼ ì •ë¦¬
        department_results = {}
        individual_results = []
        
        for employee in results:
            # ë¶€ì„œ ì •ë³´ ì¶”ì¶œ (ê¸°ì¡´ êµ¬ì¡°ì— ë§ê²Œ)
            dept = employee.get('department', 'ë¯¸ë¶„ë¥˜')
            
            # ë¶€ì„œëª… ì •ê·œí™” (ê¸°ì¡´ êµ¬ì¡°ì™€ ì¼ì¹˜ì‹œí‚¤ê¸°)
            dept_mapping = {
                'Human Resources': 'Human_Resources',
                'Research & Development': 'Research_&_Development', 
                'Research and Development': 'Research_&_Development',
                'R&D': 'Research_&_Development',
                'Sales': 'Sales',
                'HR': 'Human_Resources'
            }
            
            normalized_dept = dept_mapping.get(dept, dept.replace(' ', '_').replace('&', '_&_'))
            
            if normalized_dept not in department_results:
                department_results[normalized_dept] = {
                    'department': normalized_dept,
                    'original_name': dept,
                    'total_employees': 0,
                    'risk_distribution': {'ì•ˆì „êµ°': 0, 'ì£¼ì˜êµ°': 0, 'ê³ ìœ„í—˜êµ°': 0},
                    'employees': []
                }
            
            # ìœ„í—˜ë„ ë¶„ë¥˜
            risk_score = employee.get('risk_score', 0)
            risk_level = employee.get('risk_level', 'unknown')
            
            # ìœ„í—˜ë„ ë¶„í¬ ì—…ë°ì´íŠ¸
            if risk_level == 'low':
                department_results[normalized_dept]['risk_distribution']['ì•ˆì „êµ°'] += 1
            elif risk_level == 'medium':
                department_results[normalized_dept]['risk_distribution']['ì£¼ì˜êµ°'] += 1
            elif risk_level == 'high':
                department_results[normalized_dept]['risk_distribution']['ê³ ìœ„í—˜êµ°'] += 1
            
            department_results[normalized_dept]['total_employees'] += 1
            
            # ê°œë³„ ì§ì› ê²°ê³¼ ì •ë¦¬
            employee_result = {
                'employee_id': employee.get('employee_number', 'Unknown'),
                'department': dept,
                'risk_score': risk_score,
                'risk_level': risk_level,
                'analysis_timestamp': analysis_timestamp,
                'agent_results': {}
            }
            
            # ê° ì—ì´ì „íŠ¸ ê²°ê³¼ ì¶”ì¶œ
            analysis_result = employee.get('analysis_result', {})
            
            # Structura ê²°ê³¼ (XAI í¬í•¨)
            if 'structura_result' in analysis_result:
                structura = analysis_result['structura_result']
                employee_result['agent_results']['structura'] = {
                    'attrition_probability': structura.get('prediction', {}).get('attrition_probability', 0),
                    'predicted_attrition': structura.get('prediction', {}).get('predicted_attrition', 0),
                    'confidence': structura.get('prediction', {}).get('confidence', 0),
                    'feature_importance': structura.get('prediction', {}).get('feature_importance', {}),
                    'xai_explanation': structura.get('xai_explanation', {}),
                    'shap_values': structura.get('shap_values', {}),
                    'lime_explanation': structura.get('lime_explanation', {})
                }
            
            # Chronos ê²°ê³¼ (XAI í¬í•¨)
            if 'chronos_result' in analysis_result:
                chronos = analysis_result['chronos_result']
                employee_result['agent_results']['chronos'] = {
                    'risk_score': chronos.get('prediction', {}).get('risk_score', 0),
                    'anomaly_score': chronos.get('anomaly_detection', {}).get('anomaly_score', 0),
                    'trend_analysis': chronos.get('trend_analysis', {}),
                    'xai_explanation': chronos.get('xai_explanation', {}),
                    'attention_weights': chronos.get('attention_weights', {}),
                    'sequence_importance': chronos.get('sequence_importance', {})
                }
            
            # Cognita ê²°ê³¼
            if 'cognita_result' in analysis_result:
                cognita = analysis_result['cognita_result']
                employee_result['agent_results']['cognita'] = {
                    'overall_risk_score': cognita.get('risk_analysis', {}).get('overall_risk_score', 0),
                    'network_centrality': cognita.get('network_analysis', {}).get('centrality_score', 0),
                    'relationship_strength': cognita.get('relationship_analysis', {}).get('avg_strength', 0),
                    'influence_score': cognita.get('influence_analysis', {}).get('influence_score', 0)
                }
            
            # Sentio ê²°ê³¼
            if 'sentio_result' in analysis_result:
                sentio = analysis_result['sentio_result']
                employee_result['agent_results']['sentio'] = {
                    'sentiment_score': sentio.get('sentiment_analysis', {}).get('overall_sentiment', 0),
                    'risk_score': sentio.get('sentiment_analysis', {}).get('risk_score', 0),
                    'keyword_analysis': sentio.get('keyword_analysis', {}),
                    'emotion_distribution': sentio.get('emotion_analysis', {})
                }
            
            # Agora ê²°ê³¼
            if 'agora_result' in analysis_result:
                agora = analysis_result['agora_result']
                employee_result['agent_results']['agora'] = {
                    'market_risk_score': agora.get('market_analysis', {}).get('risk_score', 0),
                    'industry_trend': agora.get('industry_analysis', {}),
                    'job_market_analysis': agora.get('job_market', {}),
                    'external_factors': agora.get('external_factors', {})
                }
            
            individual_results.append(employee_result)
            department_results[normalized_dept]['employees'].append(employee_result)
            
            # ê¸°ì¡´ êµ¬ì¡°ì— ë§ê²Œ ê°œë³„ ì§ì› íŒŒì¼ ì €ì¥
            employee_id = employee.get('employee_number', 'Unknown')
            employee_dir = os.path.join(results_base_dir, normalized_dept, f'employee_{employee_id}')
            os.makedirs(employee_dir, exist_ok=True)
            
            # ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥ (ê¸°ì¡´ íŒŒì¼ê³¼ êµ¬ë¶„)
            batch_result_file = os.path.join(employee_dir, f'batch_analysis_{analysis_timestamp.replace(":", "-").replace(".", "-")}.json')
            with open(batch_result_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'batch_analysis',
                    'timestamp': analysis_timestamp,
                    'employee_result': employee_result,
                    'applied_settings': data.get('applied_settings', {}),
                    'xai_included': True,
                    'visualizations_generated': True
                }, f, indent=2, ensure_ascii=False)
            
            # XAI ì‹œê°í™” ìƒì„± ë° ì €ì¥ (PNG íŒŒì¼ë“¤)
            visualization_success = create_xai_visualizations(employee_result, employee_dir, employee_id)
            
            print(f"âœ… ì§ì› {employee_id} ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥: {batch_result_file}")
            if visualization_success:
                print(f"âœ… ì§ì› {employee_id} XAI ì‹œê°í™” ìƒì„± ì™„ë£Œ")
            else:
                print(f"âš ï¸ ì§ì› {employee_id} XAI ì‹œê°í™” ìƒì„± ë¶€ë¶„ ì‹¤íŒ¨")
        
        # ë¶€ì„œë³„ ê²°ê³¼ ì €ì¥ (ë°°ì¹˜ ë¶„ì„ ìš”ì•½ ë””ë ‰í† ë¦¬ì—)
        dept_summary_file = os.path.join(batch_summary_dir, f'department_summary_{analysis_timestamp.replace(":", "-").replace(".", "-")}.json')
        with open(dept_summary_file, 'w', encoding='utf-8') as f:
            json.dump({
                'analysis_timestamp': analysis_timestamp,
                'total_employees': len(results),
                'total_departments': len(department_results),
                'department_results': department_results,
                'applied_settings': data.get('applied_settings', {}),
                'summary_statistics': {
                    'overall_risk_distribution': {
                        'ì•ˆì „êµ°': sum(dept['risk_distribution']['ì•ˆì „êµ°'] for dept in department_results.values()),
                        'ì£¼ì˜êµ°': sum(dept['risk_distribution']['ì£¼ì˜êµ°'] for dept in department_results.values()),
                        'ê³ ìœ„í—˜êµ°': sum(dept['risk_distribution']['ê³ ìœ„í—˜êµ°'] for dept in department_results.values())
                    }
                }
            }, f, indent=2, ensure_ascii=False)
        
        # ê°œë³„ ì§ì› ìƒì„¸ ê²°ê³¼ ì €ì¥ (XAI í¬í•¨) - ë°°ì¹˜ ë¶„ì„ ìš”ì•½ ë””ë ‰í† ë¦¬ì—
        individual_file = os.path.join(batch_summary_dir, f'individual_results_{analysis_timestamp.replace(":", "-").replace(".", "-")}.json')
        with open(individual_file, 'w', encoding='utf-8') as f:
            json.dump({
                'analysis_timestamp': analysis_timestamp,
                'total_employees': len(individual_results),
                'individual_results': individual_results,
                'applied_settings': data.get('applied_settings', {}),
                'xai_included': True,
                'agents_analyzed': ['structura', 'chronos', 'cognita', 'sentio', 'agora']
            }, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"   ë¶€ì„œë³„ ìš”ì•½: {dept_summary_file}")
        print(f"   ê°œë³„ ìƒì„¸: {individual_file}")
        print(f"   ì´ {len(department_results)}ê°œ ë¶€ì„œ, {len(individual_results)}ëª… ì§ì›")
        
        return jsonify({
            'success': True,
            'message': f'ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.',
            'files_saved': {
                'department_summary': dept_summary_file,
                'individual_results': individual_file
            },
            'statistics': {
                'total_employees': len(individual_results),
                'total_departments': len(department_results),
                'department_breakdown': {dept: info['total_employees'] for dept, info in department_results.items()}
            }
        })
        
    except Exception as e:
        print(f"âŒ ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500

@app.route('/api/generate-employee-report', methods=['POST'])
def generate_employee_report():
    """ê°œë³„ ì§ì› ë³´ê³ ì„œ ìƒì„± (LLM ê¸°ë°˜)"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({
                'success': False,
                'error': 'ìš”ì²­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 400
        
        employee_id = data.get('employee_id')
        department = data.get('department', 'ë¯¸ë¶„ë¥˜')
        risk_level = data.get('risk_level', 'unknown')
        risk_score = data.get('risk_score', 0)
        agent_scores = data.get('agent_scores', {})
        
        if not employee_id:
            return jsonify({
                'success': False,
                'error': 'ì§ì› IDê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        print(f"ğŸ“ ì§ì› {employee_id} ë³´ê³ ì„œ ìƒì„± ì‹œì‘")
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        import os
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        
        # ë¶€ì„œëª… ì •ê·œí™”
        dept_mapping = {
            'Human Resources': 'Human_Resources',
            'Research & Development': 'Research_&_Development', 
            'Research and Development': 'Research_&_Development',
            'R&D': 'Research_&_Development',
            'Sales': 'Sales',
            'HR': 'Human_Resources'
        }
        normalized_dept = dept_mapping.get(department, department.replace(' ', '_').replace('&', '_&_'))
        
        # ì§ì› ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        employee_dir = os.path.join(project_root, 'app/results', normalized_dept, f'employee_{employee_id}')
        
        if not os.path.exists(employee_dir):
            return jsonify({
                'success': False,
                'error': f'ì§ì› {employee_id}ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }), 404
        
        # ê° ì—ì´ì „íŠ¸ ê²°ê³¼ íŒŒì¼ ë¡œë“œ
        agent_data = {}
        
        # Structura ê²°ê³¼ ë¡œë“œ
        structura_file = os.path.join(employee_dir, 'structura_result.json')
        if os.path.exists(structura_file):
            with open(structura_file, 'r', encoding='utf-8') as f:
                agent_data['structura'] = json.load(f)
        
        # Chronos ê²°ê³¼ ë¡œë“œ
        chronos_file = os.path.join(employee_dir, 'chronos_result.json')
        if os.path.exists(chronos_file):
            with open(chronos_file, 'r', encoding='utf-8') as f:
                agent_data['chronos'] = json.load(f)
        
        # Cognita ê²°ê³¼ ë¡œë“œ
        cognita_file = os.path.join(employee_dir, 'cognita_result.json')
        if os.path.exists(cognita_file):
            with open(cognita_file, 'r', encoding='utf-8') as f:
                agent_data['cognita'] = json.load(f)
        
        # Sentio ê²°ê³¼ ë¡œë“œ
        sentio_file = os.path.join(employee_dir, 'sentio_result.json')
        if os.path.exists(sentio_file):
            with open(sentio_file, 'r', encoding='utf-8') as f:
                agent_data['sentio'] = json.load(f)
        
        # ì§ì› ì •ë³´ ë¡œë“œ
        employee_info_file = os.path.join(employee_dir, 'employee_info.json')
        employee_info = {}
        if os.path.exists(employee_info_file):
            with open(employee_info_file, 'r', encoding='utf-8') as f:
                employee_info = json.load(f)
        
        # ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ë¡œë“œ (ê°€ì¥ ìµœì‹ )
        batch_files = [f for f in os.listdir(employee_dir) if f.startswith('batch_analysis_') and f.endswith('.json')]
        batch_data = {}
        if batch_files:
            latest_batch_file = sorted(batch_files)[-1]  # ê°€ì¥ ìµœì‹  íŒŒì¼
            with open(os.path.join(employee_dir, latest_batch_file), 'r', encoding='utf-8') as f:
                batch_data = json.load(f)
        
        # LLMìœ¼ë¡œ ë³´ê³ ì„œ ìƒì„±
        report = generate_llm_report(
            employee_id=employee_id,
            department=department,
            risk_level=risk_level,
            risk_score=risk_score,
            agent_scores=agent_scores,
            agent_data=agent_data,
            employee_info=employee_info,
            batch_data=batch_data
        )
        
        print(f"âœ… ì§ì› {employee_id} ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
        
        return jsonify({
            'success': True,
            'report': report,
            'employee_id': employee_id,
            'department': department,
            'risk_level': risk_level
        })
        
    except Exception as e:
        print(f"âŒ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500

def generate_llm_report(employee_id, department, risk_level, risk_score, agent_scores, agent_data, employee_info, batch_data):
    """LLMì„ ì‚¬ìš©í•œ ê°œë³„ ì§ì› ë³´ê³ ì„œ ìƒì„±"""
    try:
        # ìœ„í—˜ë„ ë¶„ë¥˜ í•œê¸€ ë³€í™˜
        risk_level_kr = {
            'high': 'ê³ ìœ„í—˜êµ°',
            'medium': 'ì£¼ì˜êµ°', 
            'low': 'ì•ˆì „êµ°'
        }.get(risk_level, 'ë¯¸ë¶„ë¥˜')
        
        # Structura ì£¼ìš” ìœ„í—˜ ìš”ì¸ ì¶”ì¶œ
        structura_risks = []
        if 'structura' in agent_data and 'explanation' in agent_data['structura']:
            explanation = agent_data['structura']['explanation']
            if 'individual_explanation' in explanation:
                top_risks = explanation['individual_explanation'].get('top_risk_factors', [])
                structura_risks = [factor.get('factor', '') for factor in top_risks[:3]]
        
        # Structura ë³´í˜¸ ìš”ì¸ ì¶”ì¶œ
        structura_protections = []
        if 'structura' in agent_data and 'explanation' in agent_data['structura']:
            explanation = agent_data['structura']['explanation']
            if 'individual_explanation' in explanation:
                top_protections = explanation['individual_explanation'].get('top_protective_factors', [])
                structura_protections = [factor.get('factor', '') for factor in top_protections[:3]]
        
        # ë³´ê³ ì„œ í…œí”Œë¦¿
        report_template = f"""
# ì§ì› ìœ„í—˜ë„ ë¶„ì„ ë³´ê³ ì„œ

## ğŸ“‹ ê¸°ë³¸ ì •ë³´
- **ì§ì› ID**: {employee_id}
- **ì†Œì† ë¶€ì„œ**: {department}
- **ìœ„í—˜ë„ ë¶„ë¥˜**: {risk_level_kr}
- **ì¢…í•© ìœ„í—˜ ì ìˆ˜**: {risk_score:.1%}

## ğŸ“Š ì—ì´ì „íŠ¸ë³„ ë¶„ì„ ê²°ê³¼

### ğŸ§  Structura (HR ë°ì´í„° ë¶„ì„)
- **ì´ì§ í™•ë¥ **: {agent_scores.get('structura', 0):.1%}
- **ì£¼ìš” ìœ„í—˜ ìš”ì¸**: {', '.join(structura_risks) if structura_risks else 'ë°ì´í„° ì—†ìŒ'}
- **ë³´í˜¸ ìš”ì¸**: {', '.join(structura_protections) if structura_protections else 'ë°ì´í„° ì—†ìŒ'}

### â° Chronos (ì‹œê³„ì—´ ë¶„ì„)
- **ì‹œê³„ì—´ ìœ„í—˜ë„**: {agent_scores.get('chronos', 0):.1%}
- **íŠ¸ë Œë“œ ë¶„ì„**: {"ìƒìŠ¹ ì¶”ì„¸" if agent_scores.get('chronos', 0) > 0.5 else "ì•ˆì •ì "}

### ğŸ”— Cognita (ê´€ê³„ ë¶„ì„)
- **ê´€ê³„ ìœ„í—˜ë„**: {agent_scores.get('cognita', 0):.1%}
- **ë„¤íŠ¸ì›Œí¬ ì˜í–¥ë ¥**: {"ë†’ìŒ" if agent_scores.get('cognita', 0) > 0.6 else "ë³´í†µ" if agent_scores.get('cognita', 0) > 0.3 else "ë‚®ìŒ"}

### ğŸ’­ Sentio (ê°ì • ë¶„ì„)
- **ê°ì • ìœ„í—˜ë„**: {agent_scores.get('sentio', 0):.1%}
- **ê°ì • ìƒíƒœ**: {"ë¶€ì •ì " if agent_scores.get('sentio', 0) > 0.5 else "ê¸ì •ì "}

### ğŸŒ Agora (ì‹œì¥ ë¶„ì„)
- **ì‹œì¥ ìœ„í—˜ë„**: {agent_scores.get('agora', 0):.1%}
- **ì™¸ë¶€ í™˜ê²½**: {"ë¶ˆë¦¬í•¨" if agent_scores.get('agora', 0) > 0.5 else "ìœ ë¦¬í•¨"}

## ğŸ¯ ì¢…í•© ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­

### ìœ„í—˜ë„ í‰ê°€
"""

        if risk_level == 'high':
            report_template += """
ì´ ì§ì›ì€ **ê³ ìœ„í—˜êµ°**ìœ¼ë¡œ ë¶„ë¥˜ë˜ì–´ ì¦‰ê°ì ì¸ ê´€ì‹¬ê³¼ ê°œì…ì´ í•„ìš”í•©ë‹ˆë‹¤.

**ì£¼ìš” ìš°ë ¤ì‚¬í•­:**
- ë†’ì€ ì´ì§ í™•ë¥ ë¡œ ì¸í•œ ì¸ì¬ ì†ì‹¤ ìœ„í—˜
- íŒ€ ë‚´ ë¶€ì •ì  ì˜í–¥ ì „íŒŒ ê°€ëŠ¥ì„±
- ì—…ë¬´ ì„±ê³¼ ë° ëª°ì…ë„ ì €í•˜ ìš°ë ¤

**ì¦‰ì‹œ ê¶Œì¥ì‚¬í•­:**
1. **1:1 ë©´ë‹´ ì‹¤ì‹œ**: ê´€ë¦¬ìì™€ì˜ ê°œë³„ ìƒë‹´ì„ í†µí•œ ë¬¸ì œì  íŒŒì•…
2. **ê·¼ë¬´ í™˜ê²½ ê°œì„ **: ì£¼ìš” ìœ„í—˜ ìš”ì¸ì— ëŒ€í•œ êµ¬ì²´ì  ê°œì„  ë°©ì•ˆ ìˆ˜ë¦½
3. **ì¸ì„¼í‹°ë¸Œ ì œê³µ**: ì„±ê³¼ ë³´ìƒ, ìŠ¹ì§„ ê¸°íšŒ, êµìœ¡ ì§€ì› ë“± ê³ ë ¤
4. **ì •ê¸°ì  ëª¨ë‹ˆí„°ë§**: ì›” 1íšŒ ì´ìƒ ìƒíƒœ ì ê²€ ë° í”¼ë“œë°±
"""
        elif risk_level == 'medium':
            report_template += """
ì´ ì§ì›ì€ **ì£¼ì˜êµ°**ìœ¼ë¡œ ë¶„ë¥˜ë˜ì–´ ì˜ˆë°©ì  ê´€ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**
- ì¤‘ê°„ ìˆ˜ì¤€ì˜ ì´ì§ ìœ„í—˜ë„
- ì ì ˆí•œ ê°œì…ì„ í†µí•œ ê°œì„  ê°€ëŠ¥ì„± ë†’ìŒ
- ì¡°ê¸° ëŒ€ì‘ì„ í†µí•œ ìœ„í—˜ë„ ê°ì†Œ ê¸°ëŒ€

**ê¶Œì¥ì‚¬í•­:**
1. **ì •ê¸°ì  í”¼ë“œë°±**: ë¶„ê¸°ë³„ ì„±ê³¼ ë©´ë‹´ ë° ì»¤ë¦¬ì–´ ìƒë‹´
2. **ì—…ë¬´ ë§Œì¡±ë„ í–¥ìƒ**: ì—…ë¬´ ë°°ì¹˜ ì¡°ì •, ì—­í•  ëª…í™•í™”
3. **êµìœ¡ ê¸°íšŒ ì œê³µ**: ì—­ëŸ‰ ê°œë°œ í”„ë¡œê·¸ë¨ ì°¸ì—¬ ì§€ì›
4. **íŒ€ ë‚´ ì†Œí†µ ê°•í™”**: ë™ë£Œ ë° ìƒì‚¬ì™€ì˜ ê´€ê³„ ê°œì„  ì§€ì›
"""
        else:
            report_template += """
ì´ ì§ì›ì€ **ì•ˆì „êµ°**ìœ¼ë¡œ ë¶„ë¥˜ë˜ì–´ í˜„ì¬ ì•ˆì •ì ì¸ ìƒíƒœì…ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**
- ë‚®ì€ ì´ì§ ìœ„í—˜ë„
- ë†’ì€ ì—…ë¬´ ë§Œì¡±ë„ ë° ì¡°ì§ ëª°ì…ë„
- íŒ€ ë‚´ ê¸ì •ì  ì˜í–¥ë ¥ ê¸°ëŒ€

**ìœ ì§€ ê´€ë¦¬ ë°©ì•ˆ:**
1. **í˜„ì¬ ìƒíƒœ ìœ ì§€**: ê¸°ì¡´ì˜ ê¸ì •ì  ìš”ì¸ë“¤ì„ ì§€ì†ì ìœ¼ë¡œ ì§€ì›
2. **ì„±ì¥ ê¸°íšŒ ì œê³µ**: ì¶”ê°€ì ì¸ ë„ì „ê³¼ ë°œì „ ê¸°íšŒ ì œê³µ
3. **ë©˜í†  ì—­í• **: ë‹¤ë¥¸ ì§ì›ë“¤ì˜ ë¡¤ëª¨ë¸ ë° ë©˜í†  ì—­í•  ë¶€ì—¬
4. **ì¥ê¸°ì  ê´€ì **: ê²½ë ¥ ê°œë°œ ê³„íš ìˆ˜ë¦½ ë° ì§€ì›
"""

        report_template += f"""

## ğŸ“ˆ ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸

### XAI (ì„¤ëª… ê°€ëŠ¥í•œ AI) ë¶„ì„
- **ê°€ì¥ ì¤‘ìš”í•œ ì˜ˆì¸¡ ìš”ì¸**: {structura_risks[0] if structura_risks else 'ë°ì´í„° ë¶„ì„ ì¤‘'}
- **ê°œì„  ê°€ëŠ¥í•œ ì˜ì—­**: {structura_protections[0] if structura_protections else 'ì¶”ê°€ ë¶„ì„ í•„ìš”'}

### ì‹œê°í™” ìë£Œ
- ìƒì„¸í•œ XAI ì‹œê°í™”ëŠ” ë‹¤ìŒ ê²½ë¡œì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤:
  `app/results/{department.replace(' ', '_').replace('&', '_&_')}/employee_{employee_id}/visualizations/`

## ğŸ“… í›„ì† ì¡°ì¹˜ ê³„íš

### ë‹¨ê¸° (1ê°œì›” ì´ë‚´)
- [ ] ì§ì† ìƒì‚¬ì™€ì˜ 1:1 ë©´ë‹´ ì‹¤ì‹œ
- [ ] ì£¼ìš” ìœ„í—˜ ìš”ì¸ì— ëŒ€í•œ êµ¬ì²´ì  ê°œì„  ë°©ì•ˆ ë…¼ì˜
- [ ] ì—…ë¬´ í™˜ê²½ ë° ì¡°ê±´ ì ê²€

### ì¤‘ê¸° (3ê°œì›” ì´ë‚´)
- [ ] ê°œì„  ë°©ì•ˆ ì‹¤í–‰ ë° íš¨ê³¼ ì¸¡ì •
- [ ] ì¶”ê°€ì ì¸ ì§€ì› ë°©ì•ˆ ê²€í† 
- [ ] ì •ê¸°ì  ëª¨ë‹ˆí„°ë§ ì²´ê³„ êµ¬ì¶•

### ì¥ê¸° (6ê°œì›” ì´í›„)
- [ ] ìœ„í—˜ë„ ì¬í‰ê°€ ì‹¤ì‹œ
- [ ] ì¥ê¸°ì  ê²½ë ¥ ê°œë°œ ê³„íš ìˆ˜ë¦½
- [ ] ì¡°ì§ ë‚´ ì—­í•  ë° ê¸°ì—¬ë„ ì¬ê²€í† 

---
*ë³¸ ë³´ê³ ì„œëŠ” AI ê¸°ë°˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìœ¼ë©°, ì‹¤ì œ ì¸ì‚¬ ê²°ì • ì‹œì—ëŠ” ì¶”ê°€ì ì¸ ì •ì„±ì  í‰ê°€ê°€ í•„ìš”í•©ë‹ˆë‹¤.*

**ë³´ê³ ì„œ ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}
"""

        return report_template.strip()
        
    except Exception as e:
        print(f"âŒ LLM ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return f"ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


@app.route('/get_employee_list', methods=['GET'])
def get_employee_list():
    """ë¡œë“œëœ ì§ì› ëª©ë¡ ì¡°íšŒ"""
    try:
        if report_generator.employee_data is None:
            return jsonify({
                'success': False,
                'error': 'ì§ì› ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'
            }), 400
        
        # ì§ì› ëª©ë¡ ì¶”ì¶œ
        if 'employee_id' in report_generator.employee_data.columns:
            employee_list = report_generator.employee_data['employee_id'].tolist()
        else:
            # employee_id ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ ì‚¬ìš©
            employee_list = report_generator.employee_data.index.tolist()
        
        return jsonify({
            'success': True,
            'total_employees': len(employee_list),
            'employee_ids': employee_list[:100],  # ì²˜ìŒ 100ëª…ë§Œ ë°˜í™˜
            'has_more': len(employee_list) > 100
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ì§ì› ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500

@app.route('/save_agent_models', methods=['POST'])
def save_agent_models():
    """ì—ì´ì „íŠ¸ ëª¨ë¸ì„ app/results/modelsì— ì €ì¥"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'ìš”ì²­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'
            }), 400
        
        models = data.get('models')
        save_path = data.get('save_path', 'app/results/models/agent_models.json')
        
        if not models:
            return jsonify({
                'success': False,
                'error': 'ì €ì¥í•  ëª¨ë¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'
            }), 400
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        import os
        # app/Integrationì—ì„œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™ (../../)
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        absolute_save_path = os.path.join(project_root, save_path)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(absolute_save_path), exist_ok=True)
        
        # íŒŒì¼ ì €ì¥
        import json
        with open(absolute_save_path, 'w', encoding='utf-8') as f:
            json.dump(models, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ì—ì´ì „íŠ¸ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
        
        return jsonify({
            'success': True,
            'message': 'ì—ì´ì „íŠ¸ ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤',
            'file_path': save_path,
            'agents_saved': len(models.get('saved_models', {})),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        print(f"âŒ ì—ì´ì „íŠ¸ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'ëª¨ë¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
        }), 500

@app.route('/save_optimized_models', methods=['POST'])
def save_optimized_models():
    """ìµœì í™”ëœ ëª¨ë¸ê³¼ ì„ê³„ê°’/ê°€ì¤‘ì¹˜ë¥¼ app/results/modelsì— ì €ì¥"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'ìš”ì²­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'
            }), 400
        
        complete_model = data.get('complete_model')
        save_path = data.get('save_path', 'app/results/models/optimized_models.json')
        
        if not complete_model:
            return jsonify({
                'success': False,
                'error': 'ì €ì¥í•  ëª¨ë¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'
            }), 400
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        import os
        # app/Integrationì—ì„œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™ (../../)
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        absolute_save_path = os.path.join(project_root, save_path)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(absolute_save_path), exist_ok=True)
        
        # ë°°ì¹˜ ë¶„ì„ì—ì„œ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬ì¡°í™”
        optimized_data = {
            'metadata': {
                'created_at': datetime.now().isoformat(),
                'stage': complete_model.get('training_metadata', {}).get('stage', 'optimization_completed'),
                'agents_used': complete_model.get('training_metadata', {}).get('agents_used', []),
                'training_data_size': complete_model.get('training_metadata', {}).get('training_data_size', 0)
            },
            'agent_models': complete_model.get('saved_models', {}),
            'agent_results': complete_model.get('agent_results', {}),
            'optimization_results': complete_model.get('optimization_results', {}),
            'ready_for_batch_analysis': True
        }
        
        # íŒŒì¼ ì €ì¥
        import json
        with open(absolute_save_path, 'w', encoding='utf-8') as f:
            json.dump(optimized_data, f, indent=2, ensure_ascii=False)
        
        # ë°°ì¹˜ ë¶„ì„ìš© ì‹¬ë³¼ë¦­ ë§í¬ ë˜ëŠ” ë³µì‚¬ë³¸ ìƒì„±
        batch_ready_path = 'app/results/models/batch_ready_models.json'
        absolute_batch_path = os.path.join(project_root, batch_ready_path)
        with open(absolute_batch_path, 'w', encoding='utf-8') as f:
            json.dump(optimized_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ìµœì í™”ëœ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
        print(f"âœ… ë°°ì¹˜ ë¶„ì„ìš© ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ: {batch_ready_path}")
        
        return jsonify({
            'success': True,
            'message': 'ìµœì í™”ëœ ëª¨ë¸ê³¼ ì„ê³„ê°’/ê°€ì¤‘ì¹˜ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤',
            'file_path': save_path,
            'batch_ready_path': batch_ready_path,
            'agents_count': len(optimized_data['agent_models']),
            'has_optimization': bool(optimized_data['optimization_results']),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        print(f"âŒ ìµœì í™”ëœ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'ëª¨ë¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
        }), 500


if __name__ == '__main__':
    print("ğŸš€ Integration Flask ì„œë²„ ì‹œì‘")
    print(f"ğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬: {DATA_DIR}")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_DIR}")
    print("ğŸŒ ì„œë²„ ì£¼ì†Œ: http://localhost:5007")
    
    # API í‚¤ ìƒíƒœ í™•ì¸ (Sentio/Agoraì™€ ë™ì¼)
    if report_generator.client:
        print("âœ… OpenAI API ì—°ê²° ì„±ê³µ - LLM ê¸°ë°˜ ë¶„ì„ ê°€ëŠ¥")
    else:
        print("âš ï¸  OpenAI API í‚¤ ì—†ìŒ - ê¸°ë³¸ ë¶„ì„ ëª¨ë“œë¡œ ë™ì‘")
        print("   ğŸ’¡ .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ /set_api_key ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
    print("\nì‚¬ìš© ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸:")
    print("  GET  /health - ì„œë²„ ìƒíƒœ í™•ì¸")
    print("  POST /set_api_key - OpenAI API í‚¤ ì„¤ì •")
    print("  POST /load_data - ë°ì´í„° ë¡œë“œ")
    print("  POST /calculate_thresholds - ì„ê³„ê°’ ê³„ì‚°")
    print("  POST /optimize_weights - ê°€ì¤‘ì¹˜ ìµœì í™”")
    print("  POST /predict_employee - ê°œë³„ ì§ì› ì˜ˆì¸¡")
    print("  GET  /get_results - í˜„ì¬ ê²°ê³¼ ì¡°íšŒ")
    print("  POST /compare_methods - ìµœì í™” ë°©ë²• ë¹„êµ")
    print("  POST /export_results - ê²°ê³¼ ë‚´ë³´ë‚´ê¸°")
    print("  POST /load_employee_data - ì§ì› ê¸°ë³¸ ë°ì´í„° ë¡œë“œ")
    print("  GET  /get_employee_list - ì§ì› ëª©ë¡ ì¡°íšŒ")
    print("  POST /generate_report - ê°œë³„ ì§ì› ë ˆí¬íŠ¸ ìƒì„± (LLM ì§€ì›)")
    print("  POST /generate_batch_reports - ì¼ê´„ ë ˆí¬íŠ¸ ìƒì„±")
    print("  POST /save_agent_models - ì—ì´ì „íŠ¸ ëª¨ë¸ ì €ì¥")
    print("  POST /save_optimized_models - ìµœì í™”ëœ ëª¨ë¸ ì €ì¥")
    
    app.run(host='0.0.0.0', port=5007, debug=True)
