# ============================================================================
# ì˜¬ë°”ë¥¸ ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ìœ„í•œ ìˆ˜ì •ëœ Chronos ë¶„ì„ ì½”ë“œ
# ============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# PyTorch ë¼ì´ë¸ŒëŸ¬ë¦¬
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
import torch.nn.functional as F

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
import optuna
from optuna.trial import TrialState

# ê¸°íƒ€ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.utils.class_weight import compute_class_weight
from tqdm import tqdm
import os
import json

# ì‹œë“œ ì„¤ì •
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

# í”Œë¡¯ ì„¤ì •
plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

# Optuna ë¡œê¹… ì„¤ì •
optuna.logging.set_verbosity(optuna.logging.WARNING)

# ============================================================================
# ì˜¬ë°”ë¥¸ ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤
# ============================================================================

class ProperTimeSeriesProcessor:
    def __init__(self, sequence_length=6, prediction_horizon=4, aggregation_unit='week'):
        """
        ì˜¬ë°”ë¥¸ ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ìœ„í•œ í”„ë¡œì„¸ì„œ
        
        Args:
            sequence_length (int): ì˜ˆì¸¡ì— ì‚¬ìš©í•  ê³¼ê±° ì‹œí€€ìŠ¤ ê¸¸ì´
            prediction_horizon (int): ì˜ˆì¸¡ ì‹œì  (Nì£¼ í›„ í‡´ì‚¬ ì—¬ë¶€ ì˜ˆì¸¡)
            aggregation_unit (str): ì§‘ê³„ ë‹¨ìœ„ ('day', 'week', 'month')
        """
        self.sequence_length = sequence_length
        self.prediction_horizon = prediction_horizon
        self.aggregation_unit = aggregation_unit
        self.scaler = StandardScaler()
        self.excluded_ratio_columns = [
            'work_focused_ratio', 'meeting_collaboration_ratio', 
            'social_dining_ratio', 'break_relaxation_ratio', 'shared_work_ratio'
        ]
        self.feature_columns = []
        
        print(f"ğŸ”§ ì˜¬ë°”ë¥¸ ì‹œê³„ì—´ ì„¤ì •:")
        print(f"   ì‹œí€€ìŠ¤ ê¸¸ì´: {sequence_length}{aggregation_unit[0]}")
        print(f"   ì˜ˆì¸¡ ì‹œì : {prediction_horizon}{aggregation_unit[0]} í›„")
        print(f"   ì§‘ê³„ ë‹¨ìœ„: {aggregation_unit}")
        
    def load_data(self, timeseries_path, personas_path):
        """ë°ì´í„° ë¡œë”©"""
        print("=" * 50)
        print("ë°ì´í„° ë¡œë”© ì¤‘...")
        print("=" * 50)
        
        # ì‹œê³„ì—´ ë°ì´í„° ë¡œë“œ
        self.ts_data = pd.read_csv(timeseries_path)
        print(f"âœ… ì‹œê³„ì—´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {self.ts_data.shape}")
        
        # ì§ì› ì†ì„± ë°ì´í„° ë¡œë“œ
        self.personas_data = pd.read_csv(personas_path)
        print(f"âœ… ì§ì› ì†ì„± ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {self.personas_data.shape}")
        
        return self.ts_data.head(), self.personas_data.head()

    def detect_columns(self):
        """ì»¬ëŸ¼ ìë™ ê°ì§€ ë° ë§¤ì¹­"""
        print("=" * 50)
        print("ì»¬ëŸ¼ ìë™ ê°ì§€ ì¤‘...")
        print("=" * 50)
        
        # ë‚ ì§œ ì»¬ëŸ¼ ìë™ ê°ì§€
        date_columns = [col for col in self.ts_data.columns 
                       if any(keyword in col.lower() for keyword in ['date', 'time', 'timestamp'])]
        
        if date_columns:
            self.date_column = date_columns[0]
            print(f"ğŸ—“ï¸  ê°ì§€ëœ ë‚ ì§œ ì»¬ëŸ¼: {self.date_column}")
        else:
            raise ValueError("âŒ ë‚ ì§œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        # ì§ì› ID ì»¬ëŸ¼ ìë™ ê°ì§€
        employee_id_candidates = [col for col in self.ts_data.columns 
                                 if any(keyword in col.lower() for keyword in ['employee', 'id', 'number'])]
        
        if employee_id_candidates:
            self.employee_id_col = employee_id_candidates[0]
            print(f"ğŸ‘¤ ê°ì§€ëœ ì‹œê³„ì—´ ì§ì› ID ì»¬ëŸ¼: {self.employee_id_col}")
        else:
            raise ValueError("âŒ ì§ì› ID ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        # ì§ì› ì†ì„± ë°ì´í„°ì—ì„œ ë§¤ì¹­ë˜ëŠ” ID ì»¬ëŸ¼ ì°¾ê¸°
        personas_id_candidates = [col for col in self.personas_data.columns 
                                 if any(keyword in col.lower() for keyword in ['employee', 'id', 'number'])]
        
        max_overlap = 0
        best_match_col = None
        
        for col in personas_id_candidates:
            try:
                overlap = len(set(self.ts_data[self.employee_id_col]).intersection(
                             set(self.personas_data[col])))
                if overlap > max_overlap:
                    max_overlap = overlap
                    best_match_col = col
            except:
                continue
                
        if best_match_col:
            self.personas_id_col = best_match_col
            print(f"âœ… ìµœì  ë§¤ì¹­ ì»¬ëŸ¼: {self.personas_id_col} ({max_overlap}ëª… ê²¹ì¹¨)")
        else:
            raise ValueError("âŒ ì§ì› ì†ì„± ë°ì´í„°ì—ì„œ ë§¤ì¹­ë˜ëŠ” ID ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # Attrition ì»¬ëŸ¼ ì°¾ê¸°
        attrition_cols = [col for col in self.personas_data.columns 
                         if 'attrition' in col.lower()]
        
        if attrition_cols:
            self.attrition_col = attrition_cols[0]
            print(f"ğŸ¯ ê°ì§€ëœ Attrition ì»¬ëŸ¼: {self.attrition_col}")
        else:
            raise ValueError("âŒ Attrition ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    def preprocess_data(self):
        """ë°ì´í„° ì „ì²˜ë¦¬ ë° ì‹œê°„ ë²”ìœ„ ë¶„ì„"""
        print("=" * 50)
        print("ì˜¬ë°”ë¥¸ ì‹œê³„ì—´ ì „ì²˜ë¦¬ ì¤‘...")
        print("=" * 50)
        
        # ë‚ ì§œ ë³€í™˜
        self.ts_data[self.date_column] = pd.to_datetime(self.ts_data[self.date_column])
        
        # 2023-2024ë…„ ë°ì´í„°ë§Œ í•„í„°ë§
        start_date = datetime(2023, 1, 1)
        end_date = datetime(2024, 12, 31)
        
        original_shape = self.ts_data.shape
        self.ts_data = self.ts_data[
            (self.ts_data[self.date_column] >= start_date) & 
            (self.ts_data[self.date_column] <= end_date)
        ].copy()
        
        print(f"ğŸ“… 2023-2024ë…„ í•„í„°ë§: {original_shape} â†’ {self.ts_data.shape}")
        
        # Attrition ë¼ë²¨ì„ 0/1ë¡œ ë³€í™˜
        def convert_attrition(value):
            if pd.isna(value):
                return 0
            value_str = str(value).lower().strip()
            if value_str in ['yes', 'y', 'true', '1', '1.0']:
                return 1
            else:
                return 0
                
        self.personas_data['attrition_binary'] = self.personas_data[self.attrition_col].apply(convert_attrition)
        
        attrition_dist = self.personas_data['attrition_binary'].value_counts()
        print(f"\nğŸ¯ Attrition ë¶„í¬:")
        print(f"   ì¬ì§(0): {attrition_dist.get(0, 0)}ëª…")
        print(f"   í‡´ì‚¬(1): {attrition_dist.get(1, 0)}ëª…")
        print(f"   í‡´ì‚¬ìœ¨: {attrition_dist.get(1, 0) / len(self.personas_data) * 100:.1f}%")

    def identify_features(self):
        """í”¼ì²˜ ì‹ë³„"""
        print("=" * 50)
        print("í”¼ì²˜ ì‹ë³„ ì¤‘...")
        print("=" * 50)
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì‹ë³„
        numeric_columns = self.ts_data.select_dtypes(include=[np.number]).columns.tolist()
        
        # ì œì™¸í•  ì»¬ëŸ¼ë“¤ ì œê±°
        exclude_columns = [self.employee_id_col] + self.excluded_ratio_columns
        exclude_columns = [col for col in exclude_columns if col in numeric_columns]
        
        self.feature_columns = [col for col in numeric_columns if col not in exclude_columns]
        
        print(f"ğŸ” ì „ì²´ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: {len(numeric_columns)}ê°œ")
        print(f"âŒ ì œì™¸ëœ ì»¬ëŸ¼: {exclude_columns}")
        print(f"âœ… ì‚¬ìš©í•  í”¼ì²˜: {len(self.feature_columns)}ê°œ")

    def create_proper_sequences(self):
        """ì˜¬ë°”ë¥¸ ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„± - ì‹œê°„ ìˆœì„œ ê³ ë ¤"""
        print("=" * 50)
        print("ì˜¬ë°”ë¥¸ ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
        print("=" * 50)
        
        # ì‹œê°„ë³„ ì§‘ê³„
        if self.aggregation_unit == 'week':
            self.ts_data['year'] = self.ts_data[self.date_column].dt.year
            self.ts_data['week'] = self.ts_data[self.date_column].dt.isocalendar().week
            self.ts_data['time_period'] = self.ts_data['year'].astype(str) + '-W' + self.ts_data['week'].astype(str).str.zfill(2)
        
        # ì‹œê°„ë³„ ì§‘ê³„
        agg_data = self.ts_data.groupby([self.employee_id_col, 'time_period'])[self.feature_columns].mean().reset_index()
        
        # ì§ì› ì†ì„± ë°ì´í„°ì™€ ë³‘í•©
        merged_data = pd.merge(
            agg_data,
            self.personas_data[[self.personas_id_col, 'attrition_binary']],
            left_on=self.employee_id_col,
            right_on=self.personas_id_col,
            how='inner'
        )
        
        # ì‹œê°„ ìˆœì„œ ì •ë ¬
        merged_data = merged_data.sort_values(['employee_id', 'time_period'])
        
        sequences = []
        labels = []
        employee_ids = []
        time_points = []
        
        print("ğŸ”„ ì§ì›ë³„ ì˜¬ë°”ë¥¸ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
        
        for employee_id in tqdm(merged_data[self.employee_id_col].unique(), desc="ì§ì›ë³„ ì²˜ë¦¬"):
            employee_data = merged_data[
                merged_data[self.employee_id_col] == employee_id
            ].sort_values('time_period').reset_index(drop=True)
            
            attrition_label = employee_data['attrition_binary'].iloc[0]
            
            # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
            min_required_length = self.sequence_length + self.prediction_horizon
            if len(employee_data) >= min_required_length:
                
                # ì˜¬ë°”ë¥¸ ì ‘ê·¼: ê° ì§ì›ë‹¹ í•˜ë‚˜ì˜ ì „ì²´ ì‹œê³„ì—´ íŒ¨í„´
                # 1470ëª… ê°ê°ì˜ ì™„ì „í•œ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ í•™ìŠµ
                
                # ì „ì²´ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ê³ ì • ê¸¸ì´ë¡œ ë§ì¶¤ (íŒ¨ë”© ë˜ëŠ” ìƒ˜í”Œë§)
                if len(employee_data) >= self.sequence_length:
                    if len(employee_data) > self.sequence_length:
                        # ë°ì´í„°ê°€ ë” ê¸¸ë©´ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
                        indices = np.linspace(0, len(employee_data)-1, self.sequence_length, dtype=int)
                        sequence_data = employee_data.iloc[indices][self.feature_columns].values
                    else:
                        # ë°ì´í„°ê°€ ì •í™•íˆ ë§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        sequence_data = employee_data[self.feature_columns].values
                    
                    # ê° ì§ì›ë‹¹ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë§Œ ìƒì„±
                    sequences.append(sequence_data)
                    labels.append(attrition_label)
                    employee_ids.append(employee_id)
                    time_points.append(employee_data.iloc[0]['time_period'])  # ì‹œì‘ ì‹œì 
        
        self.X = np.array(sequences, dtype=np.float32)
        self.y = np.array(labels, dtype=np.int64)
        self.employee_ids_seq = np.array(employee_ids)
        self.time_points = np.array(time_points)
        
        print(f"âœ… ì˜¬ë°”ë¥¸ ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ:")
        print(f"   ì´ ì‹œí€€ìŠ¤: {len(self.X)}ê°œ")
        print(f"   ì‹œí€€ìŠ¤ í˜•íƒœ: {self.X.shape}")
        print(f"   í‡´ì‚¬ ë¼ë²¨ ë¹„ìœ¨: {np.mean(self.y) * 100:.1f}%")
        
        return self.X, self.y, self.employee_ids_seq

# ============================================================================
# ëª¨ë¸ ì •ì˜ (ê¸°ì¡´ê³¼ ë™ì¼)
# ============================================================================

class GRU_CNN_HybridModel(nn.Module):
    """íŒŒë¼ë¯¸í„°í™”ëœ GRU+CNN í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸"""
    
    def __init__(self, input_size, gru_hidden=32, cnn_filters=16, kernel_sizes=[2, 3], dropout=0.2):
        super(GRU_CNN_HybridModel, self).__init__()
        
        # GRU ë¸Œëœì¹˜
        self.gru = nn.GRU(
            input_size=input_size,
            hidden_size=gru_hidden,
            num_layers=1,
            dropout=dropout,
            batch_first=True
        )
        self.gru_dropout = nn.Dropout(dropout)
        
        # CNN ë¸Œëœì¹˜
        self.conv_layers = nn.ModuleList()
        for kernel_size in kernel_sizes:
            conv_layer = nn.Sequential(
                nn.Conv1d(input_size, cnn_filters, kernel_size, padding=kernel_size//2),
                nn.ReLU(),
                nn.BatchNorm1d(cnn_filters),
                nn.AdaptiveMaxPool1d(1)
            )
            self.conv_layers.append(conv_layer)
        
        # ë¶„ë¥˜ê¸°
        combined_features = gru_hidden + len(kernel_sizes) * cnn_filters
        self.classifier = nn.Sequential(
            nn.Linear(combined_features, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(32, 2)
        )
        
        # ì–´í…ì…˜
        self.attention = nn.Sequential(
            nn.Linear(gru_hidden, 1),
            nn.Softmax(dim=1)
        )
    
    def forward(self, x):
        # GRU + ì–´í…ì…˜
        gru_out, _ = self.gru(x)
        attention_weights = self.attention(gru_out)
        gru_features = torch.sum(gru_out * attention_weights, dim=1)
        gru_features = self.gru_dropout(gru_features)
        
        # CNN
        x_cnn = x.transpose(1, 2)
        cnn_outputs = []
        for conv_layer in self.conv_layers:
            conv_out = conv_layer(x_cnn).squeeze(-1)
            cnn_outputs.append(conv_out)
        
        cnn_features = torch.cat(cnn_outputs, dim=1)
        
        # ê²°í•© ë° ë¶„ë¥˜
        combined_features = torch.cat([gru_features, cnn_features], dim=1)
        output = self.classifier(combined_features)
        return output

# ============================================================================
# ì‹œê°„ ê¸°ë°˜ ë°ì´í„° ë¶„í•  í•¨ìˆ˜
# ============================================================================

def employee_based_train_test_split(X, y, employee_ids, test_ratio=0.2):
    """ì§ì› ê¸°ë°˜ train/test ë¶„í•  (ì‹œê°„ ìˆœì„œ ê³ ë ¤í•˜ë©´ì„œ í´ë˜ìŠ¤ ê· í˜• ìœ ì§€)"""
    
    # ì§ì›ë³„ ë¼ë²¨ í™•ì¸
    unique_employees = np.unique(employee_ids)
    employee_labels = []
    
    for emp_id in unique_employees:
        emp_sequences = y[employee_ids == emp_id]
        # í•´ë‹¹ ì§ì›ì˜ ì‹œí€€ìŠ¤ ì¤‘ í•˜ë‚˜ë¼ë„ positiveë©´ í‡´ì‚¬ ì§ì›ìœ¼ë¡œ ë¶„ë¥˜
        emp_label = 1 if np.any(emp_sequences == 1) else 0
        employee_labels.append(emp_label)
    
    employee_labels = np.array(employee_labels)
    
    print(f"ğŸ“Š ì§ì›ë³„ ë¼ë²¨ ë¶„í¬:")
    print(f"   ì´ ì§ì› ìˆ˜: {len(unique_employees)}ëª…")
    print(f"   í‡´ì‚¬ ì§ì›: {np.sum(employee_labels)}ëª…")
    print(f"   ì¬ì§ ì§ì›: {np.sum(employee_labels == 0)}ëª…")
    
    # ì§ì› ë ˆë²¨ì—ì„œ stratified split
    try:
        train_employees, test_employees = train_test_split(
            unique_employees, test_size=test_ratio, random_state=42, 
            stratify=employee_labels
        )
    except ValueError:
        # stratifyê°€ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ì¼ë°˜ ë¶„í• 
        train_employees, test_employees = train_test_split(
            unique_employees, test_size=test_ratio, random_state=42
        )
    
    # ì§ì› ê¸°ë°˜ìœ¼ë¡œ ì‹œí€€ìŠ¤ ë¶„í• 
    train_mask = np.isin(employee_ids, train_employees)
    test_mask = np.isin(employee_ids, test_employees)
    
    X_train = X[train_mask]
    X_test = X[test_mask]
    y_train = y[train_mask]
    y_test = y[test_mask]
    
    print(f"ğŸ‘¥ ì§ì› ê¸°ë°˜ ë¶„í•  ì™„ë£Œ:")
    print(f"   í›ˆë ¨ ì§ì›: {len(train_employees)}ëª…")
    print(f"   í…ŒìŠ¤íŠ¸ ì§ì›: {len(test_employees)}ëª…")
    print(f"   í›ˆë ¨ ì‹œí€€ìŠ¤: {len(X_train)}ê°œ (í‡´ì‚¬ìœ¨: {np.mean(y_train)*100:.1f}%)")
    print(f"   í…ŒìŠ¤íŠ¸ ì‹œí€€ìŠ¤: {len(X_test)}ê°œ (í‡´ì‚¬ìœ¨: {np.mean(y_test)*100:.1f}%)")
    
    return X_train, X_test, y_train, y_test

# ============================================================================
# ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
# ============================================================================

if __name__ == "__main__":
    print("ğŸš€ ì˜¬ë°”ë¥¸ ì‹œê³„ì—´ ì˜ˆì¸¡ ì‹œì‘")
    print("=" * 70)
    
    # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” (ë” ê¸´ ì‹œí€€ìŠ¤ë¡œ ì „ì²´ íŒ¨í„´ ìº¡ì²˜)
    processor = ProperTimeSeriesProcessor(
        sequence_length=50,  # 50ì£¼ = ì•½ 1ë…„ê°„ì˜ íŒ¨í„´
        prediction_horizon=4,  # ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ í˜¸í™˜ì„± ìœ ì§€
        aggregation_unit='week'
    )
    
    # ë°ì´í„° ë¡œë”©
    ts_sample, personas_sample = processor.load_data(
        'data/IBM_HR_timeseries.csv', 
        'data/IBM_HR_personas_assigned.csv'
    )
    
    # ì „ì²˜ë¦¬
    processor.detect_columns()
    processor.preprocess_data()
    processor.identify_features()
    
    # ì˜¬ë°”ë¥¸ ì‹œí€€ìŠ¤ ìƒì„±
    X, y, employee_ids = processor.create_proper_sequences()
    
    # ì§ì› ê¸°ë°˜ ë¶„í•  (í´ë˜ìŠ¤ ê· í˜• ìœ ì§€)
    X_train, X_test, y_train, y_test = employee_based_train_test_split(
        X, y, employee_ids, test_ratio=0.2
    )
    
    print(f"\nğŸ¯ ì˜¬ë°”ë¥¸ ì‹œê³„ì—´ ì˜ˆì¸¡ ì¤€ë¹„ ì™„ë£Œ!")
    print(f"   ì´ì œ í˜„ì‹¤ì ì¸ ì„±ëŠ¥ ì§€í‘œë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print(f"   ì˜ˆìƒ ì„±ëŠ¥: AUC 0.6-0.8 (í˜„ì‹¤ì ì¸ ë²”ìœ„)")
    
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì€ í•¨ìˆ˜ë“¤ì´ ì •ì˜ëœ í›„ì— ìˆ˜í–‰ë©ë‹ˆë‹¤.

# ============================================================================
# í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í•¨ìˆ˜ë“¤
# ============================================================================

def prepare_data_for_training(X_train, y_train, X_test, y_test, val_size=0.2):
    """í›ˆë ¨ìš© ë°ì´í„° ì¤€ë¹„"""
    # ì‹œê°„ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ ë¶„í• ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì¶”ê°€ ë¶„í•  ì—†ì´ ì§„í–‰
    # Validationì€ trainì—ì„œ ì‹œê°„ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì—¬ ë¶„í• 
    split_idx = int(len(X_train) * (1 - val_size))
    
    X_train_split = X_train[:split_idx]
    X_val = X_train[split_idx:]
    y_train_split = y_train[:split_idx]
    y_val = y_train[split_idx:]
    
    # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    print(f"ğŸ“Š ì‹œê°„ ê¸°ë°˜ ë¶„í•  í›„ í´ë˜ìŠ¤ ë¶„í¬:")
    print(f"   Train: {np.bincount(y_train_split)} (í‡´ì‚¬ìœ¨: {np.mean(y_train_split)*100:.1f}%)")
    print(f"   Val: {np.bincount(y_val)} (í‡´ì‚¬ìœ¨: {np.mean(y_val)*100:.1f}%)")
    print(f"   Test: {np.bincount(y_test)} (í‡´ì‚¬ìœ¨: {np.mean(y_test)*100:.1f}%)")
    
    # ì •ê·œí™”
    scaler = StandardScaler()
    n_samples, n_timesteps, n_features = X_train_split.shape
    X_train_reshaped = X_train_split.reshape(-1, n_features)
    
    scaler.fit(X_train_reshaped)
    
    X_train_scaled = scaler.transform(X_train_reshaped).reshape(n_samples, n_timesteps, n_features)
    X_val_scaled = scaler.transform(X_val.reshape(-1, n_features)).reshape(X_val.shape)
    X_test_scaled = scaler.transform(X_test.reshape(-1, n_features)).reshape(X_test.shape)
    
    # PyTorch í…ì„œë¡œ ë³€í™˜
    X_train_tensor = torch.FloatTensor(X_train_scaled)
    y_train_tensor = torch.LongTensor(y_train_split)
    X_val_tensor = torch.FloatTensor(X_val_scaled)
    y_val_tensor = torch.LongTensor(y_val)
    X_test_tensor = torch.FloatTensor(X_test_scaled)
    y_test_tensor = torch.LongTensor(y_test)
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ (ì•ˆì „í•œ ì²˜ë¦¬)
    unique_classes = np.unique(y_train_split)
    if len(unique_classes) == 2:
        class_weights = compute_class_weight('balanced', classes=unique_classes, y=y_train_split)
        class_weights_tensor = torch.FloatTensor(class_weights).to(device)
    else:
        # í•œ í´ë˜ìŠ¤ë§Œ ìˆëŠ” ê²½ìš° ê°€ì¤‘ì¹˜ ì—†ì´ ì²˜ë¦¬
        class_weights_tensor = None
        print(f"âš ï¸  ê²½ê³ : í›ˆë ¨ ë°ì´í„°ì— {len(unique_classes)}ê°œ í´ë˜ìŠ¤ë§Œ ì¡´ì¬í•©ë‹ˆë‹¤.")
    
    return (X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, 
            X_test_tensor, y_test_tensor, class_weights_tensor, scaler)

def train_model(model, X_train, y_train, X_val, y_val, class_weights, 
                lr=0.001, epochs=30, batch_size=64, patience=10):
    """ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜"""
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    train_dataset = TensorDataset(X_train, y_train)
    val_dataset = TensorDataset(X_val, y_val)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    # í•™ìŠµ ì„¤ì •
    criterion = nn.CrossEntropyLoss(weight=class_weights) if class_weights is not None else nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
    
    best_val_loss = float('inf')
    patience_counter = 0
    best_model_state = None
    
    for epoch in range(epochs):
        # í•™ìŠµ
        model.train()
        train_loss = 0.0
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item()
        
        # ê²€ì¦
        model.eval()
        val_loss = 0.0
        val_predictions = []
        val_labels = []
        
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                val_loss += loss.item()
                
                probs = F.softmax(outputs, dim=1)
                val_predictions.extend(probs[:, 1].cpu().numpy())
                val_labels.extend(batch_y.cpu().numpy())
        
        avg_val_loss = val_loss / len(val_loader)
        scheduler.step(avg_val_loss)
        
        # Early stopping
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            best_model_state = model.state_dict().copy()
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            break
    
    # ìµœê³  ëª¨ë¸ ë¡œë“œ
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
    
    # ìµœì¢… ê²€ì¦ ì ìˆ˜ ê³„ì‚°
    model.eval()
    val_predictions = []
    val_labels = []
    
    with torch.no_grad():
        for batch_X, batch_y in val_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            outputs = model(batch_X)
            probs = F.softmax(outputs, dim=1)
            val_predictions.extend(probs[:, 1].cpu().numpy())
            val_labels.extend(batch_y.cpu().numpy())
    
    try:
        auc_score = roc_auc_score(val_labels, val_predictions)
    except:
        auc_score = 0.5
    
    return auc_score

def objective_hybrid(trial, X_train, y_train, feature_count):
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ëª©ì í•¨ìˆ˜"""
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
    gru_hidden = trial.suggest_int('gru_hidden', 16, 128, step=16)
    cnn_filters = trial.suggest_int('cnn_filters', 8, 64, step=8)
    dropout = trial.suggest_float('dropout', 0.1, 0.5)
    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)
    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])
    
    # ë°ì´í„° ì¤€ë¹„
    try:
        (X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, 
         _, _, class_weights, _) = prepare_data_for_training(X_train, y_train, X_train, y_train)
    except:
        return 0.5
    
    # ëª¨ë¸ ìƒì„±
    model = GRU_CNN_HybridModel(
        input_size=feature_count,
        gru_hidden=gru_hidden,
        cnn_filters=cnn_filters,
        kernel_sizes=[2, 3],
        dropout=dropout
    ).to(device)
    
    # í•™ìŠµ ë° í‰ê°€
    auc_score = train_model(
        model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, class_weights,
        lr=lr, epochs=25, batch_size=batch_size, patience=8
    )
    
    return auc_score

def run_hyperparameter_optimization(X_train, y_train, feature_count, n_trials=30):
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰"""
    print("ğŸš€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")
    print("=" * 50)
    print(f"ì´ {n_trials}íšŒ ì‹œë„ë¡œ ìµœì  íŒŒë¼ë¯¸í„° íƒìƒ‰ ì¤‘...")
    
    # Optuna ìŠ¤í„°ë”” ìƒì„±
    study = optuna.create_study(
        direction='maximize',
        sampler=optuna.samplers.TPESampler(seed=42),
        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)
    )
    
    # ìµœì í™” ì‹¤í–‰
    study.optimize(lambda trial: objective_hybrid(trial, X_train, y_train, feature_count), 
                   n_trials=n_trials, show_progress_bar=True)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ† ìµœì í™” ì™„ë£Œ!")
    print("=" * 50)
    print(f"âœ… ìµœê³  AUC ìŠ¤ì½”ì–´: {study.best_value:.4f}")
    print(f"ğŸ¯ ìµœì  íŒŒë¼ë¯¸í„°:")
    
    for key, value in study.best_params.items():
        print(f"   {key}: {value}")
    
    return study

def evaluate_final_model(model, X_test, y_test, model_name="Fixed Hybrid Model"):
    """ìµœì¢… ëª¨ë¸ í‰ê°€"""
    model.eval()
    
    dataset = TensorDataset(X_test, y_test)
    loader = DataLoader(dataset, batch_size=64, shuffle=False)
    
    all_predictions = []
    all_probabilities = []
    all_labels = []
    
    with torch.no_grad():
        for batch_X, batch_y in loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            outputs = model(batch_X)
            probabilities = F.softmax(outputs, dim=1)
            _, predicted = torch.max(outputs, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_probabilities.extend(probabilities[:, 1].cpu().numpy())
            all_labels.extend(batch_y.cpu().numpy())
    
    all_predictions = np.array(all_predictions)
    all_probabilities = np.array(all_probabilities)
    all_labels = np.array(all_labels)
    
    accuracy = np.mean(all_predictions == all_labels)
    try:
        auc_score = roc_auc_score(all_labels, all_probabilities)
    except:
        auc_score = 0.5
    
    print(f"\nğŸ¯ {model_name} ìµœì¢… í‰ê°€")
    print("=" * 50)
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"   ì •í™•ë„: {accuracy:.4f}")
    print(f"   AUC: {auc_score:.4f}")
    
    print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
    print(classification_report(all_labels, all_predictions, target_names=['ì¬ì§', 'í‡´ì‚¬']))
    
    cm = confusion_matrix(all_labels, all_predictions)
    print(f"\nğŸ” í˜¼ë™ í–‰ë ¬:")
    print(cm)
    
    return {
        'accuracy': accuracy,
        'auc': auc_score,
        'predictions': all_predictions,
        'probabilities': all_probabilities,
        'labels': all_labels,
        'confusion_matrix': cm
    }

def predict_employee_with_fixed_model(employee_id, processor, model, scaler, sequence_length):
    """ìˆ˜ì •ëœ ëª¨ë¸ë¡œ ê°œë³„ ì§ì› ì˜ˆì¸¡"""
    
    # í•´ë‹¹ ì§ì›ì˜ ë°ì´í„° ì°¾ê¸°
    employee_data = processor.ts_data[
        processor.ts_data[processor.employee_id_col] == employee_id
    ].sort_values(processor.date_column)
    
    if len(employee_data) < sequence_length:
        raise ValueError(f"ì§ì› {employee_id}ì˜ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    # ì‹œê°„ë³„ ì§‘ê³„
    employee_data['year'] = employee_data[processor.date_column].dt.year
    employee_data['week'] = employee_data[processor.date_column].dt.isocalendar().week
    employee_data['time_period'] = employee_data['year'].astype(str) + '-W' + employee_data['week'].astype(str).str.zfill(2)
    
    agg_data = employee_data.groupby('time_period')[processor.feature_columns].mean().reset_index()
    agg_data = agg_data.sort_values('time_period')
    
    # ìµœê·¼ ì‹œí€€ìŠ¤ ìƒì„±
    sequence_data = agg_data[processor.feature_columns].values[-sequence_length:]
    
    # ì •ê·œí™”
    sequence_scaled = scaler.transform(sequence_data.reshape(-1, len(processor.feature_columns)))
    sequence_scaled = sequence_scaled.reshape(1, sequence_length, -1)
    
    # PyTorch í…ì„œë¡œ ë³€í™˜
    sequence_tensor = torch.FloatTensor(sequence_scaled).to(device)
    
    # ì˜ˆì¸¡
    model.eval()
    with torch.no_grad():
        outputs = model(sequence_tensor)
        probabilities = F.softmax(outputs, dim=1)
        attrition_prob = probabilities[0, 1].item()
        prediction = torch.argmax(outputs, dim=1).item()
    
    return {
        'employee_id': employee_id,
        'attrition_probability': attrition_prob,
        'prediction': prediction,
        'risk_level': 'High' if attrition_prob > 0.7 else 'Medium' if attrition_prob > 0.3 else 'Low',
        'sequence_length_used': sequence_length
    }

def predict_all_employees_and_save_fixed(processor, model, scaler, sequence_length, 
                                        save_path='Data analysis/data/employee_attrition_predictions_fixed.csv'):
    """ì „ì²´ ì§ì›ì— ëŒ€í•œ ìˆ˜ì •ëœ ì˜ˆì¸¡ ìˆ˜í–‰ ë° CSV ì €ì¥"""
    print("ğŸ”® ì „ì²´ ì§ì› ìˆ˜ì •ëœ ì‹œê³„ì—´ ì˜ˆì¸¡ ìˆ˜í–‰")
    print("=" * 50)
    
    # ëª¨ë“  ê³ ìœ  ì§ì› ID ê°€ì ¸ì˜¤ê¸°
    all_employees = processor.ts_data[processor.employee_id_col].unique()
    print(f"ğŸ“Š ì´ ì§ì› ìˆ˜: {len(all_employees)}ëª…")
    
    # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    all_predictions = []
    successful_predictions = 0
    failed_predictions = 0
    
    print("ğŸš€ ì˜ˆì¸¡ ì§„í–‰ ì¤‘...")
    
    # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ tqdm ì‚¬ìš©
    for emp_id in tqdm(all_employees, desc="ì§ì›ë³„ ì˜ˆì¸¡"):
        try:
            result = predict_employee_with_fixed_model(emp_id, processor, model, scaler, sequence_length)
            result['prediction_method'] = 'Fixed Timeseries Model'
            all_predictions.append(result)
            successful_predictions += 1
            
        except Exception as e:
            # ì˜ˆì¸¡ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ ì €ì¥
            failed_result = {
                'employee_id': emp_id,
                'attrition_probability': 0.5,
                'prediction': 0,
                'risk_level': 'Unknown',
                'sequence_length_used': sequence_length,
                'prediction_method': 'Failed',
                'prediction_status': 'Failed',
                'error_message': str(e)
            }
            all_predictions.append(failed_result)
            failed_predictions += 1
    
    print(f"\nğŸ“Š ì˜ˆì¸¡ ì™„ë£Œ ìš”ì•½:")
    print(f"   âœ… ì„±ê³µ: {successful_predictions}ëª…")
    print(f"   âŒ ì‹¤íŒ¨: {failed_predictions}ëª…")
    print(f"   ğŸ“ˆ ì„±ê³µë¥ : {successful_predictions/len(all_employees)*100:.1f}%")
    
    # DataFrameìœ¼ë¡œ ë³€í™˜
    predictions_df = pd.DataFrame(all_predictions)
    
    # í‡´ì‚¬ í™•ë¥  ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)
    predictions_df = predictions_df.sort_values('attrition_probability', ascending=False)
    
    # CSV íŒŒì¼ë¡œ ì €ì¥
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    predictions_df.to_csv(save_path, index=False, encoding='utf-8-sig')
    
    print(f"\nğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {save_path}")
    print(f"   ğŸ“„ ì €ì¥ëœ ë ˆì½”ë“œ ìˆ˜: {len(predictions_df)}ê°œ")
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    successful_df = predictions_df[predictions_df.get('prediction_method', '') == 'Fixed Timeseries Model']
    
    if len(successful_df) > 0:
        print(f"\nğŸ“ˆ ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½:")
        print(f"   í‰ê·  í‡´ì‚¬ í™•ë¥ : {successful_df['attrition_probability'].mean():.3f}")
        print(f"   ìµœê³  í‡´ì‚¬ í™•ë¥ : {successful_df['attrition_probability'].max():.3f}")
        print(f"   ìµœì € í‡´ì‚¬ í™•ë¥ : {successful_df['attrition_probability'].min():.3f}")
        
        # ìœ„í—˜ë„ë³„ ë¶„í¬
        risk_dist = successful_df['risk_level'].value_counts()
        print(f"\nğŸ¯ ìœ„í—˜ë„ë³„ ë¶„í¬:")
        for risk, count in risk_dist.items():
            if risk != 'Unknown':
                print(f"   {risk}: {count}ëª… ({count/len(successful_df)*100:.1f}%)")
        
        # ìƒìœ„ 10ëª… ì¶œë ¥
        print(f"\nğŸš¨ í‡´ì‚¬ ìœ„í—˜ ìƒìœ„ 10ëª…:")
        top_10 = successful_df.head(10)[['employee_id', 'attrition_probability', 'risk_level']]
        for idx, row in top_10.iterrows():
            print(f"   ì§ì› {row['employee_id']}: {row['attrition_probability']:.3f} ({row['risk_level']})")
    
    return predictions_df

# ============================================================================
# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# ============================================================================

if __name__ == "__main__":
    # ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ì—ì„œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    
    # ë°ì´í„° ì¤€ë¹„
    (X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, 
     X_test_tensor, y_test_tensor, class_weights_tensor, scaler) = prepare_data_for_training(
        X_train, y_train, X_test, y_test
    )
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
    study = run_hyperparameter_optimization(X_train, y_train, len(processor.feature_columns), n_trials=20)
    
    # ìµœì  ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
    print("\nğŸ† ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ")
    print("=" * 50)
    
    best_params = study.best_params
    best_model = GRU_CNN_HybridModel(
        input_size=len(processor.feature_columns),
        gru_hidden=best_params['gru_hidden'],
        cnn_filters=best_params['cnn_filters'],
        kernel_sizes=[2, 3],
        dropout=best_params['dropout']
    ).to(device)
    
    # ìµœì¢… í•™ìŠµ
    final_auc = train_model(
        best_model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, class_weights_tensor,
        lr=best_params['lr'], epochs=50, batch_size=best_params['batch_size'], patience=15
    )
    
    print(f"âœ… ìµœì¢… ê²€ì¦ AUC: {final_auc:.4f}")
    
    # ìµœì¢… í‰ê°€
    final_results = evaluate_final_model(best_model, X_test_tensor, y_test_tensor)
    
    # ì „ì²´ ì§ì› ì˜ˆì¸¡ ë° ì €ì¥
    print("\n" + "="*70)
    print("ğŸ¯ ì „ì²´ ì§ì› ìˆ˜ì •ëœ ì‹œê³„ì—´ ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥ ì‹œì‘")
    print("="*70)
    
    final_predictions_df = predict_all_employees_and_save_fixed(
        processor, 
        best_model, 
        scaler, 
        processor.sequence_length,
        save_path='data/employee_attrition_predictions_fixed.csv'
    )
    
    print(f"\nğŸ‰ ìˆ˜ì •ëœ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
    print(f"   ğŸ“Š ì´ {len(final_predictions_df)}ëª…ì˜ ì§ì› ì˜ˆì¸¡ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"   ğŸ“ ì €ì¥ ìœ„ì¹˜: data/employee_attrition_predictions_fixed.csv")
    print(f"   ğŸ¯ í˜„ì‹¤ì ì¸ ì„±ëŠ¥: AUC {final_results['auc']:.3f} (ë°ì´í„° ëˆ„ìˆ˜ ì œê±° í›„)")
